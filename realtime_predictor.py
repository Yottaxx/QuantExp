import torch
import pandas as pd
import numpy as np
import os
from typing import List, Dict, Optional, Tuple
from tqdm import tqdm
from dataclasses import dataclass

# å‡è®¾å¼•ç”¨æ‚¨çš„é¡¹ç›®æ¨¡å—
from src.config import Config
from src.model import PatchTSTForStock
from src.data_provider import DataProvider


@dataclass
class PredictionResult:
    code: str
    score: float
    target_date: pd.Timestamp


class RealTimePredictor:
    def __init__(self):
        self.device = torch.device(Config.DEVICE)
        self.seq_len = Config.CONTEXT_LEN
        self.model_path = os.path.join(Config.OUTPUT_DIR, "final_model")
        self.model = self._load_model()

    def _load_model(self) -> PatchTSTForStock:
        """åŠ è½½æ¨¡å‹å¹¶è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼ï¼Œç¡®ä¿æ— æ¢¯åº¦è®¡ç®—"""
        if not os.path.exists(self.model_path):
            raise FileNotFoundError(f"âŒ Critical: Model artifact not found at {self.model_path}")

        # æ˜¾å¼ä½¿ç”¨ CPU/GPU æ˜ å°„ï¼Œé˜²æ­¢è·¨è®¾å¤‡åŠ è½½é”™è¯¯
        model = PatchTSTForStock.from_pretrained(self.model_path).to(self.device)
        model.eval()
        return model

    def predict_next_day(self, date_str: str) -> pd.DataFrame:
        """
        æ ¸å¿ƒå…¥å£ï¼šç»™å®šæ—¥æœŸ T (date_str)ï¼Œé¢„æµ‹ T+1 æ—¥çš„è¡¨ç°ã€‚
        é€»è¾‘ï¼š
        1. è·å– [T - Lookback, T] çš„æ•°æ®ã€‚
        2. ä¸¥æ ¼ç­›é€‰æœ€åä¸€å¤©å¿…é¡»æ˜¯ T çš„è‚¡ç¥¨ã€‚
        3. æ‰¹é‡æ¨ç†ã€‚
        """
        target_date = pd.to_datetime(date_str)
        print(f"\nâš¡ [Inference] Target Date (T): {target_date.date()} | Predicting for: T+1")

        # 1. æ•°æ®å‡†å¤‡ (Data Preparation)
        # ç­–ç•¥ï¼šå‘å‰å¤šå–ä¸€äº›æ•°æ® (2.5å€ Context)ï¼Œä»¥åº”å¯¹éäº¤æ˜“æ—¥æˆ–åœç‰Œå¸¦æ¥çš„Gap
        # è¿™æ ·èƒ½ä¿è¯ç»å¤§å¤šæ•°è‚¡ç¥¨éƒ½èƒ½å‡‘é½ seq_len é•¿åº¦çš„äº¤æ˜“æ—¥æ•°æ®
        lookback_days = int(self.seq_len * 2.5)
        start_date = target_date - pd.Timedelta(days=lookback_days)

        # è°ƒç”¨ DataProvider (éœ€ç”¨æˆ·å®ç°è¯¥æ¥å£æ”¯æŒæ—¥æœŸè¿‡æ»¤)
        # å…³é”®ï¼šè¿™é‡Œè·å–çš„æ•°æ®å¿…é¡»å·²ç»åšè¿‡ å½’ä¸€åŒ–(Normalization) å¤„ç†ï¼Œä¸”ä¸è®­ç»ƒæ—¶ä¸€è‡´ï¼
        panel_df, feature_cols = DataProvider.load_and_process_panel(
            start_date=start_date,
            end_date=target_date
        )

        if panel_df.empty:
            print(f"âš ï¸ Warning: No data found ending on {target_date.date()}. Is it a trading day?")
            return pd.DataFrame()

        # 2. å¼ é‡æ„å»º (Tensor Construction)
        inputs, meta_data = self._build_inference_batches(
            panel_df, feature_cols, target_date
        )

        if not inputs:
            print("âš ï¸ Warning: No valid sequences constructed. Check data integrity or date matching.")
            return pd.DataFrame()

        # 3. æ¨¡å‹æ¨ç† (Model Inference)
        scores = self._run_inference(inputs)

        # 4. ç»“æœæ•´åˆ (Result Aggregation)
        results = []
        for meta, score in zip(meta_data, scores):
            results.append({
                'date': target_date,
                'code': meta['code'],
                'score': score
            })

        df_res = pd.DataFrame(results).sort_values(by='score', ascending=False).reset_index(drop=True)
        return df_res

    def _build_inference_batches(
            self,
            df: pd.DataFrame,
            cols: List[str],
            target_date: pd.Timestamp
    ) -> Tuple[List[np.ndarray], List[Dict]]:
        """
        æ„å»ºæ¨ç†æ‰¹æ¬¡ã€‚
        æ ¸å¿ƒé€»è¾‘ï¼š
        ä½¿ç”¨ Numpy å‘é‡åŒ–æ“ä½œè¿›è¡Œåˆ‡ç‰‡ï¼Œé¿å… Pandas GroupBy çš„ä½æ•ˆå¾ªç¯ã€‚
        ä¸¥æ ¼æ ¡éªŒï¼šSequence çš„æœ€åä¸€å¤©å¿…é¡»ç­‰äº Target Dateã€‚
        """
        # ç¡®ä¿æ•°æ®æŒ‰ code, date æ’åº
        df = df.sort_values(['code', 'date'])

        feat_vals = df[cols].values.astype(np.float32)
        codes = df['code'].values
        dates = df['date'].values

        # æ‰¾åˆ°æ¯ä¸ª code çš„åˆ‡åˆ†ç‚¹
        unique_codes, code_indices = np.unique(codes, return_index=True)
        code_indices = np.append(code_indices, len(codes))  # æ·»åŠ æœ«å°¾ç´¢å¼•

        valid_inputs = []
        valid_meta = []

        # ä½¿ç”¨ tqdm æ˜¾ç¤ºè¿›åº¦ï¼Œå› ä¸ºè‚¡ç¥¨æ•°é‡å¯èƒ½å¾ˆå¤§
        for k in range(len(unique_codes)):
            start_idx = code_indices[k]
            end_idx = code_indices[k + 1]

            curr_len = end_idx - start_idx

            # Check 1: é•¿åº¦ä¸è¶³
            if curr_len < self.seq_len:
                continue

            # Check 2: (Critical) é”šå®šæ£€æŸ¥
            # æœ€åä¸€ä¸ªæ•°æ®ç‚¹çš„æ—¶é—´æˆ³å¿…é¡»ä¸¥æ ¼ç­‰äº target_date
            # å¦‚æœä¸ç­‰äºï¼Œè¯´æ˜è¯¥è‚¡ç¥¨åœ¨ target_date åœç‰Œæˆ–æ•°æ®ç¼ºå¤±ï¼Œä¸å¯é¢„æµ‹
            last_date = dates[end_idx - 1]
            if last_date != np.datetime64(target_date):
                # Optional: è®°å½•æ—¥å¿— "Stock {unique_codes[k]} skipped: Last date {last_date} != {target_date}"
                continue

            # æ„é€ åˆ‡ç‰‡
            # å–æœ€å seq_len è¡Œ
            slice_start = end_idx - self.seq_len
            slice_end = end_idx

            seq = feat_vals[slice_start:slice_end]

            # Check 3: (Optional but Safe) æ£€æŸ¥ NaN
            # å¦‚æœè¾“å…¥åŒ…å« NaNï¼Œæ¨¡å‹è¾“å‡ºä¹Ÿä¼šæ˜¯ NaN
            if np.isnan(seq).any():
                continue

            valid_inputs.append(seq)
            valid_meta.append({'code': unique_codes[k]})

        return valid_inputs, valid_meta

    def _run_inference(self, inputs_list: List[np.ndarray]) -> np.ndarray:
        """æ‰¹é‡æ‰§è¡Œæ¨ç†ï¼Œä¼˜åŒ–æ˜¾å­˜ä½¿ç”¨"""
        batch_size = Config.ANALYSIS_BATCH_SIZE
        all_scores = []

        # å°† list è½¬ä¸º tensor å¹¶ä¸æ˜¯æœ€é«˜æ•ˆçš„ï¼Œå¦‚æœå†…å­˜å…è®¸ï¼Œå¯ä»¥é¢„åˆ†é…å¤§ Tensor
        # è¿™é‡Œä¸ºäº†é€šç”¨æ€§ä½¿ç”¨ batch è¿­ä»£
        total_samples = len(inputs_list)

        with torch.no_grad():
            for i in range(0, total_samples, batch_size):
                batch_data = inputs_list[i: i + batch_size]

                # è½¬æ¢ä¸º Tensor [Batch, Seq, Feat]
                tensor_batch = torch.tensor(np.array(batch_data), dtype=torch.float32).to(self.device)

                # Forward
                outputs = self.model(past_values=tensor_batch)

                # å¤„ç† logits
                logits = outputs.logits.squeeze()
                if logits.ndim == 0:
                    logits = logits.unsqueeze(0)

                all_scores.append(logits.cpu().numpy())

        if not all_scores:
            return np.array([])

        return np.concatenate(all_scores)


# --- æ¥å£æ¨¡æ‹Ÿä¸è°ƒç”¨ç¤ºä¾‹ ---

if __name__ == "__main__":
    # æ¨¡æ‹Ÿé…ç½® (å®é™…ä½¿ç”¨æ—¶ä¸éœ€è¦)
    # ç”¨æˆ·éœ€è¦è®¾ç½®çš„é¢„æµ‹æ—¥æœŸ (å³æ‹¥æœ‰å®Œæ•´æ”¶ç›˜æ•°æ®çš„æœ€åä¸€å¤©)
    TARGET_DATE = '2025-11-15'

    try:
        predictor = RealTimePredictor()

        # æ‰§è¡Œé¢„æµ‹
        df_rank = predictor.predict_next_day(TARGET_DATE)

        if not df_rank.empty:
            print("\n" + "=" * 50)
            print(f"ğŸš€ Top 10 Predictions for Next Trading Day (Based on {TARGET_DATE})")
            print("=" * 50)
            print(df_rank.head(10).to_markdown(index=False))

            # è¿™é‡Œçš„ Top 1 å°±æ˜¯æ¨¡å‹æœ€çœ‹å¥½çš„è‚¡ç¥¨
        else:
            print("No predictions generated.")

    except Exception as e:
        import traceback

        traceback.print_exc()