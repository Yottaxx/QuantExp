# main.py (v2) - unified CLI entry for the quant platform
from __future__ import annotations

import argparse
import importlib
import os
import sys
from typing import Any, Dict, Optional

from utils.logging_utils import get_logger, init_logger

from utils.utils_func import (
    apply_config_overrides,
    debug_print_config,
    ensure_dirs,
    parse_codes_arg,
    parse_kv_pairs,
    patch_dataprovider_defaults,
    setup_debug_mode,
)


def _import_src(name: str):
    """
    Import module from the src/ package.
    """
    return importlib.import_module(f"src.{name}")


def _set_seed(seed: int) -> None:
    """
    Prefer utils/seed_utils.set_global_seed; fall back to minimal seeding.
    """
    try:
        seed_mod = importlib.import_module("utils.seed_utils")
        if hasattr(seed_mod, "set_global_seed"):
            seed_mod.set_global_seed(seed)  # type: ignore[attr-defined]
            return
    except Exception:
        pass

    import random
    import numpy as np

    random.seed(seed)
    np.random.seed(seed)
    try:
        import torch  # type: ignore

        torch.manual_seed(seed)
        torch.cuda.manual_seed_all(seed)
    except Exception:
        pass


def build_parser() -> argparse.ArgumentParser:
    """
    Build the top-level CLI.

    å‘½ä»¤åˆ†å±‚ï¼š
      - config     : æŸ¥çœ‹é…ç½®
      - download   : ä¸‹è½½/æ›´æ–°åŸå§‹è¡Œæƒ…&è´¢åŠ¡æ•°æ®
      - panel      : æ„å»ºç‰¹å¾ panel ç¼“å­˜
      - train      : è®­ç»ƒæ·±åº¦å­¦ä¹ æ¨¡å‹
      - predict    : æœ€æ–°äº¤æ˜“æ—¥é€‰è‚¡
      - backtest   : å•æ¬¡å›æµ‹ï¼ˆå›ºå®šæ ‡çš„æˆ–åŸºäºå½“æ—¥é€‰è‚¡ï¼‰
      - walkforward: æ»šåŠ¨çª—å£ Walk-Forward å›æµ‹
      - analysis   : å†å²é¢„æµ‹ç”Ÿæˆ + å›æµ‹åˆ†æï¼ˆå¯è§†åŒ–ã€ç»Ÿè®¡ï¼‰
      - factor-eval: å› å­ IC/IR è¯„ä»·ï¼ˆéœ€è¦ evaluator.AlphaEvaluatorï¼‰
      - debug      : ç«¯åˆ°ç«¯ smoke testï¼ˆdata -> model -> inference -> backtestï¼‰
    """
    p = argparse.ArgumentParser(
        "Quant Platform",
        description="Institutional-grade A-share DL + multi-factor quant platform",
    )
    # global options
    p.add_argument(
        "--debug",
        action="store_true",
        help="Debug mode: å•è¿›ç¨‹/å°‘é‡æ ·æœ¬/ä¸²è¡Œ alphaï¼Œä¾¿äºæœ¬åœ°æ–­ç‚¹è°ƒè¯•",
    )
    p.add_argument(
        "--print-config",
        action="store_true",
        help="åœ¨æ‰§è¡Œå‰æ‰“å°æœ‰æ•ˆ Config",
    )
    p.add_argument(
        "--strict-config",
        action="store_true",
        help="é‡åˆ°æœªçŸ¥çš„ --set KEY æŠ¥é”™è€Œä¸æ˜¯é™é»˜å¿½ç•¥",
    )
    p.add_argument(
        "--set",
        action="append",
        default=[],
        help="è¦†ç›– Configï¼šKEY=VALï¼ˆå¯é‡å¤ï¼‰",
    )
    p.add_argument(
        "--seed",
        type=int,
        default=None,
        help="éšæœºç§å­è¦†ç›–ï¼ˆé»˜è®¤ä½¿ç”¨ Config.SEED æˆ– 42ï¼‰",
    )
    p.add_argument(
        "--exp-name",
        type=str,
        default=None,
        help="å®éªŒåç§°ï¼Œä¼šæ‹¼æ¥åˆ°æœ¬æ¬¡è¿è¡Œæ—¶é—´ç”Ÿæˆçš„ logger åç§°ä¸­",
    )

    sub = p.add_subparsers(dest="cmd", required=True)

    # 1) Config / data
    sub.add_parser("config", help="æ‰“å°æœ‰æ•ˆ Config å¹¶é€€å‡º")
    sub.add_parser("download", help="è¿è¡Œ DataProvider.download_data()")

    panel = sub.add_parser("panel", help="æ„å»º panel ç¼“å­˜ (load_and_process_panel)")
    panel.add_argument("--mode", default="train", choices=["train", "predict"])
    panel.add_argument("--adjust", default="qfq")
    panel.add_argument("--force-refresh", action="store_true")

    # 2) æ¨¡å‹è®­ç»ƒ / æ¨ç† / å›æµ‹
    sub.add_parser("train", help="è®­ç»ƒæ¨¡å‹ (run_training)")

    pred = sub.add_parser("predict", help="è¿è¡Œæ¨ç† (run_inference)")
    pred.add_argument("--top-k", type=int, default=None)
    pred.add_argument("--min-score", type=float, default=None)
    pred.add_argument(
        "--save-csv",
        type=str,
        default=None,
        help="å¦‚æŒ‡å®šè·¯å¾„ï¼Œåˆ™å°†é€‰è‚¡ç»“æœä¿å­˜ä¸º CSV",
    )

    bt = sub.add_parser(
        "backtest",
        help="å›æµ‹ (run_single_backtest æˆ– åŸºäºå½“æ—¥é€‰è‚¡çš„å›æµ‹)",
    )
    bt.add_argument(
        "--codes",
        default="",
        help="é€—å·åˆ†éš”çš„è‚¡ç¥¨ä»£ç åˆ—è¡¨ï¼›ä¸ºç©ºåˆ™ä½¿ç”¨ predict å¾—åˆ°çš„å½“æ—¥é€‰è‚¡",
    )
    bt.add_argument(
        "--with-fees",
        action="store_true",
        help="æ˜¯å¦å¯ç”¨ A è‚¡äº¤æ˜“è´¹ç”¨ï¼ˆä½£é‡‘+å°èŠ±ç¨ï¼‰",
    )
    bt.add_argument(
        "--initial-cash",
        type=float,
        default=1_000_000.0,
        help="åˆå§‹èµ„é‡‘",
    )
    bt.add_argument("--top-k", type=int, default=None)
    bt.add_argument("--min-score", type=float, default=None)

    wf = sub.add_parser("walkforward", help="Walk-forward å›æµ‹ (run_walk_forward_backtest)")
    wf.add_argument("--start", required=True, help="YYYY-MM-DD")
    wf.add_argument("--end", required=True, help="YYYY-MM-DD")
    wf.add_argument(
        "--initial-cash",
        type=float,
        default=1_000_000.0,
        help="åˆå§‹èµ„é‡‘",
    )
    wf.add_argument("--top-k", type=int, default=None)

    ana = sub.add_parser(
        "analysis",
        help="å†å²é¢„æµ‹ + å›æµ‹åˆ†æ (BacktestAnalyzer)",
        aliases=["test", "eval"],
    )
    ana.add_argument(
        "--target-set",
        default="test",
        choices=["test", "validation", "val", "eval", "train", "custom"],
    )
    ana.add_argument("--start", default=None)
    ana.add_argument("--end", default=None)

    fe = sub.add_parser("factor-eval", help="å› å­è¯„ä¼° (AlphaEvaluator)")
    fe.add_argument("--mode", default="train", choices=["train", "predict"])
    fe.add_argument("--adjust", default="qfq")
    fe.add_argument("--force-refresh", action="store_true")

    dbg = sub.add_parser(
        "debug",
        help="ç«¯åˆ°ç«¯ smoke testï¼španel -> inference -> backtestï¼ˆé…åˆ --debug æ›´é€‚åˆæœ¬åœ°ï¼‰",
    )
    dbg.add_argument("--top-k", type=int, default=None)
    dbg.add_argument("--min-score", type=float, default=None)
    dbg.add_argument(
        "--initial-cash",
        type=float,
        default=100_000.0,
        help="debug å›æµ‹åˆå§‹èµ„é‡‘",
    )

    return p


def apply_runtime_config(args: argparse.Namespace) -> Any:
    """
    ç»Ÿä¸€å¤„ç† Config è¦†ç›– / debug æ¨¡å¼ / éšæœºç§å­ / ç›®å½•åˆ›å»ºã€‚
    """
    # 1) å¯¼å…¥ Configï¼Œå¹¶åœ¨å…¶å®ƒæ¨¡å— import ä¹‹å‰å®Œæˆè¦†ç›–
    cfg_mod = _import_src("config")
    Config = cfg_mod.Config

    # 2) CLI è¦†ç›–
    overrides: Dict[str, Any] = parse_kv_pairs(args.set)
    apply_config_overrides(Config, overrides, strict=args.strict_config)

    # 2.1) æ—¥å¿—
    level = str(getattr(Config, "LOG_LEVEL", "INFO") or "INFO")
    exp_name = args.exp_name or getattr(Config, "EXPERIMENT_NAME", "default")
    global _RUN_LOGGER
    _RUN_LOGGER = init_logger(exp_name, level=level)
    log = _RUN_LOGGER

    # 3) debug profile
    if args.debug:
        setup_debug_mode(Config)

    # 4) éšæœºç§å­
    seed = args.seed if args.seed is not None else getattr(Config, "SEED", 42)
    _set_seed(int(seed))

    # 5) ç¡®ä¿ç›®å½•å­˜åœ¨
    ensure_dirs(Config)

    # 6) æ‰“å° Config æ¦‚è§ˆ
    if args.print_config or args.cmd == "config" or args.debug:
        debug_print_config(Config, logger=log)
        # DataProvider å¯èƒ½æ²¡æœ‰ debug_print_configï¼Œæ–°ç‰ˆä»…æ‰“å° VERSION å³å¯
        try:
            dp_mod = _import_src("data_provider")
            DP = getattr(dp_mod, "DataProvider", None)
            if DP is not None and hasattr(DP, "VERSION"):
                log.info(f"[DataProvider] VERSION = {getattr(DP, 'VERSION')}")
        except Exception as e:  # noqa: BLE001
            log.warning(f"[warn] DataProvider introspection failed: {e}")

    return Config


def cmd_download() -> None:
    log = _get_run_logger()
    dp_mod = _import_src("data_provider")
    dp_mod.DataProvider.download_data()


def cmd_panel(mode: str, adjust: str, force_refresh: bool) -> None:
    log = _get_run_logger()
    dp_mod = _import_src("data_provider")
    panel_df, feature_cols = dp_mod.DataProvider.load_and_process_panel(
        mode=mode,
        adjust=adjust,
        force_refresh=force_refresh,
    )
    log.info(
        f"âœ… Panel ready: shape={panel_df.shape}, features={len(feature_cols)} (mode={mode}, adjust={adjust})"
    )


def cmd_train() -> None:
    log = _get_run_logger()
    tr_mod = _import_src("train")
    tr_mod.run_training()


def cmd_predict(
    top_k: Optional[int],
    min_score: Optional[float],
    save_csv: Optional[str] = None,
):
    log = _get_run_logger()
    cfg_mod = _import_src("config")
    Config = cfg_mod.Config

    inf_mod = _import_src("inference")
    k = int(top_k if top_k is not None else Config.TOP_K)
    thr = float(min_score if min_score is not None else Config.MIN_SCORE_THRESHOLD)

    picks = inf_mod.run_inference(top_k=k, min_score_threshold=thr)

    if save_csv:
        import pandas as pd

        df = pd.DataFrame(picks, columns=["code", "score", "pe"])
        os.makedirs(os.path.dirname(save_csv) or ".", exist_ok=True)
        df.to_csv(save_csv, index=False, encoding="utf-8-sig")
        log.info(f"ğŸ’¾ Picks saved to {save_csv}")

    return picks


def cmd_backtest(
    codes: str,
    with_fees: bool,
    initial_cash: float,
    top_k: Optional[int],
    min_score: Optional[float],
) -> None:
    log = _get_run_logger()
    cfg_mod = _import_src("config")
    Config = cfg_mod.Config
    bt_mod = _import_src("backtest")

    code_list = parse_codes_arg(codes)
    if not code_list:
        picks = cmd_predict(top_k=top_k, min_score=min_score, save_csv=None)
        code_list = [c for (c, *_rest) in picks]

    if not code_list:
        log.warning("âš ï¸ No codes to backtest, exiting.")
        return

    k = int(top_k if top_k is not None else Config.TOP_K)
    bt_mod.run_single_backtest(
        code_list,
        with_fees=with_fees,
        initial_cash=initial_cash,
        top_k=k,
    )


def cmd_walkforward(start: str, end: str, initial_cash: float, top_k: Optional[int]) -> None:
    cfg_mod = _import_src("config")
    Config = cfg_mod.Config
    bt_mod = _import_src("backtest")

    k = int(top_k if top_k is not None else Config.TOP_K)
    bt_mod.run_walk_forward_backtest(start, end, initial_cash, top_k=k)


def cmd_analysis(target_set: str, start: Optional[str], end: Optional[str]) -> None:
    log = _get_run_logger()
    an_mod = _import_src("analysis")
    analyzer = an_mod.BacktestAnalyzer(
        target_set=target_set,
        start_date=start,
        end_date=end,
    )
    analyzer.generate_historical_predictions()
    analyzer.analyze_performance()


def cmd_factor_eval(mode: str, adjust: str, force_refresh: bool) -> None:
    dp_mod = _import_src("data_provider")
    ev_mod = _import_src("evaluator")

    panel_df, feat_cols = dp_mod.DataProvider.load_and_process_panel(
        mode=mode,
        adjust=adjust,
        force_refresh=force_refresh,
    )
    valid = ev_mod.AlphaEvaluator.evaluate(panel_df, feat_cols, target_col="target")
    log.info(f"âœ… Valid factors: {len(valid)}/{len(feat_cols)}")


def cmd_debug(
    top_k: Optional[int],
    min_score: Optional[float],
    initial_cash: float,
) -> None:
    """
    ç«¯åˆ°ç«¯ smoke testï¼Œæ–¹ä¾¿æ£€æŸ¥æ•´ä¸ªæµæ°´çº¿æ˜¯å¦é—­ç¯å¯è·‘ï¼š
      DataProvider -> SignalEngine / inference -> Backtrader
    """
    cfg_mod = _import_src("config")
    Config = cfg_mod.Config

    log = _get_run_logger()
    log.info("=" * 80)
    log.info("ğŸ§ª DEBUG PIPELINE: panel -> inference -> backtest")
    log.info("=" * 80)

    # Step 1: å°è¯•æ„å»ºä¸€ä»½ train panelï¼ˆå¦‚æœ DataProvider æ”¯æŒ debug å‚æ•°ï¼Œä¼šç”± main ç»Ÿä¸€æ‰“è¡¥ä¸ï¼‰
    try:
        dp_mod = _import_src("data_provider")
        panel_df, feature_cols = dp_mod.DataProvider.load_and_process_panel(mode="train")
        log.info(
            f"[1/3] panel_df.shape={panel_df.shape}, features={len(feature_cols)}  âœ…",
        )
    except Exception as e:  # noqa: BLE001
        log.error(f"[1/3] âŒ DataProvider.load_and_process_panel failed: {e}")
        return

    # Step 2: è¿è¡Œä¸€æ¬¡ inferenceï¼ˆé€šå¸¸æ˜¯æœ€è¿‘äº¤æ˜“æ—¥ï¼‰
    try:
        k = int(top_k if top_k is not None else Config.TOP_K)
        thr = float(min_score if min_score is not None else Config.MIN_SCORE_THRESHOLD)
        inf_mod = _import_src("inference")
        picks = inf_mod.run_inference(top_k=k, min_score_threshold=thr)
        log.info(f"[2/3] inference picks={len(picks)}  âœ…")
    except Exception as e:  # noqa: BLE001
        log.error(f"[2/3] âŒ run_inference failed: {e}")
        return

    if not picks:
        log.warning("[2/3] âš ï¸ No picks returned; skip backtest.")
        return

    # Step 3: å¯¹è¿™æ‰¹æ ‡çš„åšä¸€è½®å¿«é€Ÿå›æµ‹ï¼ˆä¸å«è´¹ç”¨ï¼‰
    try:
        codes = [c for (c, *_rest) in picks]
        bt_mod = _import_src("backtest")
        k_bt = min(len(codes), int(getattr(Config, "TOP_K", len(codes))))
        log.info(
            f"[3/3] Running debug backtest on {k_bt} codes, initial_cash={initial_cash:.0f} ...",
        )
        bt_mod.run_single_backtest(
            codes,
            with_fees=False,
            initial_cash=initial_cash,
            top_k=k_bt,
        )
        log.info("[3/3] debug backtest finished  âœ…")
    except Exception as e:  # noqa: BLE001
        log.error(f"[3/3] âŒ debug backtest failed: {e}")


def main(argv=None) -> None:
    parser = build_parser()
    args = parser.parse_args(argv)

    # å…ˆå¤„ç† Config / éšæœºç§å­ / ç›®å½•ç­‰
    Config = apply_runtime_config(args)

    # å¯¹ DataProvider.load_and_process_panel æ‰“è¡¥ä¸ï¼Œè®©å†…éƒ¨è°ƒç”¨ä¹Ÿèƒ½ç»§æ‰¿ CLI é€‰æ‹©
    try:
        dp_mod = _import_src("data_provider")
        patch_dataprovider_defaults(
            dp_mod.DataProvider,
            adjust=getattr(args, "adjust", None),  # panel/factor-eval æä¾›
            force_refresh=getattr(args, "force_refresh", None),
            debug_flag=getattr(args, "debug", False),
        )
    except Exception as e:  # noqa: BLE001
        _get_run_logger().warning(f"[warn] patch_dataprovider_defaults failed: {e}")

    # æ ¹æ®å­å‘½ä»¤åˆ†å‘é€»è¾‘
    if args.cmd == "config":
        # å·²åœ¨ apply_runtime_config ä¸­æ‰“å° Configï¼Œè¿™é‡Œç›´æ¥é€€å‡º
        return
    if args.cmd == "download":
        cmd_download()
        return
    if args.cmd == "panel":
        cmd_panel(args.mode, args.adjust, args.force_refresh)
        return
    if args.cmd == "train":
        cmd_train()
        return
    if args.cmd == "predict":
        cmd_predict(args.top_k, args.min_score, args.save_csv)
        return
    if args.cmd == "backtest":
        cmd_backtest(args.codes, args.with_fees, args.initial_cash, args.top_k, args.min_score)
        return
    if args.cmd == "walkforward":
        cmd_walkforward(args.start, args.end, args.initial_cash, args.top_k)
        return
    if args.cmd in {"analysis", "test", "eval"}:
        cmd_analysis(args.target_set, args.start, args.end)
        return
    if args.cmd == "factor-eval":
        cmd_factor_eval(args.mode, args.adjust, args.force_refresh)
        return
    if args.cmd == "debug":
        cmd_debug(args.top_k, args.min_score, args.initial_cash)
        return

    raise RuntimeError(f"Unknown cmd: {args.cmd}")


if __name__ == "__main__":
    # Ensure project root is on sys.path (usually already when `python main.py`)
    root = os.path.dirname(os.path.abspath(__file__))
    if root not in sys.path:
        sys.path.insert(0, root)
    main()

_RUN_LOGGER = None


def _get_run_logger():
    global _RUN_LOGGER
    if _RUN_LOGGER is None:
        _RUN_LOGGER = get_logger()
    return _RUN_LOGGER
