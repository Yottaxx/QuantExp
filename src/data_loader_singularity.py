import akshare as ak
import pandas as pd
import os
import glob
import numpy as np
import time
import random
import requests
import threading
import datetime
import concurrent.futures
from datasets import Dataset
from sklearn.preprocessing import StandardScaler
from tqdm import tqdm
from .config import Config
from .vpn_rotator import vpn_rotator


class DataProvider:
    _vpn_lock = threading.Lock()
    _last_switch_time = 0

    # ==============================================================================
    #   é…ç½®åŒºï¼šæ•°æ®ç²’åº¦
    #   å¯é€‰å€¼: 'daily' (æ—¥çº¿), '1' (1åˆ†é’Ÿ), '5' (5åˆ†é’Ÿ), '15', '30', '60'
    # ==============================================================================
    DATA_PERIOD = '5'  # <--- ä¿®æ”¹è¿™é‡Œæ¥æ”¹å˜ç²’åº¦

    @staticmethod
    def _setup_proxy_env():
        """è®¾ç½®ä»£ç†ç¯å¢ƒ"""
        proxy_url = "http://127.0.0.1:7890"
        os.environ['http_proxy'] = proxy_url
        os.environ['https_proxy'] = proxy_url
        os.environ['all_proxy'] = proxy_url
        os.environ['HTTP_PROXY'] = proxy_url
        os.environ['HTTPS_PROXY'] = proxy_url
        os.environ['ALL_PROXY'] = proxy_url

    @classmethod
    def _safe_switch_vpn(cls):
        with cls._vpn_lock:
            if time.time() - cls._last_switch_time < 5:
                return
            vpn_rotator.switch_random()
            cls._last_switch_time = time.time()
            time.sleep(2)

    @staticmethod
    def _download_worker(code):
        """
        é€šç”¨ä¸‹è½½å•å…ƒ (æ”¯æŒ æ—¥çº¿/åˆ†é’Ÿçº¿ è‡ªåŠ¨åˆ‡æ¢)
        """
        # æ ¹æ®ç²’åº¦åŒºåˆ†æ–‡ä»¶åï¼Œé¿å…è¦†ç›–
        # ä¾‹å¦‚: 000001_daily.parquet æˆ– 000001_5m.parquet
        suffix = "daily" if DataProvider.DATA_PERIOD == 'daily' else f"{DataProvider.DATA_PERIOD}m"
        path = os.path.join(Config.DATA_DIR, f"{code}_{suffix}.parquet")

        for attempt in range(5):
            try:
                time.sleep(random.uniform(0.05, 0.2))

                df = None

                # --- åˆ†æ”¯ 1: ä¸‹è½½æ—¥çº¿æ•°æ® ---
                if DataProvider.DATA_PERIOD == 'daily':
                    df = ak.stock_zh_a_hist(
                        symbol=code,
                        period="daily",
                        start_date=Config.START_DATE,
                        adjust="qfq"
                    )
                    if df is not None and not df.empty:
                        df.rename(columns={'æ—¥æœŸ': 'date', 'å¼€ç›˜': 'open', 'æ”¶ç›˜': 'close',
                                           'æœ€é«˜': 'high', 'æœ€ä½': 'low', 'æˆäº¤é‡': 'volume'}, inplace=True)

                # --- åˆ†æ”¯ 2: ä¸‹è½½åˆ†é’Ÿçº§æ•°æ® ---
                else:
                    # åˆ†é’Ÿçº¿æ¥å£: period å¯é€‰ '1', '5', '15', '30', '60'
                    df = ak.stock_zh_a_hist_min_em(
                        symbol=code,
                        start_date=f"{Config.START_DATE} 09:00:00",  # æ ¼å¼å…¼å®¹
                        period=DataProvider.DATA_PERIOD,
                        adjust="qfq"
                    )
                    if df is not None and not df.empty:
                        # åˆ†é’Ÿçº¿åˆ—åé€šå¸¸æ˜¯ 'æ—¶é—´', 'å¼€ç›˜', ...
                        df.rename(columns={'æ—¶é—´': 'date', 'å¼€ç›˜': 'open', 'æ”¶ç›˜': 'close',
                                           'æœ€é«˜': 'high', 'æœ€ä½': 'low', 'æˆäº¤é‡': 'volume'}, inplace=True)

                if df is None or df.empty:
                    return code, True, "Empty"

                # ç»Ÿä¸€å¤„ç†ç´¢å¼•
                df['date'] = pd.to_datetime(df['date'])
                df.set_index('date', inplace=True)

                # å­˜ç›˜
                if len(df) > 0:
                    df.to_parquet(path)

                return code, True, "Success"

            except Exception as e:
                # print(f"Err {code}: {e}")
                DataProvider._safe_switch_vpn()
                continue

        return code, False, "Failed"

    @staticmethod
    def download_data():
        """ä¸‹è½½å…¥å£"""
        print(f">>> [Phase 1] åˆå§‹åŒ–ä¸‹è½½å¼•æ“ (ç²’åº¦: {DataProvider.DATA_PERIOD})...")
        DataProvider._setup_proxy_env()

        # è·å–è‚¡ç¥¨åˆ—è¡¨
        codes = []
        for _ in range(5):
            try:
                stock_info = ak.stock_zh_a_spot_em()
                codes = stock_info['ä»£ç '].tolist()
                break
            except:
                vpn_rotator.switch_random()
                time.sleep(2)

        if not codes:
            print("âŒ æ— æ³•è·å–è‚¡ç¥¨åˆ—è¡¨")
            return

        if not os.path.exists(Config.DATA_DIR):
            os.makedirs(Config.DATA_DIR)

        # æ™ºèƒ½æ–­ç‚¹ç»­ä¼  (æ ¹æ®å½“å‰ç²’åº¦åç¼€è¿‡æ»¤)
        suffix = "daily" if DataProvider.DATA_PERIOD == 'daily' else f"{DataProvider.DATA_PERIOD}m"
        print(f">>> æ‰«ææœ¬åœ°å·²ä¸‹è½½æ•°æ® (åç¼€: _{suffix}.parquet)...")

        files = os.listdir(Config.DATA_DIR)
        # åªæ£€æŸ¥å½“å‰ç²’åº¦çš„æ–‡ä»¶
        existing_codes = {
            f.split('_')[0] for f in files
            if f.endswith(f"_{suffix}.parquet") and os.path.getsize(os.path.join(Config.DATA_DIR, f)) > 1024
        }

        all_codes_set = set(codes)
        todo_codes = list(all_codes_set - existing_codes)
        todo_codes.sort()

        print(f"ğŸ“Š ä»»åŠ¡ç»Ÿè®¡: æ€»æ•° {len(codes)} | å·²å®Œæˆ {len(existing_codes)} | å¾…ä¸‹è½½ {len(todo_codes)}")

        if not todo_codes:
            print("âœ… å½“å‰ç²’åº¦æ•°æ®å·²å…¨éƒ¨ä¸‹è½½å®Œæ¯•ã€‚")
            return

        MAX_WORKERS = 8  # åˆ†é’Ÿçº¿æ•°æ®é‡å¤§ï¼Œå»ºè®®é™ä½å¹¶å‘æ•°é˜²æ­¢å†…å­˜æº¢å‡ºæˆ–å°é”è¿‡å¿«
        print(f"ğŸš€ å¯åŠ¨ {MAX_WORKERS} çº¿ç¨‹å¹¶å‘ä¸‹è½½ (åˆ†é’Ÿçº¿é€Ÿåº¦è¾ƒæ…¢è¯·è€å¿ƒç­‰å¾…)...")

        failed_codes = []
        with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
            future_to_code = {executor.submit(DataProvider._download_worker, code): code for code in todo_codes}
            progress_bar = tqdm(concurrent.futures.as_completed(future_to_code), total=len(todo_codes), unit="it")

            for future in progress_bar:
                code = future_to_code[future]
                try:
                    _, is_success, _ = future.result()
                    if not is_success:
                        failed_codes.append(code)
                except:
                    failed_codes.append(code)

        print(f"ä¸‹è½½ç»“æŸã€‚å¤±è´¥æ•°: {len(failed_codes)}")

    # ==============================================================================
    #   Phase 2: æ•°æ®å¤„ç†
    # ==============================================================================

    @staticmethod
    def process_single_stock(df):
        from .alpha_lib import AlphaFactory
        # åˆ†é’Ÿçº§é¢„æµ‹é€šå¸¸é¢„æµ‹æœªæ¥ N ä¸ª Barï¼Œæ¯”å¦‚æœªæ¥ 12 ä¸ª 5åˆ†é’Ÿ(1å°æ—¶)
        df['target'] = df['close'].shift(-Config.PRED_LEN) / df['close'] - 1
        factory = AlphaFactory(df)
        df = factory.make_factors()
        factor_cols = [c for c in df.columns if c.startswith('alpha_')]
        keep_cols = factor_cols + ['target']
        df.dropna(subset=keep_cols, inplace=True)
        return df, factor_cols

    def generator(self):
        # è¿™é‡Œä¹Ÿè¦é€‚é…æ–‡ä»¶å
        suffix = "daily" if DataProvider.DATA_PERIOD == 'daily' else f"{DataProvider.DATA_PERIOD}m"
        pattern = f"*_{suffix}.parquet"

        files = glob.glob(os.path.join(Config.DATA_DIR, pattern))
        target_files = files

        for fpath in target_files:
            try:
                df = pd.read_parquet(fpath)
                if len(df) < 100: continue

                df_proc, factor_cols = self.process_single_stock(df)

                scaler = StandardScaler()
                x_data = scaler.fit_transform(df_proc[factor_cols].values)
                y_data = df_proc['target'].values
                for i in range(0, len(x_data) - Config.CONTEXT_LEN, 5):
                    yield {
                        "past_values": x_data[i: i + Config.CONTEXT_LEN].astype(np.float32),
                        "labels": y_data[i + Config.CONTEXT_LEN - 1].astype(np.float32)
                    }
            except:
                continue
