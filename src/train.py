import torch
import numpy as np
from transformers import Trainer, TrainingArguments, EarlyStoppingCallback
from scipy.stats import spearmanr
from .config import Config
from .model import PatchTSTForStock, SotaConfig
from .data_provider import get_dataset
import os
from transformers import default_data_collator
def compute_metrics(eval_pred):
    """
    è®¡ç®— Validation é›†æŒ‡æ ‡ (ç”¨äº Early Stopping)
    """
    predictions, labels = eval_pred
    if isinstance(predictions, tuple):
        predictions = predictions[0]

    preds = predictions.flatten()
    labs = labels.flatten()
    preds = np.nan_to_num(preds)
    labs = np.nan_to_num(labs)

    ic, p_value = spearmanr(preds, labs)
    return {"ic": ic}


def run_training():
    print("\n" + "=" * 60)
    print(">>> å¯åŠ¨æ¨¡å‹è®­ç»ƒ (Train / Validation Split)")
    print("=" * 60)

    # è·å–åŒ…å« train, validation, test çš„æ•°æ®é›†
    ds, num_features = get_dataset()

    print(f"Feature Dim: {num_features}")
    # [Check] ç¡®ä¿åªä½¿ç”¨ Train å’Œ Validation
    print(f"Training on: {len(ds['train'])} samples")
    print(f"Evaluating on: {len(ds['validation'])} samples (Early Stopping)")
    print(f"Held-out Test: {len(ds['test'])} samples (Ignored during training)")


    model_config = SotaConfig(
        num_input_channels=num_features,
        context_length=Config.CONTEXT_LEN,
        patch_length=Config.PATCH_LEN,
        stride=Config.STRIDE,
        d_model=Config.D_MODEL,
        num_hidden_layers=3,
        n_heads=4,
        dropout=Config.DROPOUT,
        mse_weight=Config.MSE_WEIGHT,
        rank_weight=getattr(Config, "RANK_WEIGHT", 1.0)  # å‘åå…¼å®¹
    )

    model = PatchTSTForStock(model_config)

    training_args = TrainingArguments(
        output_dir=Config.OUTPUT_DIR,
        overwrite_output_dir=True,
        num_train_epochs=Config.EPOCHS,
        per_device_train_batch_size=Config.BATCH_SIZE,
        per_device_eval_batch_size=Config.INFERENCE_BATCH_SIZE,

        # --- [å…³é”®ä¼˜åŒ–ï¼šé…åˆ DataProvider çš„ Lazy Mapping] ---
        # 1. å¼€å¯å¤š worker åŠ é€Ÿ IO
        #    Linux ä¸‹å»ºè®®è®¾ä¸º CPU æ ¸æ•°çš„ä¸€åŠï¼Œæˆ– 4-8
        #    Windows ä¸‹å»ºè®®ä¿æŒ 0 (å› ä¸ºæ²¡æœ‰ fork æœºåˆ¶ï¼Œå¤šè¿›ç¨‹ä¼šå¤åˆ¶å†…å­˜å¯¼è‡´çˆ†ç‚¸)
        dataloader_num_workers=0 if os.name != 'nt' else 0,

        # 2. é”é¡µå†…å­˜ï¼ŒåŠ é€Ÿ CPU -> GPU ä¼ è¾“
        dataloader_pin_memory=True,

        # 3. ä¿æŒ worker è¿›ç¨‹å­˜æ´»ï¼Œé¿å…æ¯ä¸ª Epoch é‡æ–°åˆ›å»ºè¿›ç¨‹çš„å¼€é”€
        dataloader_persistent_workers=True if os.name != 'nt' else False,
        # ----------------------------------------------------

        learning_rate=Config.LR,
        weight_decay=1e-4,
        max_grad_norm=Config.MAX_GRAD_NORM,

        eval_strategy="steps",
        eval_steps=500,  # å»ºè®®è°ƒå°ä¸€ç‚¹ï¼Œè§‚å¯ŸéªŒè¯é›†æ”¶æ•›
        save_steps=1000,
        save_total_limit=2,

        logging_steps=50,
        fp16=torch.cuda.is_available(),

        # ç¦ç”¨ HF é»˜è®¤çš„ remove_unused_columnsï¼Œ
        # å› ä¸ºæˆ‘ä»¬çš„ Dataset æ˜¯åŠ¨æ€ç”Ÿæˆçš„ï¼Œæ²¡æœ‰ç‰©ç†åˆ—ï¼Œé˜²æ­¢ HF è¯¯åˆ 
        remove_unused_columns=False,

        load_best_model_at_end=True,
        metric_for_best_model="ic",
        greater_is_better=True,
        report_to="none"
    )

    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=ds['train'],  # è®­ç»ƒé›†
        eval_dataset=ds['validation'],  # éªŒè¯é›† (Eval Set)
        compute_metrics=compute_metrics,
        callbacks=[EarlyStoppingCallback(early_stopping_patience=5)]
    )

    print(f"ğŸš€ å¼€å§‹è®­ç»ƒ...")
    trainer.train()

    final_path = f"{Config.OUTPUT_DIR}/final_model"
    trainer.save_model(final_path)
    print(f"âœ… æ¨¡å‹å·²ä¿å­˜: {final_path}")