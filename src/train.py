import torch
import numpy as np
from transformers import Trainer, TrainingArguments, EarlyStoppingCallback
from scipy.stats import spearmanr
from .config import Config
from .model import PatchTSTForStock, SotaConfig
from .data_provider import get_dataset


def compute_metrics(eval_pred):
    """
    ã€æ–°å¢ã€‘è‡ªå®šä¹‰è¯„ä¼°å‡½æ•°
    åœ¨éªŒè¯é›†ä¸Šè®¡ç®— IC (Information Coefficient)
    """
    predictions, labels = eval_pred
    # predictions shape: [Batch, 1] or [Batch]
    # labels shape: [Batch]

    preds = predictions.flatten()
    labs = labels.flatten()

    # è®¡ç®— Spearman Rank IC
    ic, _ = spearmanr(preds, labs)

    # è®¡ç®— Pearson IC
    # pearson_ic = np.corrcoef(preds, labs)[0, 1]

    return {
        "ic": ic,
        # "pearson_ic": pearson_ic
    }


def run_training():
    print("\n" + "=" * 50)
    print(">>> å¯åŠ¨æ¨¡å‹è®­ç»ƒ (Training Pipeline)")
    print("=" * 50)

    # 1. è·å–æ•°æ® (è‡ªåŠ¨è°ƒç”¨å…¨å†…å­˜åŠ è½½ + ç¼“å­˜)
    # ds åŒ…å« {'train': ..., 'test': ...}
    ds, num_features = get_dataset()

    print(f"Input Features: {num_features}")
    print(f"Train Size: {len(ds['train'])} | Test Size: {len(ds['test'])}")

    # 2. é…ç½®æ¨¡å‹
    model_config = SotaConfig(
        num_input_channels=num_features,
        context_length=Config.CONTEXT_LEN,
        patch_length=8,  # PatchTST æ ¸å¿ƒå‚æ•°
        stride=4,  # Patch æ­¥é•¿
        d_model=128,  # éšå±‚ç»´åº¦
        num_hidden_layers=3,  # å±‚æ•°
        n_heads=4,
        dropout=0.2
    )

    model = PatchTSTForStock(model_config)

    # 3. è®­ç»ƒå‚æ•° (å·¥ä¸šçº§é…ç½®)
    training_args = TrainingArguments(
        output_dir=Config.OUTPUT_DIR,
        overwrite_output_dir=True,
        num_train_epochs=10,  # é€‚å½“å¢åŠ  Epochï¼Œå› ä¸ºæœ‰ EarlyStopping
        per_device_train_batch_size=64,
        per_device_eval_batch_size=256,  # éªŒè¯é›† Batch å¯ä»¥å¤§ä¸€ç‚¹

        learning_rate=1e-4,
        weight_decay=1e-4,  # L2 æ­£åˆ™åŒ–

        evaluation_strategy="steps",  # æŒ‰æ­¥æ•°è¯„ä¼°
        eval_steps=200,  # æ¯ 200 æ­¥éªŒè¯ä¸€æ¬¡
        save_steps=200,  # æ¯ 200 æ­¥ä¿å­˜ä¸€æ¬¡ Checkpoint
        save_total_limit=3,  # æœ€å¤šä¿ç•™ 3 ä¸ª Checkpoint

        logging_steps=50,
        fp16=True,  # å¼€å¯æ··åˆç²¾åº¦åŠ é€Ÿ
        dataloader_num_workers=0,  # Windows/Mac æœ‰æ—¶å¤šè¿›ç¨‹ä¼šæŠ¥é”™ï¼Œè®¾ä¸º0æœ€ç¨³ï¼ŒLinuxå¯è®¾ä¸º4

        load_best_model_at_end=True,  # è®­ç»ƒç»“æŸååŠ è½½æœ€å¥½çš„æ¨¡å‹
        metric_for_best_model="ic",  # ã€å…³é”®ã€‘ä»¥ IC ä½œä¸ºæœ€ä¼˜æ¨¡å‹çš„è¯„åˆ¤æ ‡å‡†
        greater_is_better=True,  # IC è¶Šå¤§è¶Šå¥½

        remove_unused_columns=False,  # é˜²æ­¢ feature åˆ—è¢«è‡ªåŠ¨è¿‡æ»¤
        report_to="none"  # ä¸ä¸Šä¼  WandB
    )

    # 4. åˆå§‹åŒ– Trainer
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=ds['train'],
        eval_dataset=ds['test'],
        compute_metrics=compute_metrics,  # æŒ‚è½½è‡ªå®šä¹‰è¯„ä¼°æŒ‡æ ‡
        callbacks=[EarlyStoppingCallback(early_stopping_patience=5)]  # è¿ç»­5æ¬¡ IC ä¸æå‡åˆ™åœæ­¢
    )

    # 5. å¼€å§‹è®­ç»ƒ
    print("ğŸš€ å¼€å§‹è®­ç»ƒ...")
    trainer.train()

    # 6. ä¿å­˜æœ€ç»ˆæ¨¡å‹
    final_path = f"{Config.OUTPUT_DIR}/final_model"
    trainer.save_model(final_path)
    print(f"âœ… æ¨¡å‹è®­ç»ƒå®Œæˆï¼Œå·²ä¿å­˜è‡³: {final_path}")