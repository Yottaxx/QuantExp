import torch
import numpy as np
from transformers import Trainer, TrainingArguments, EarlyStoppingCallback
from scipy.stats import spearmanr
from .config import Config
from .model import PatchTSTForStock, SotaConfig
from .data_provider import get_dataset


def compute_metrics(eval_pred):
    """
    è®¡ç®—éªŒè¯é›†æŒ‡æ ‡
    æ³¨æ„ï¼šåœ¨ HF Trainer ä¸­ç›´æ¥è®¡ç®— Daily Rank IC æ¯”è¾ƒå›°éš¾ï¼ˆç¼ºå¤± Date ä¿¡æ¯ï¼‰ã€‚
    è¿™é‡Œä½¿ç”¨ Flatten åçš„ Spearman IC ä½œä¸ºè¿‘ä¼¼ä»£ç†ï¼Œ
    æ›´ä¸¥è°¨çš„ Daily IC ä¼šåœ¨ Analysis é˜¶æ®µé€šè¿‡ analysis.py è®¡ç®—ã€‚
    """
    predictions, labels = eval_pred
    # ç¡®ä¿æ˜¯ Numpy æ•°ç»„
    if isinstance(predictions, tuple):
        predictions = predictions[0]

    preds = predictions.flatten()
    labs = labels.flatten()

    # å¼‚å¸¸å€¼å¤„ç† (é˜²æ­¢ NaN å¯¼è‡´ crash)
    preds = np.nan_to_num(preds)
    labs = np.nan_to_num(labs)

    # è®¡ç®— IC
    ic, p_value = spearmanr(preds, labs)
    return {"ic": ic}


def run_training():
    print("\n" + "=" * 60)
    print(">>> å¯åŠ¨æ¨¡å‹è®­ç»ƒ (Training Pipeline)")
    print(f">>> Device: {Config.DEVICE}")
    print("=" * 60)

    ds, num_features = get_dataset()

    print(f"Feature Dim: {num_features}")
    print(f"Train Samples: {len(ds['train'])} | Test Samples: {len(ds['test'])}")

    # 2. é…ç½®æ¨¡å‹
    model_config = SotaConfig(
        num_input_channels=num_features,
        context_length=Config.CONTEXT_LEN,
        patch_length=Config.PATCH_LEN,
        stride=Config.STRIDE,
        d_model=128,
        num_hidden_layers=3,
        n_heads=4,
        dropout=Config.DROPOUT,
        mse_weight=Config.MSE_WEIGHT
    )

    model = PatchTSTForStock(model_config)

    # 3. è®­ç»ƒå‚æ•° (Production Grade)
    training_args = TrainingArguments(
        output_dir=Config.OUTPUT_DIR,
        overwrite_output_dir=True,
        num_train_epochs=Config.EPOCHS,
        per_device_train_batch_size=Config.BATCH_SIZE,
        per_device_eval_batch_size=Config.INFERENCE_BATCH_SIZE,

        # ä¼˜åŒ–å™¨é…ç½®
        learning_rate=Config.LR,
        weight_decay=1e-4,
        max_grad_norm=Config.MAX_GRAD_NORM,  # æ¢¯åº¦è£å‰ª

        # è¯„ä¼°ç­–ç•¥
        evaluation_strategy="steps",
        eval_steps=200,
        save_steps=200,
        save_total_limit=2,  # åªä¿ç•™æœ€è¿‘2ä¸ªCheckpointï¼ŒèŠ‚çœç©ºé—´

        logging_steps=50,
        fp16=torch.cuda.is_available(),  # è‡ªåŠ¨å¼€å¯æ··åˆç²¾åº¦
        dataloader_num_workers=0,  # é¿å…å¤šè¿›ç¨‹æ­»é” (ç‰¹åˆ«æ˜¯ DataLoader åœ¨ AkShare ç¯å¢ƒä¸‹)

        load_best_model_at_end=True,
        metric_for_best_model="ic",
        greater_is_better=True,

        report_to="none"
    )

    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=ds['train'],
        eval_dataset=ds['test'],
        compute_metrics=compute_metrics,
        callbacks=[EarlyStoppingCallback(early_stopping_patience=5)]
    )

    print(f"ğŸš€ å¼€å§‹è®­ç»ƒ (Epochs={Config.EPOCHS}, Batch={Config.BATCH_SIZE})...")
    trainer.train()

    final_path = f"{Config.OUTPUT_DIR}/final_model"
    trainer.save_model(final_path)
    print(f"âœ… æ¨¡å‹è®­ç»ƒå®Œæˆï¼Œå·²ä¿å­˜è‡³: {final_path}")