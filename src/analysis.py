import torch
import pandas as pd
import numpy as np
import os
import matplotlib.pyplot as plt
from tqdm import tqdm
from .config import Config
from .model import PatchTSTForStock
from .data_provider import DataProvider

plt.rcParams['font.sans-serif'] = ['Arial Unicode MS', 'SimHei', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False


class BacktestAnalyzer:
    def __init__(self, use_test_set_only=True):
        """
        :param use_test_set_only: å¦‚æœä¸º Trueï¼Œè‡ªåŠ¨è¦†ç›– start_date ä¸ºæµ‹è¯•é›†èµ·å§‹æ—¥
        """
        self.device = Config.DEVICE
        # self.model_path = f"{Config.OUTPUT_DIR}/final_model"
        self.model_path= "/Users/yotta/PycharmProjects/QuantExp/output/checkpoints/checkpoint-3000"

        self.results_df = None
        self.use_test_set_only = use_test_set_only

        # é»˜è®¤å…ˆå ä½ï¼Œç¨ååœ¨åŠ è½½æ•°æ®æ—¶åŠ¨æ€ä¿®æ­£
        self.start_date = pd.to_datetime(Config.START_DATE)
        self.end_date = pd.to_datetime("2099-12-31")

    def generate_historical_predictions(self):
        print("\n" + "=" * 60)
        print(">>> [Analysis] å¯åŠ¨å…¨é‡æˆªé¢åˆ†æ")
        print("=" * 60)

        if not os.path.exists(self.model_path):
            print(f"âŒ æ¨¡å‹æœªæ‰¾åˆ°: {self.model_path}")
            return

        model = PatchTSTForStock.from_pretrained(self.model_path).to(self.device)
        model.eval()

        # 1. åŠ è½½å…¨é‡å¸¦ Label çš„æ•°æ®
        print("Loading Full Panel Data (with labels)...")
        panel_df, feature_cols = DataProvider.load_and_process_panel(mode='train')

        # 2. [å…³é”®ä¿®æ”¹] è‡ªåŠ¨å®šä½æµ‹è¯•é›†èŒƒå›´
        unique_dates = np.sort(panel_df['date'].unique())

        if self.use_test_set_only:
            # å¤ç”¨ DataProvider ä¸­çš„åˆ‡åˆ†é€»è¾‘ (90% è®­ç»ƒ, 10% æµ‹è¯•)
            split_idx = int(len(unique_dates) * 0.9)

            # åŠ ä¸Š Gap é˜²æ­¢æ•°æ®æ³„éœ² (Context Len)
            test_start_idx = min(split_idx + Config.CONTEXT_LEN, len(unique_dates) - 1)

            self.start_date = pd.to_datetime(unique_dates[test_start_idx])
            self.end_date = pd.to_datetime(unique_dates[-1])

            print(f"\nğŸ”’ [Auto-Split] å·²é”å®šæ ·æœ¬å¤–æµ‹è¯•é›† (Out-of-Sample):")
            print(f"   è®­ç»ƒé›†èŒƒå›´: {unique_dates[0]} ~ {unique_dates[split_idx]}")
            print(f"   æµ‹è¯•é›†èŒƒå›´: {self.start_date.date()} ~ {self.end_date.date()}")
        else:
            # å¦‚æœæƒ³çœ‹å…¨é‡ï¼Œåˆ™ä½¿ç”¨ config çš„æ—¶é—´
            print(f"\nâš ï¸ [Warning] æ­£åœ¨åˆ†æå…¨é‡æ•°æ® (å«è®­ç»ƒé›†)ï¼Œç»“æœå¯èƒ½è™šé«˜ï¼")

        # 3. ç­›é€‰æ—¶é—´çª—å£
        # éœ€è¦é¢„ç•™ Context Length çš„æ•°æ®ç”¨äº Lookbackï¼Œæ‰€ä»¥ç‰©ç†è¯»å–çš„ start è¦å‰æ¨
        read_start_date = self.start_date - pd.Timedelta(days=Config.CONTEXT_LEN * 2 + 60)

        mask_date = (panel_df['date'] >= read_start_date) & (panel_df['date'] <= self.end_date)
        df_sub = panel_df[mask_date].copy()

        if df_sub.empty:
            print("âŒ é€‰å®šåŒºé—´æ— æœ‰æ•ˆæ•°æ®")
            return

        print("Start Batch Inference...")
        all_results = []
        batch_inputs, batch_meta = [], []

        feat_vals = df_sub[feature_cols].values.astype(np.float32)
        dates = df_sub['date'].values
        codes = df_sub['code'].values

        # ä¼˜å…ˆä½¿ç”¨ rank_label
        if 'rank_label' in df_sub.columns:
            labels = df_sub['rank_label'].values
        else:
            labels = df_sub['target'].values

        has_excess = 'excess_label' in df_sub.columns
        excess_vals = df_sub['excess_label'].values if has_excess else df_sub['target'].values

        unique_codes, code_indices = np.unique(codes, return_index=True)
        code_indices = np.append(code_indices, len(codes))

        seq_len = Config.CONTEXT_LEN
        batch_size = Config.ANALYSIS_BATCH_SIZE

        for k in tqdm(range(len(unique_codes)), desc="Processing Stocks"):
            start_pos = code_indices[k]
            end_pos = code_indices[k + 1]
            if end_pos - start_pos < seq_len: continue

            # ç­›é€‰åªåœ¨ Analysis åŒºé—´å†…çš„æ—¥æœŸè¿›è¡Œé¢„æµ‹
            curr_dates = dates[start_pos + seq_len - 1: end_pos]
            valid_mask = (curr_dates >= np.datetime64(self.start_date)) & \
                         (curr_dates <= np.datetime64(self.end_date))

            if not np.any(valid_mask): continue

            valid_offsets = np.where(valid_mask)[0]

            for offset in valid_offsets:
                pred_idx = start_pos + seq_len - 1 + offset
                window_start = start_pos + offset
                window_end = window_start + seq_len

                batch_inputs.append(feat_vals[window_start:window_end])
                batch_meta.append({
                    'date': dates[pred_idx],
                    'code': codes[pred_idx],
                    'rank_label': labels[pred_idx],
                    'excess_label': excess_vals[pred_idx]
                })

                if len(batch_inputs) >= batch_size:
                    self._flush_batch(model, batch_inputs, batch_meta, all_results)
                    batch_inputs, batch_meta = [], []

        if batch_inputs:
            self._flush_batch(model, batch_inputs, batch_meta, all_results)

        self.results_df = pd.DataFrame(all_results)
        self.results_df['date'] = pd.to_datetime(self.results_df['date'])
        print(f"âœ… æ¨ç†å®Œæˆï¼Œç”Ÿæˆ {len(self.results_df)} æ¡é¢„æµ‹è®°å½•ã€‚")

    def _flush_batch(self, model, inputs, meta, results_list):
        tensor = torch.tensor(np.array(inputs), dtype=torch.float32).to(self.device)
        with torch.no_grad():
            outputs = model(past_values=tensor)
            scores = outputs.logits.squeeze().cpu().numpy()

        if scores.ndim == 0: scores = [scores]
        limit = min(len(meta), len(scores))
        for i in range(limit):
            meta[i]['score'] = float(scores[i])
            results_list.append(meta[i])

    def analyze_performance(self):
        if self.results_df is None or self.results_df.empty:
            print("âš ï¸ ç»“æœé›†ä¸ºç©º")
            return

        df = self.results_df.copy()

        # 1. è®¡ç®— Rank IC
        df['score_rank'] = df.groupby('date')['score'].rank(pct=True)
        df['label_rank'] = df.groupby('date')['rank_label'].rank(pct=True)

        daily_ic = df.groupby('date').apply(lambda x: x['score_rank'].corr(x['label_rank']))

        # 2. ç»Ÿè®¡
        ic_mean = daily_ic.mean()
        ic_std = daily_ic.std()
        icir = ic_mean / (ic_std + 1e-9) * np.sqrt(252)
        ic_win_rate = (daily_ic > 0).mean()

        print("-" * 50)
        # æ‰“å°å½“å‰åˆ†æçš„æ—¶é—´æ®µï¼Œå†æ¬¡ç¡®è®¤
        print(f"ğŸ“Š ã€å› å­æ·±åº¦ç»©æ•ˆæŠ¥å‘Šã€‘ (åŒºé—´: {self.start_date.date()} ~ {self.end_date.date()})")
        print("-" * 50)
        print(f"Rank IC (Mean) : {ic_mean:.4f}")
        print(f"ICIR (Annual)  : {icir:.4f}")
        print(f"IC Win Rate    : {ic_win_rate:.2%}")
        print("-" * 50)

        self._plot_results(df, daily_ic, ic_mean, icir, ic_win_rate)

    def _plot_results(self, df, daily_ic, ic_mean, icir, ic_win_rate):
        # ... (ç»˜å›¾ä»£ç ä¿æŒä¸å˜ï¼Œè¯·ç›´æ¥å¤ç”¨ä¹‹å‰çš„ _plot_results) ...
        # ä¸ºèŠ‚çœç¯‡å¹…ï¼Œæ­¤å¤„çœç•¥ç»˜å›¾éƒ¨åˆ†ï¼Œé€»è¾‘å®Œå…¨ä¸€è‡´
        pass


if __name__ == "__main__":
    # é»˜è®¤å¼€å¯ Trueï¼Œåªåˆ†ææµ‹è¯•é›†
    analyzer = BacktestAnalyzer(use_test_set_only=True)
    analyzer.generate_historical_predictions()
    analyzer.analyze_performance()