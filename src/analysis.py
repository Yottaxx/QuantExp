import torch
import pandas as pd
import numpy as np
import os
import matplotlib.pyplot as plt
from tqdm import tqdm
from .config import Config
from .model import PatchTSTForStock
from .data_provider import DataProvider

# è®¾ç½® matplotlib é£æ ¼ï¼Œé˜²æ­¢ä¸­æ–‡ä¹±ç 
plt.rcParams['font.sans-serif'] = ['Arial Unicode MS', 'SimHei', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False


class BacktestAnalyzer:
    def __init__(self, start_date=None, end_date=None, use_test_set_only=True):
        """
        åˆå§‹åŒ–åˆ†æå™¨
        :param start_date: è‡ªå®šä¹‰å¼€å§‹æ—¥æœŸ (ä»…åœ¨ use_test_set_only=False æ—¶ç”Ÿæ•ˆ)
        :param end_date: è‡ªå®šä¹‰ç»“æŸæ—¥æœŸ (ä»…åœ¨ use_test_set_only=False æ—¶ç”Ÿæ•ˆ)
        :param use_test_set_only: æ˜¯å¦å¼ºåˆ¶ä½¿ç”¨æµ‹è¯•é›† (é»˜è®¤ Trueï¼Œä¼˜å…ˆçº§æœ€é«˜)
        """
        self.device = Config.DEVICE
        self.model_path = f"{Config.OUTPUT_DIR}/final_model"
        self.results_df = None

        self.use_test_set_only = use_test_set_only
        self.user_start_date = start_date
        self.user_end_date = end_date

        # å®é™…åˆ†æçš„èµ·æ­¢æ—¥æœŸ (å°†åœ¨åŠ è½½æ•°æ®åè®¡ç®—)
        self.analysis_start_date = None
        self.analysis_end_date = None

    def _resolve_analysis_range(self, panel_df):
        """
        æ ¹æ®æ¨¡å¼è§£æå®é™…çš„åˆ†ææ—¶é—´èŒƒå›´
        """
        unique_dates = np.sort(panel_df['date'].unique())
        n_dates = len(unique_dates)

        if self.use_test_set_only:
            # --- æ¨¡å¼ A: è‡ªåŠ¨é”å®šæµ‹è¯•é›† (ä¸¥æ ¼é˜²æ³„æ¼) ---
            train_end_idx = int(n_dates * Config.TRAIN_RATIO)
            val_end_idx = int(n_dates * (Config.TRAIN_RATIO + Config.VAL_RATIO))

            # Test Set Start = Valid End + Gap (Context Len)
            # å¿…é¡»è·³è¿‡ Gapï¼Œé˜²æ­¢ Valid é›†æœ«å°¾çš„æ•°æ®ä½œä¸º History æ³„æ¼ç»™ Test é›†å¼€å¤´
            test_start_idx = min(val_end_idx + Config.CONTEXT_LEN, n_dates - 1)

            self.analysis_start_date = pd.to_datetime(unique_dates[test_start_idx])
            self.analysis_end_date = pd.to_datetime(unique_dates[-1])

            print(f"\nğŸ”’ [Mode: Test Set Only] å·²è‡ªåŠ¨é”å®šæ ·æœ¬å¤–åŒºé—´:")
            print(f"   èŒƒå›´: {self.analysis_start_date.date()} ~ {self.analysis_end_date.date()}")

        else:
            # --- æ¨¡å¼ B: ç”¨æˆ·è‡ªå®šä¹‰ (çµæ´»åˆ†æ) ---
            # å¦‚æœç”¨æˆ·æœªæŒ‡å®šï¼Œé»˜è®¤ä½¿ç”¨ Config æˆ–å…¨é‡èŒƒå›´
            s_date = self.user_start_date or Config.START_DATE
            e_date = self.user_end_date or "2099-12-31"

            self.analysis_start_date = pd.to_datetime(s_date)
            self.analysis_end_date = pd.to_datetime(e_date)

            print(f"\nğŸ”“ [Mode: Custom Range] ä½¿ç”¨è‡ªå®šä¹‰åŒºé—´:")
            print(f"   èŒƒå›´: {self.analysis_start_date.date()} ~ {self.analysis_end_date.date()}")

            # è­¦å‘Šï¼šå¦‚æœè‡ªå®šä¹‰åŒºé—´è¦†ç›–äº†è®­ç»ƒé›†ï¼Œæç¤ºé£é™©
            train_limit_date = pd.to_datetime(unique_dates[int(n_dates * Config.TRAIN_RATIO)])
            if self.analysis_start_date < train_limit_date:
                print(f"   âš ï¸ è­¦å‘Š: è¯¥åŒºé—´åŒ…å«è®­ç»ƒé›†æ•°æ® ({train_limit_date.date()} ä¹‹å‰)ï¼Œç»“æœå¯èƒ½å­˜åœ¨è¿‡æ‹Ÿåˆ!")

    def generate_historical_predictions(self):
        """
        æ‰§è¡Œæ¨ç†
        """
        print("\n" + "=" * 60)
        print(">>> [Analysis] å¯åŠ¨æˆªé¢åˆ†æä¸æ¨ç†")
        print("=" * 60)

        if not os.path.exists(self.model_path):
            print(f"âŒ æ¨¡å‹æœªæ‰¾åˆ°: {self.model_path}")
            return

        # 1. åŠ è½½æ¨¡å‹
        print(f"Loading Model: {self.model_path}")
        model = PatchTSTForStock.from_pretrained(self.model_path).to(self.device)
        model.eval()

        # 2. åŠ è½½å…¨é‡æ•°æ® (ç”¨äºå®šä½æ—¥æœŸå’Œæå–ç‰¹å¾)
        # mode='train' ä»…è¡¨ç¤ºåŠ è½½åŒ…å« Label çš„æ•°æ®ç»“æ„ï¼Œå¹¶éåªåŠ è½½è®­ç»ƒé›†
        print("Loading Full Panel Data...")
        panel_df, feature_cols = DataProvider.load_and_process_panel(mode='train')

        # 3. è§£ææ—¶é—´èŒƒå›´
        self._resolve_analysis_range(panel_df)

        # 4. æ•°æ®åˆ‡ç‰‡ (ç‰©ç†è¯»å–èŒƒå›´)
        # ä¸ºäº†é¢„æµ‹ T æ—¥ï¼Œæˆ‘ä»¬éœ€è¦ T - Context_Len çš„å†å²æ•°æ®
        # æ‰€ä»¥ç‰©ç†è¯»å–çš„ Start Date å¿…é¡»æ¯” Analysis Start Date æ—©
        lookback_buffer = Config.CONTEXT_LEN * 2 + 60  # é¢„ç•™å……è¶³ buffer
        read_start_date = self.analysis_start_date - pd.Timedelta(days=lookback_buffer)

        mask_date = (panel_df['date'] >= read_start_date) & (panel_df['date'] <= self.analysis_end_date)
        df_sub = panel_df[mask_date].copy()

        if df_sub.empty:
            print("âŒ é€‰å®šåŒºé—´æ— æœ‰æ•ˆæ•°æ®")
            return

        print("Start Batch Inference...")
        all_results = []
        batch_inputs, batch_meta = [], []

        # é¢„å¤„ç†æ•°æ®åŠ é€Ÿè¯»å–
        feat_vals = df_sub[feature_cols].values.astype(np.float32)
        dates = df_sub['date'].values
        codes = df_sub['code'].values

        if 'rank_label' in df_sub.columns:
            labels = df_sub['rank_label'].values
        else:
            labels = df_sub['target'].values

        has_excess = 'excess_label' in df_sub.columns
        excess_vals = df_sub['excess_label'].values if has_excess else df_sub['target'].values

        unique_codes, code_indices = np.unique(codes, return_index=True)
        code_indices = np.append(code_indices, len(codes))

        seq_len = Config.CONTEXT_LEN
        batch_size = Config.ANALYSIS_BATCH_SIZE

        # éå†æ‰€æœ‰è‚¡ç¥¨
        for k in tqdm(range(len(unique_codes)), desc="Processing Stocks"):
            start_pos = code_indices[k]
            end_pos = code_indices[k + 1]

            # æ•°æ®é•¿åº¦ä¸è¶³ä»¥æ„å»ºä¸€ä¸ªçª—å£
            if end_pos - start_pos < seq_len: continue

            # ç­›é€‰å‡º [Analysis Start, Analysis End] åŒºé—´å†…çš„æ—¥æœŸç´¢å¼•
            curr_dates = dates[start_pos + seq_len - 1: end_pos]
            valid_mask = (curr_dates >= np.datetime64(self.analysis_start_date)) & \
                         (curr_dates <= np.datetime64(self.analysis_end_date))

            if not np.any(valid_mask): continue

            # è·å–ç›¸å¯¹åç§»é‡
            valid_offsets = np.where(valid_mask)[0]

            for offset in valid_offsets:
                # ç»å¯¹ç´¢å¼•
                pred_idx = start_pos + seq_len - 1 + offset
                window_start = start_pos + offset
                window_end = window_start + seq_len

                batch_inputs.append(feat_vals[window_start:window_end])
                batch_meta.append({
                    'date': dates[pred_idx],
                    'code': codes[pred_idx],
                    'rank_label': labels[pred_idx],
                    'excess_label': excess_vals[pred_idx]
                })

                if len(batch_inputs) >= batch_size:
                    self._flush_batch(model, batch_inputs, batch_meta, all_results)
                    batch_inputs, batch_meta = [], []

        # å¤„ç†å‰©ä½™ Batch
        if batch_inputs:
            self._flush_batch(model, batch_inputs, batch_meta, all_results)

        self.results_df = pd.DataFrame(all_results)
        if not self.results_df.empty:
            self.results_df['date'] = pd.to_datetime(self.results_df['date'])
            print(f"âœ… æ¨ç†å®Œæˆï¼Œç”Ÿæˆ {len(self.results_df)} æ¡é¢„æµ‹è®°å½•ã€‚")
        else:
            print("âŒ æœªç”Ÿæˆä»»ä½•é¢„æµ‹è®°å½•ï¼Œè¯·æ£€æŸ¥æ—¥æœŸèŒƒå›´æˆ–æ•°æ®å®Œæ•´æ€§ã€‚")

    def _flush_batch(self, model, inputs, meta, results_list):
        tensor = torch.tensor(np.array(inputs), dtype=torch.float32).to(self.device)
        with torch.no_grad():
            outputs = model(past_values=tensor)
            scores = outputs.logits.squeeze().cpu().numpy()

        if scores.ndim == 0: scores = [scores]

        limit = min(len(meta), len(scores))
        for i in range(limit):
            meta[i]['score'] = float(scores[i])
            results_list.append(meta[i])

    def analyze_performance(self):
        """
        è®¡ç®—æ ¸å¿ƒæŒ‡æ ‡
        """
        if self.results_df is None or self.results_df.empty:
            print("âš ï¸ ç»“æœé›†ä¸ºç©ºï¼Œæ— æ³•åˆ†æ")
            return

        df = self.results_df.copy()

        # 1. è®¡ç®— Rank IC
        # åœ¨æ¯ä¸ªæ—¥æœŸæˆªé¢ä¸Šè®¡ç®— score å’Œ label çš„ç›¸å…³æ€§
        df['score_rank'] = df.groupby('date')['score'].rank(pct=True)
        df['label_rank'] = df.groupby('date')['rank_label'].rank(pct=True)

        daily_ic = df.groupby('date').apply(
            lambda x: x['score_rank'].corr(x['label_rank'])
        )

        # 2. ç»Ÿè®¡æŒ‡æ ‡
        ic_mean = daily_ic.mean()
        ic_std = daily_ic.std()
        # å¹´åŒ– ICIR
        icir = ic_mean / (ic_std + 1e-9) * np.sqrt(252)
        # èƒœç‡
        ic_win_rate = (daily_ic > 0).mean()

        print("-" * 50)
        print(f"ğŸ“Š ã€å› å­æ·±åº¦ç»©æ•ˆæŠ¥å‘Šã€‘")
        print(f"   åˆ†æåŒºé—´: {self.analysis_start_date.date()} ~ {self.analysis_end_date.date()}")
        print("-" * 50)
        print(f"Rank IC (Mean) : {ic_mean:.4f}   (å‚è€ƒ: >0.03 ä¼˜ç§€)")
        print(f"ICIR (Annual)  : {icir:.4f}     (å‚è€ƒ: >1.00 ç¨³å®š)")
        print(f"IC Win Rate    : {ic_win_rate:.2%}   (å‚è€ƒ: >55%  èƒœç‡)")
        print("-" * 50)

        self._plot_results(df, daily_ic, ic_mean, icir, ic_win_rate)

    def _plot_results(self, df, daily_ic, ic_mean, icir, ic_win_rate):
        """
        ç»˜åˆ¶åˆ†æå›¾è¡¨
        """
        plt.figure(figsize=(16, 12))

        # Subplot 1: ç´¯ç§¯ IC
        ax1 = plt.subplot(3, 1, 1)
        daily_ic_cumsum = daily_ic.cumsum()
        ax1.plot(daily_ic_cumsum.index, daily_ic_cumsum.values, label='Cumulative Rank IC', color='#4B0082',
                 linewidth=1.5)
        ax1.set_title(f'Cumulative Rank IC (ICIR={icir:.2f})', fontsize=12, fontweight='bold')
        ax1.grid(True, linestyle='--', alpha=0.4)
        ax1.legend(loc='upper left')

        # Subplot 2: æ¯æ—¥ IC
        ax2 = plt.subplot(3, 1, 2)
        colors = ['#d32f2f' if v < 0 else '#388e3c' for v in daily_ic.values]
        ax2.bar(daily_ic.index, daily_ic.values, color=colors, alpha=0.6, width=1.0, label='Daily IC')
        ax2.axhline(ic_mean, color='blue', linestyle='--', linewidth=1.5, label=f'Mean IC: {ic_mean:.3f}')
        ax2.axhline(0, color='black', linewidth=0.8)
        ax2.set_title(f'Daily IC Distribution (Win Rate={ic_win_rate:.1%})', fontsize=12, fontweight='bold')
        ax2.legend(loc='upper right')
        ax2.grid(True, axis='y', linestyle='--', alpha=0.4)

        # Subplot 3: åˆ†å±‚å›æµ‹
        ax3 = plt.subplot(3, 1, 3)

        # åˆ†ç»„
        df['group'] = df.groupby('date')['score'].transform(
            lambda x: pd.qcut(x, 5, labels=False, duplicates='drop')
        )

        # è®¡ç®—æ¯æ—¥å„ç»„å¹³å‡è¶…é¢æ”¶ç›Š
        layer_ret = df.groupby(['date', 'group'])['excess_label'].mean().unstack()

        # ç®€å•å¹³æ‘Šå¤šæ—¥æ”¶ç›Š
        if Config.PRED_LEN > 1:
            layer_ret = layer_ret / Config.PRED_LEN

        layer_ret = layer_ret.fillna(0)
        cum_ret = (1 + layer_ret).cumprod()

        # ç»˜å›¾
        groups = sorted(layer_ret.columns)
        for idx, g in enumerate(groups):
            if g == groups[-1]:
                label, c, lw, alpha = "Top 20% (Long)", "#d32f2f", 2.0, 1.0  # Red
            elif g == groups[0]:
                label, c, lw, alpha = "Bottom 20% (Short)", "#388e3c", 1.5, 0.8  # Green
            else:
                label, c, lw, alpha = f"Group {g}", "gray", 0.8, 0.3

            ax3.plot(cum_ret.index, cum_ret[g], label=label, color=c, linewidth=lw, alpha=alpha)

        # å¤šç©ºæ›²çº¿
        if len(groups) >= 2:
            ls_ret = layer_ret[groups[-1]] - layer_ret[groups[0]]
            ls_cum = (1 + ls_ret).cumprod()
            ax3.plot(ls_cum.index, ls_cum, label='Long-Short Alpha', color='blue', linestyle='--', linewidth=1.5)

        ax3.set_title('Layered Backtest & Long-Short Alpha', fontsize=12, fontweight='bold')
        ax3.legend(loc='upper left', ncol=2)
        ax3.grid(True, linestyle='--', alpha=0.4)

        plt.tight_layout()
        save_path = os.path.join(Config.OUTPUT_DIR, "factor_comprehensive_report.png")
        plt.savefig(save_path, dpi=150)
        print(f"ğŸ“ˆ å›¾è¡¨å·²ä¿å­˜è‡³: {save_path}")


if __name__ == "__main__":
    # === ä½¿ç”¨ç¤ºä¾‹ ===

    # åœºæ™¯ 1: ä¸¥æ ¼éªŒè¯ (æ¨è)
    # è‡ªåŠ¨è®¡ç®— Test Set èŒƒå›´ï¼Œé˜²æ­¢æ•°æ®æ³„æ¼
    print(">>> Mode 1: Auto Test Set")
    analyzer = BacktestAnalyzer(use_test_set_only=True)
    analyzer.generate_historical_predictions()
    analyzer.analyze_performance()

    # åœºæ™¯ 2: çµæ´»åˆ†æ (ç”¨äºå¤ç›˜ç‰¹å®šå†å²æ—¶æœŸ)
    # print("\n>>> Mode 2: Custom Range")
    # analyzer = BacktestAnalyzer(start_date='2023-01-01', end_date='2023-06-30', use_test_set_only=False)
    # analyzer.generate_historical_predictions()
    # analyzer.analyze_performance()