import torch
import pandas as pd
import numpy as np
import os
import matplotlib.pyplot as plt
from scipy.stats import spearmanr
from tqdm import tqdm
from .config import Config
from .model import PatchTSTForStock
from .data_provider import DataProvider


class BacktestAnalyzer:
    def __init__(self, start_date='2024-01-01', end_date='2025-12-31'):
        self.start_date = pd.to_datetime(start_date)
        self.end_date = pd.to_datetime(end_date)
        self.device = Config.DEVICE
        self.model_path = f"{Config.OUTPUT_DIR}/final_model"
        self.results_df = None

    def generate_historical_predictions(self):
        """
        å…¨é‡å†å²å›æº¯æ¨ç†
        é€»è¾‘ï¼šä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹ï¼Œå¯¹è¿‡å»æ¯ä¸€å¤©çš„å…¨å¸‚åœºè‚¡ç¥¨è¿›è¡Œæ‰“åˆ†
        """
        print("\n" + "=" * 60)
        print(">>> å¯åŠ¨å…¨é‡æˆªé¢åˆ†æ (Full Cross-Sectional Analysis)")
        print("=" * 60)

        if not os.path.exists(self.model_path):
            print(f"âŒ æ¨¡å‹æœªæ‰¾åˆ°: {self.model_path}")
            return

        print(f"åŠ è½½æ¨¡å‹: {self.model_path}")
        model = PatchTSTForStock.from_pretrained(self.model_path).to(self.device)
        model.eval()

        # 1. åŠ è½½æ•°æ® (Train æ¨¡å¼ï¼Œå› ä¸ºæˆ‘ä»¬éœ€è¦ Target/Label æ¥éªŒè¯æ•ˆæœ)
        # åˆ©ç”¨ç¼“å­˜åŠ é€Ÿ
        print("åŠ è½½å…¨å¸‚åœº Panel æ•°æ® (éªŒè¯æ¨¡å¼)...")
        panel_df, feature_cols = DataProvider.load_and_process_panel(mode='train')

        # 2. ç­›é€‰æ—¶é—´æ®µ (ä¸ºäº†å›æµ‹æ•ˆç‡ï¼Œåªå–ç›®æ ‡åŒºé—´ + çª—å£æœŸ)
        # start_date å¾€å‰æ¨ Context_Len å¤©ï¼Œç¡®ä¿ç¬¬ä¸€å¤©å°±èƒ½æ„å»ºçª—å£
        mask_date = (panel_df['date'] >= (self.start_date - pd.Timedelta(days=Config.CONTEXT_LEN * 2))) & \
                    (panel_df['date'] <= self.end_date)
        df_sub = panel_df[mask_date].copy()

        if df_sub.empty:
            print("âŒ é€‰å®šåŒºé—´æ— æ•°æ®")
            return

        print("æ­£åœ¨æ„å»ºæ—¶åºçª—å£å¹¶æ¨ç†...")

        all_results = []
        batch_size = 2048  # æ ¹æ®æ˜¾å­˜è°ƒæ•´
        batch_inputs = []
        batch_meta = []  # (date, code, rank_label, excess_label)

        grouped = df_sub.groupby('code')

        for code, group in tqdm(grouped, desc="Processing Stocks"):
            if len(group) < Config.CONTEXT_LEN: continue

            # æå– Numpy æ•°ç»„ (Float32)
            feats = group[feature_cols].values.astype(np.float32)
            dates = group['date'].values

            # è·å–éªŒè¯ç”¨çš„çœŸå®æ ‡ç­¾
            # rank_label: 0~1, ç”¨äºè®¡ç®— IC
            # excess_label: çœŸå®è¶…é¢æ”¶ç›Š, ç”¨äºç”»èµ„é‡‘æ›²çº¿
            # target: å®ç›˜ç»å¯¹æ”¶ç›Š (Close_N / Open_1 - 1)

            # ä¼˜å…ˆè·å–é¢„è®¡ç®—å¥½çš„æ ‡ç­¾ï¼Œå¦‚æœæ²¡æœ‰åˆ™ fallback
            ranks = group['rank_label'].values if 'rank_label' in group.columns else np.zeros(len(group))
            excess = group['excess_label'].values if 'excess_label' in group.columns else group['target'].values

            # æ»‘åŠ¨çª—å£åˆ‡ç‰‡
            # i æ˜¯çª—å£èµ·ç‚¹ï¼Œé¢„æµ‹çš„æ˜¯ i + seq_len - 1 è¿™ä¸ªæ—¶é—´ç‚¹çš„è¡¨ç°
            seq_len = Config.CONTEXT_LEN

            for i in range(len(group) - seq_len + 1):
                # é¢„æµ‹æ—¥æœŸæ˜¯çª—å£çš„æœ€åä¸€å¤©
                pred_date_ts = dates[i + seq_len - 1]
                pred_date = pd.to_datetime(pred_date_ts)

                if pred_date < self.start_date or pred_date > self.end_date:
                    continue

                # åŠ å…¥ Batch
                batch_inputs.append(feats[i: i + seq_len])
                batch_meta.append({
                    'date': pred_date,
                    'code': code,
                    'rank_label': ranks[i + seq_len - 1],
                    'excess_label': excess[i + seq_len - 1]
                })

                if len(batch_inputs) >= batch_size:
                    self._flush_batch(model, batch_inputs, batch_meta, all_results)
                    batch_inputs = []
                    batch_meta = []

        # å¤„ç†å‰©ä½™ Batch
        if batch_inputs:
            self._flush_batch(model, batch_inputs, batch_meta, all_results)

        self.results_df = pd.DataFrame(all_results)
        print(f"æ¨ç†å®Œæˆï¼Œç”Ÿæˆ {len(self.results_df)} æ¡é¢„æµ‹è®°å½•ã€‚")

    def _flush_batch(self, model, inputs, meta, results_list):
        tensor = torch.tensor(np.array(inputs), dtype=torch.float32).to(self.device)
        with torch.no_grad():
            outputs = model(past_values=tensor)
            scores = outputs.logits.squeeze().cpu().numpy()

        if scores.ndim == 0: scores = [scores]

        for i, score in enumerate(scores):
            item = meta[i]
            item['score'] = float(score)
            results_list.append(item)

    def analyze_performance(self):
        if self.results_df is None or self.results_df.empty: return

        # æŒ‰æ—¥æœŸå’Œåˆ†æ•°æ’åº
        df = self.results_df.sort_values(['date', 'score'], ascending=[True, False])

        print("\nè®¡ç®—æˆªé¢ IC æŒ‡æ ‡...")

        # 1. è®¡ç®— Rank IC (Spearman Correlation)
        # é¢„æµ‹åˆ†(score) vs çœŸå®æ’å(rank_label)
        # å¦‚æœæ¨¡å‹å¥½ï¼Œé¢„æµ‹åˆ†é«˜çš„åœ°æ–¹ï¼ŒçœŸå®æ’åä¹Ÿåº”è¯¥é å‰(æ¥è¿‘1.0)
        daily_ic = df.groupby('date').apply(
            lambda x: spearmanr(x['score'], x['rank_label'])[0]
        )

        ic_mean = daily_ic.mean()
        ic_std = daily_ic.std()
        # å¹´åŒ– ICIR = ICå‡å€¼ / ICæ³¢åŠ¨ç‡ * sqrt(252)
        icir = ic_mean / (ic_std + 1e-9) * np.sqrt(252)

        print("-" * 40)
        print(f"ğŸ“Š ã€å› å­ç»©æ•ˆæŠ¥å‘Šã€‘")
        print(f"Rank IC (Mean): {ic_mean:.4f}  (>0.05 ä¼˜ç§€)")
        print(f"ICIR (Annual) : {icir:.4f}    (>2.0 ç¨³å®š)")
        print(f"IC Win Rate   : {(daily_ic > 0).mean():.2%}")
        print("-" * 40)

        # 2. åˆ†å±‚å›æµ‹ (Layered Backtest)
        # å°†æ¯æ—¥è‚¡ç¥¨æŒ‰åˆ†æ•°åˆ†ä¸º 5 ç»„ï¼Œçœ‹æ¯ç»„çš„å¹³å‡è¶…é¢æ”¶ç›Š
        def get_layer_ret(g):
            try:
                # åˆ†5ç»„ï¼Œlabel=4æ˜¯æœ€é«˜åˆ†(Long)ï¼Œlabel=0æ˜¯æœ€ä½åˆ†(Short)
                g['group'] = pd.qcut(g['score'], 5, labels=False, duplicates='drop')
                # è®¡ç®—æ¯ç»„çš„å¹³å‡è¶…é¢æ”¶ç›Š
                return g.groupby('group')['excess_label'].mean()
            except:
                return None

        layer_ret = df.groupby('date').apply(get_layer_ret)

        if layer_ret is not None:
            # ç´¯ç§¯æ”¶ç›Š
            cum_ret = (1 + layer_ret).cumprod()
            # å¤šç©ºæ”¶ç›Š = Top - Bottom
            long_short = (1 + (layer_ret[4] - layer_ret[0])).cumprod()

            plt.figure(figsize=(14, 8))

            # å­å›¾1: åˆ†å±‚æ”¶ç›Šæ›²çº¿
            plt.subplot(2, 1, 1)
            colors = ['green', 'lime', 'grey', 'orange', 'red']
            labels = ['Bottom 20%', '40%-60%', 'Middle', '60%-80%', 'Top 20%']

            for i in range(5):
                if i in cum_ret.columns:
                    plt.plot(cum_ret.index, cum_ret[i], label=labels[i], color=colors[i], alpha=0.8)

            plt.plot(long_short.index, long_short, label='Long-Short (Alpha)', color='blue', linestyle='--',
                     linewidth=2)
            plt.title('Layered Backtest (Cumulative Excess Return)')
            plt.legend(loc='upper left')
            plt.grid(True, alpha=0.3)

            # å­å›¾2: æ¯æ—¥ IC æŸ±çŠ¶å›¾
            plt.subplot(2, 1, 2)
            plt.bar(daily_ic.index, daily_ic.values, color='orange', alpha=0.5, label='Daily IC')
            plt.axhline(ic_mean, color='red', linestyle='--', label=f'Mean IC: {ic_mean:.3f}')
            plt.title('Daily Rank IC Series')
            plt.legend()
            plt.grid(True, alpha=0.3)

            save_path = os.path.join(Config.OUTPUT_DIR, "cross_section_analysis.png")
            plt.tight_layout()
            plt.savefig(save_path)
            print(f"ğŸ“ˆ æŠ¥è¡¨å·²ä¿å­˜: {save_path}")