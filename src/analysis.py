import torch
import pandas as pd
import numpy as np
import os
import matplotlib.pyplot as plt
from scipy.stats import spearmanr
from tqdm import tqdm
from .config import Config
from .model import PatchTSTForStock
from .data_provider import DataProvider


class BacktestAnalyzer:
    def __init__(self, start_date='2024-01-01', end_date='2025-12-31'):
        self.start_date = pd.to_datetime(start_date)
        self.end_date = pd.to_datetime(end_date)
        self.device = Config.DEVICE
        self.model_path = f"{Config.OUTPUT_DIR}/final_model"
        self.results_df = None

    def generate_historical_predictions(self):
        """
        å…¨é‡å†å²å›æº¯æ¨ç†
        é€»è¾‘ï¼šä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹ï¼Œå¯¹è¿‡å»æ¯ä¸€å¤©çš„å…¨å¸‚åœºè‚¡ç¥¨è¿›è¡Œæ‰“åˆ†
        """
        print("\n" + "=" * 60)
        print(">>> å¯åŠ¨å…¨é‡æˆªé¢åˆ†æ (Full Cross-Sectional Analysis)")
        print("=" * 60)

        if not os.path.exists(self.model_path):
            print(f"âŒ æ¨¡å‹æœªæ‰¾åˆ°: {self.model_path}")
            return

        print(f"åŠ è½½æ¨¡å‹: {self.model_path}")
        model = PatchTSTForStock.from_pretrained(self.model_path).to(self.device)
        model.eval()

        # 1. åŠ è½½æ•°æ® (Train æ¨¡å¼ï¼Œå› ä¸ºæˆ‘ä»¬éœ€è¦ Target/Label æ¥éªŒè¯æ•ˆæœ)
        # åˆ©ç”¨ç¼“å­˜åŠ é€Ÿ
        print("åŠ è½½å…¨å¸‚åœº Panel æ•°æ® (éªŒè¯æ¨¡å¼)...")
        panel_df, feature_cols = DataProvider.load_and_process_panel(mode='train')

        # 2. ç­›é€‰æ—¶é—´æ®µ (ä¸ºäº†å›æµ‹æ•ˆç‡ï¼Œåªå–ç›®æ ‡åŒºé—´ + çª—å£æœŸ)
        mask_date = (panel_df['date'] >= (self.start_date - pd.Timedelta(days=Config.CONTEXT_LEN * 2))) & \
                    (panel_df['date'] <= self.end_date)
        df_sub = panel_df[mask_date].copy()

        if df_sub.empty:
            print("âŒ é€‰å®šåŒºé—´æ— æ•°æ®")
            return

        print("æ­£åœ¨æ„å»ºæ—¶åºçª—å£å¹¶æ¨ç†...")

        all_results = []
        batch_size = Config.ANALYSIS_BATCH_SIZE
        batch_inputs = []
        batch_meta = []  # (date, code, rank_label, excess_label)

        grouped = df_sub.groupby('code')

        for code, group in tqdm(grouped, desc="Processing Stocks"):
            if len(group) < Config.CONTEXT_LEN: continue

            # æå– Numpy æ•°ç»„ (Float32)
            feats = group[feature_cols].values.astype(np.float32)
            dates = group['date'].values

            # ä¼˜å…ˆè·å–é¢„è®¡ç®—å¥½çš„æ ‡ç­¾
            ranks = group['rank_label'].values if 'rank_label' in group.columns else np.zeros(len(group))
            excess = group['excess_label'].values if 'excess_label' in group.columns else group['target'].values

            seq_len = Config.CONTEXT_LEN

            for i in range(len(group) - seq_len + 1):
                pred_date_ts = dates[i + seq_len - 1]
                pred_date = pd.to_datetime(pred_date_ts)

                if pred_date < self.start_date or pred_date > self.end_date:
                    continue

                batch_inputs.append(feats[i: i + seq_len])
                batch_meta.append({
                    'date': pred_date,
                    'code': code,
                    'rank_label': ranks[i + seq_len - 1],
                    'excess_label': excess[i + seq_len - 1]
                })

                if len(batch_inputs) >= batch_size:
                    self._flush_batch(model, batch_inputs, batch_meta, all_results)
                    batch_inputs = []
                    batch_meta = []

        # å¤„ç†å‰©ä½™ Batch
        if batch_inputs:
            self._flush_batch(model, batch_inputs, batch_meta, all_results)

        self.results_df = pd.DataFrame(all_results)
        print(f"æ¨ç†å®Œæˆï¼Œç”Ÿæˆ {len(self.results_df)} æ¡é¢„æµ‹è®°å½•ã€‚")

    def _flush_batch(self, model, inputs, meta, results_list):
        tensor = torch.tensor(np.array(inputs), dtype=torch.float32).to(self.device)
        with torch.no_grad():
            outputs = model(past_values=tensor)
            scores = outputs.logits.squeeze().cpu().numpy()

        if scores.ndim == 0: scores = [scores]

        for i, score in enumerate(scores):
            item = meta[i]
            item['score'] = float(score)
            results_list.append(item)

    def analyze_performance(self):
        if self.results_df is None or self.results_df.empty: return

        # æŒ‰æ—¥æœŸå’Œåˆ†æ•°æ’åº
        df = self.results_df.sort_values(['date', 'score'], ascending=[True, False])

        print("\nè®¡ç®—æˆªé¢ IC æŒ‡æ ‡...")

        # 1. è®¡ç®— Rank IC
        daily_ic = df.groupby('date').apply(
            lambda x: spearmanr(x['score'], x['rank_label'])[0]
        )

        ic_mean = daily_ic.mean()
        ic_std = daily_ic.std()
        icir = ic_mean / (ic_std + 1e-9) * np.sqrt(252)

        print("-" * 40)
        print(f"ğŸ“Š ã€å› å­ç»©æ•ˆæŠ¥å‘Šã€‘")
        print(f"Rank IC (Mean): {ic_mean:.4f}")
        print(f"ICIR (Annual) : {icir:.4f}")
        print(f"IC Win Rate   : {(daily_ic > 0).mean():.2%}")
        print("-" * 40)

        # 2. åˆ†å±‚å›æµ‹ (Layered Backtest) - ã€é€»è¾‘ä¿®æ­£ç‰ˆã€‘
        def get_layer_ret(g):
            try:
                # å°è¯•åˆ†5ç»„ï¼Œå¦‚æœæ•°æ®å¤ªå°‘ï¼Œqcut ä¼šè‡ªåŠ¨å‡å°‘ç»„æ•°ï¼ˆéœ€é…åˆ duplicates='drop'ï¼‰
                # labels=False è¿”å›ç»„å· 0, 1, 2...
                # å› ä¸º score è¶Šå¤§è¶Šå¥½ï¼Œæˆ‘ä»¬å¸Œæœ›ç»„å·è¶Šå¤§ä»£è¡¨åˆ†æ•°è¶Šé«˜
                g['group'] = pd.qcut(g['score'], 5, labels=False, duplicates='drop')
                return g.groupby('group')['excess_label'].mean()
            except:
                return None

        # groupby apply å¯èƒ½è¿”å› MultiIndex Series (Date, Group)
        layer_ret_raw = df.groupby('date').apply(get_layer_ret)

        if layer_ret_raw is not None and not layer_ret_raw.empty:
            # ã€æ ¸å¿ƒä¿®æ­£ 1ã€‘çŸ©é˜µå±•å¼€ä¸å¯¹é½
            # ç¡®ä¿å¾—åˆ°ä¸€ä¸ª DataFrame: Index=Date, Columns=Group_ID
            if isinstance(layer_ret_raw, pd.Series):
                layer_ret = layer_ret_raw.unstack(level=-1)
            else:
                layer_ret = layer_ret_raw

            # ã€æ ¸å¿ƒä¿®æ­£ 2ã€‘ç©ºå€¼å¡«å……
            # å¦‚æœæŸå¤©æŸç»„æ²¡ç¥¨ï¼Œæ”¶ç›Šå¡«0ï¼Œé˜²æ­¢ cumprod æ–­è£‚
            layer_ret = layer_ret.fillna(0.0)

            # è®¡ç®—å„ç»„ç´¯ç§¯æ”¶ç›Š
            cum_ret = (1 + layer_ret).cumprod()

            plt.figure(figsize=(14, 8))

            # å­å›¾1: åˆ†å±‚æ”¶ç›Šæ›²çº¿
            plt.subplot(2, 1, 1)
            colors = ['green', 'lime', 'grey', 'orange', 'red']

            # è·å–å®é™…å­˜åœ¨çš„ç»„å·
            available_groups = sorted(layer_ret.columns)

            for i in available_groups:
                # åŠ¨æ€ç”Ÿæˆæ ‡ç­¾
                if i == available_groups[-1]:
                    label = "Top Group (Long)"
                    c = 'red'
                    alpha = 1.0
                elif i == available_groups[0]:
                    label = "Bottom Group (Short)"
                    c = 'green'
                    alpha = 1.0
                else:
                    label = f"Group {i}"
                    c = 'grey'
                    alpha = 0.3

                plt.plot(cum_ret.index, cum_ret[i], label=label, color=c, alpha=alpha)

            # ã€æ ¸å¿ƒä¿®æ­£ 3ã€‘åŠ¨æ€è®¡ç®—å¤šç©ºæ›²çº¿
            # åªæœ‰å½“è‡³å°‘æœ‰ 2 ä¸ªç»„æ—¶æ‰è®¡ç®—å¤šç©º
            if len(available_groups) >= 2:
                top_grp = available_groups[-1]
                bot_grp = available_groups[0]
                long_short_ret = layer_ret[top_grp] - layer_ret[bot_grp]
                long_short_cum = (1 + long_short_ret).cumprod()
                plt.plot(long_short_cum.index, long_short_cum, label='Long-Short (Alpha)', color='blue', linestyle='--',
                         linewidth=2)

            plt.title('Layered Backtest (Cumulative Excess Return)')
            plt.legend(loc='upper left')
            plt.grid(True, alpha=0.3)

            # å­å›¾2: æ¯æ—¥ IC æŸ±çŠ¶å›¾
            plt.subplot(2, 1, 2)
            plt.bar(daily_ic.index, daily_ic.values, color='orange', alpha=0.5, label='Daily IC')
            plt.axhline(ic_mean, color='red', linestyle='--', label=f'Mean IC: {ic_mean:.3f}')
            plt.title('Daily Rank IC Series')
            plt.legend()

            save_path = os.path.join(Config.OUTPUT_DIR, "cross_section_analysis.png")
            plt.tight_layout()
            plt.savefig(save_path)
            print(f"ğŸ“ˆ æŠ¥è¡¨å·²ä¿å­˜: {save_path}")