import torch
import pandas as pd
import numpy as np
import os
import matplotlib.pyplot as plt
from tqdm import tqdm
from .config import Config
from .model import PatchTSTForStock
from .data_provider import DataProvider

# è®¾ç½® matplotlib é£æ ¼ï¼Œé˜²æ­¢ä¸­æ–‡ä¹±ç 
plt.rcParams['font.sans-serif'] = ['Arial Unicode MS', 'SimHei', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False


class BacktestAnalyzer:
    def __init__(self, start_date='2024-01-01', end_date='2025-12-31'):
        self.start_date = pd.to_datetime(start_date)
        self.end_date = pd.to_datetime(end_date)
        self.device = Config.DEVICE
        self.model_path = f"{Config.OUTPUT_DIR}/final_model"
        self.results_df = None

    def generate_historical_predictions(self):
        """
        [Step 1] å…¨é‡å†å²å›æº¯æ¨ç†
        ä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹ï¼Œå¯¹ç›®æ ‡åŒºé—´å†…çš„å…¨å¸‚åœºè‚¡ç¥¨è¿›è¡Œæ»šåŠ¨é¢„æµ‹
        """
        print("\n" + "=" * 60)
        print(">>> [Analysis] å¯åŠ¨å…¨é‡æˆªé¢åˆ†æ (Full Cross-Sectional Inference)")
        print("=" * 60)

        if not os.path.exists(self.model_path):
            print(f"âŒ æ¨¡å‹æœªæ‰¾åˆ°: {self.model_path}ï¼Œè¯·å…ˆè¿è¡Œ train.py")
            return

        print(f"Loading Model: {self.model_path}")
        model = PatchTSTForStock.from_pretrained(self.model_path).to(self.device)
        model.eval()

        # åŠ è½½æ•°æ® (Train æ¨¡å¼åŒ…å« Labelï¼Œç”¨äºåç»­éªŒè¯)
        # å¼ºåˆ¶åˆ·æ–°ç¼“å­˜ä»¥ç¡®ä¿æ•°æ®æ˜¯æœ€æ–°çš„ï¼Œæˆ–è€…æ ¹æ®éœ€æ±‚ remove force_refresh
        print("Loading Panel Data (Train Mode)...")
        panel_df, feature_cols = DataProvider.load_and_process_panel(mode='train')

        # ç­›é€‰æ—¶é—´çª—å£ï¼šéœ€è¦é¢„ç•™ Context Length çš„æ•°æ®ç”¨äº Lookback
        start_buffer = self.start_date - pd.Timedelta(days=Config.CONTEXT_LEN * 2 + 60)
        mask_date = (panel_df['date'] >= start_buffer) & (panel_df['date'] <= self.end_date)
        df_sub = panel_df[mask_date].copy()

        if df_sub.empty:
            print("âŒ é€‰å®šåŒºé—´æ— æœ‰æ•ˆæ•°æ®")
            return

        print(f"Inference Range: {self.start_date.date()} ~ {self.end_date.date()}")
        print("Start Batch Inference...")

        all_results = []
        batch_inputs = []
        batch_meta = []  # å­˜å‚¨å…ƒæ•°æ® (date, code, label)

        # é¢„è®¡ç®—å­—æ®µç´¢å¼•ï¼Œé¿å…åœ¨å¾ªç¯ä¸­é¢‘ç¹å­—ç¬¦ä¸²æŸ¥æ‰¾
        feat_vals = df_sub[feature_cols].values.astype(np.float32)
        dates = df_sub['date'].values
        codes = df_sub['code'].values

        # ä¼˜å…ˆä½¿ç”¨ rank_label (å¦‚æœ‰)ï¼Œå¦åˆ™ç”¨ raw target
        if 'rank_label' in df_sub.columns:
            labels = df_sub['rank_label'].values
        else:
            labels = df_sub['target'].values

        # è¯†åˆ« excess_label
        has_excess = 'excess_label' in df_sub.columns
        excess_vals = df_sub['excess_label'].values if has_excess else df_sub['target'].values

        # è·å–æ¯ä¸ª code çš„åˆ‡ç‰‡ä½ç½®ï¼Œæ›¿ä»£ groupby ä»¥æå‡æ€§èƒ½
        # å‰æï¼šdf_sub å·²ç»æŒ‰ code, date æ’åº (DataProvider ä¿è¯äº†è¿™ç‚¹)
        # åˆ©ç”¨ pandas çš„ index ç‰¹æ€§æˆ–è€… numpy diff æ‰¾è¾¹ç•Œ
        unique_codes, code_indices = np.unique(codes, return_index=True)
        # è¿½åŠ æœ€åä¸€ä¸ªç´¢å¼•ä½œä¸ºç»“æŸè¾¹ç•Œ
        code_indices = np.append(code_indices, len(codes))

        seq_len = Config.CONTEXT_LEN
        batch_size = Config.ANALYSIS_BATCH_SIZE

        # éå†æ¯åªè‚¡ç¥¨
        for k in tqdm(range(len(unique_codes)), desc="Processing Stocks"):
            start_pos = code_indices[k]
            end_pos = code_indices[k + 1]

            # è¯¥è‚¡ç¥¨çš„æ•°æ®é•¿åº¦
            series_len = end_pos - start_pos
            if series_len < seq_len:
                continue

            # å‘é‡åŒ–æ„å»ºåˆ‡ç‰‡ç´¢å¼•
            # æˆ‘ä»¬éœ€è¦é¢„æµ‹çš„æ—¶é—´ç‚¹ç´¢å¼•ï¼šä» (start + seq_len - 1) åˆ° (end - 1)
            # å¯¹åº”çš„è¾“å…¥çª—å£èµ·å§‹ç‚¹ï¼šä» start åˆ° (end - seq_len)

            # ç­›é€‰ç¬¦åˆ date èŒƒå›´çš„ç´¢å¼•
            curr_dates = dates[start_pos + seq_len - 1: end_pos]
            valid_mask = (curr_dates >= np.datetime64(self.start_date)) & \
                         (curr_dates <= np.datetime64(self.end_date))

            if not np.any(valid_mask):
                continue

            # ç›¸å¯¹åç§»é‡
            valid_offsets = np.where(valid_mask)[0]

            # æ„å»º Batch
            for offset in valid_offsets:
                # ç»å¯¹ç´¢å¼•
                pred_idx = start_pos + seq_len - 1 + offset
                window_start = start_pos + offset
                window_end = window_start + seq_len

                batch_inputs.append(feat_vals[window_start:window_end])

                batch_meta.append({
                    'date': dates[pred_idx],
                    'code': codes[pred_idx],
                    'rank_label': labels[pred_idx],
                    'excess_label': excess_vals[pred_idx]
                })

                if len(batch_inputs) >= batch_size:
                    self._flush_batch(model, batch_inputs, batch_meta, all_results)
                    batch_inputs = []
                    batch_meta = []

        # å¤„ç†å‰©ä½™å°¾éƒ¨æ•°æ®
        if batch_inputs:
            self._flush_batch(model, batch_inputs, batch_meta, all_results)

        self.results_df = pd.DataFrame(all_results)
        # è½¬æ¢æ—¥æœŸæ ¼å¼ç¡®ä¿å¯¹é½
        self.results_df['date'] = pd.to_datetime(self.results_df['date'])
        print(f"âœ… æ¨ç†å®Œæˆï¼Œç”Ÿæˆ {len(self.results_df)} æ¡é¢„æµ‹è®°å½•ã€‚")

    def _flush_batch(self, model, inputs, meta, results_list):
        """æ‰¹é‡æ¨ç†å¹¶å›å¡«ç»“æœ"""
        tensor = torch.tensor(np.array(inputs), dtype=torch.float32).to(self.device)
        with torch.no_grad():
            outputs = model(past_values=tensor)
            # å…¼å®¹ä¸åŒç»´åº¦çš„è¾“å‡º
            scores = outputs.logits
            if scores.dim() > 1:
                scores = scores.squeeze()
            scores = scores.cpu().numpy()

        # å¤„ç†æ ‡é‡æˆ–å•æ ·æœ¬æƒ…å†µ
        if scores.ndim == 0:
            scores = [scores]

        # å®‰å…¨å¯¹é½
        limit = min(len(meta), len(scores))
        for i in range(limit):
            meta[i]['score'] = float(scores[i])
            results_list.append(meta[i])

    def analyze_performance(self):
        """
        [Step 2] è®¡ç®—æ ¸å¿ƒæŒ‡æ ‡ (IC, ICIR, WinRate) å¹¶ç»˜å›¾
        """
        if self.results_df is None or self.results_df.empty:
            print("âš ï¸ ç»“æœé›†ä¸ºç©ºï¼Œæ— æ³•åˆ†æ")
            return

        df = self.results_df.copy()

        print("\n>>> [Metrics] è®¡ç®—æ¯æ—¥æˆªé¢æŒ‡æ ‡...")

        # ----------------------------------------------------------------------
        # 1. é«˜æ•ˆ IC è®¡ç®— (ä½¿ç”¨ Groupby + Rank + Corr æ›¿ä»£ å¾ªç¯ Spearmanr)
        # Spearman IC æœ¬è´¨ä¸Šå°±æ˜¯ Rank åçš„ Pearson IC
        # ----------------------------------------------------------------------
        # å…ˆåœ¨ç»„å†…è®¡ç®— Rank
        df['score_rank'] = df.groupby('date')['score'].rank(pct=True)
        df['label_rank'] = df.groupby('date')['rank_label'].rank(pct=True)

        # è®¡ç®—æ¯æ—¥ç›¸å…³æ€§ (Rank IC)
        daily_ic = df.groupby('date').apply(
            lambda x: x['score_rank'].corr(x['label_rank'])
        )

        # ----------------------------------------------------------------------
        # 2. æ ¸å¿ƒæŒ‡æ ‡ç»Ÿè®¡
        # ----------------------------------------------------------------------
        ic_mean = daily_ic.mean()
        ic_std = daily_ic.std()

        # å¹´åŒ– ICIR = Mean / Std * sqrt(252)
        icir = ic_mean / (ic_std + 1e-9) * np.sqrt(252)

        # IC èƒœç‡
        ic_win_rate = (daily_ic > 0).mean()

        # æ‰“å°ä½“æ£€æŠ¥å‘Š
        print("-" * 50)
        print(f"ğŸ“Š ã€å› å­æ·±åº¦ç»©æ•ˆæŠ¥å‘Šã€‘ ({self.start_date.date()} ~ {self.end_date.date()})")
        print("-" * 50)
        print(f"Rank IC (Mean) : {ic_mean:.4f}   (å‚è€ƒ: >0.03 ä¼˜ç§€)")
        print(f"ICIR (Annual)  : {icir:.4f}     (å‚è€ƒ: >1.00 ç¨³å®š)")
        print(f"IC Win Rate    : {ic_win_rate:.2%}   (å‚è€ƒ: >55%  èƒœç‡)")
        print(f"IC Std Dev     : {ic_std:.4f}")
        print("-" * 50)

        # ----------------------------------------------------------------------
        # 3. ç”Ÿæˆå¯è§†åŒ–æŠ¥è¡¨
        # ----------------------------------------------------------------------
        self._plot_results(df, daily_ic, ic_mean, icir, ic_win_rate)

    def _plot_results(self, df, daily_ic, ic_mean, icir, ic_win_rate):
        """
        [Step 3] ç»˜åˆ¶æ·±åº¦åˆ†æå›¾è¡¨
        """
        plt.figure(figsize=(16, 12))

        # --- å­å›¾ 1: ç´¯ç§¯ IC æ›²çº¿ (Cumulative IC) ---
        # å®ƒæ˜¯åˆ¤æ–­å› å­ç¨³å®šæ€§çš„é‡‘æ ‡å‡†ï¼Œæ–œç‡è¶Šç¨³å®šè¶Šå¥½
        ax1 = plt.subplot(3, 1, 1)
        daily_ic_cumsum = daily_ic.cumsum()
        ax1.plot(daily_ic_cumsum.index, daily_ic_cumsum.values, label='Cumulative Rank IC', color='#4B0082',
                 linewidth=1.5)
        ax1.set_title(f'Cumulative Rank IC (ICIR={icir:.2f})', fontsize=12, fontweight='bold')
        ax1.grid(True, linestyle='--', alpha=0.4)
        ax1.legend(loc='upper left')

        # --- å­å›¾ 2: æ¯æ—¥ IC åˆ†å¸ƒæŸ±çŠ¶å›¾ ---
        ax2 = plt.subplot(3, 1, 2)
        colors = ['#d32f2f' if v < 0 else '#388e3c' for v in daily_ic.values]  # çº¢ç»¿æŸ±
        ax2.bar(daily_ic.index, daily_ic.values, color=colors, alpha=0.6, width=1.0, label='Daily IC')
        ax2.axhline(ic_mean, color='blue', linestyle='--', linewidth=1.5, label=f'Mean IC: {ic_mean:.3f}')
        ax2.axhline(0, color='black', linewidth=0.8)
        ax2.set_title(f'Daily IC Distribution (Win Rate={ic_win_rate:.1%})', fontsize=12, fontweight='bold')
        ax2.legend(loc='upper right')
        ax2.grid(True, axis='y', linestyle='--', alpha=0.4)

        # --- å­å›¾ 3: åˆ†å±‚ç´¯è®¡æ”¶ç›Šæ›²çº¿ (Layered Backtest) ---
        ax3 = plt.subplot(3, 1, 3)

        # è®¡ç®—åˆ†å±‚æ”¶ç›Š
        # å°† score åˆ†æˆ 5 ç»„ (Group 0: Worst, Group 4: Best)
        # æ³¨æ„ï¼šduplicates='drop' é˜²æ­¢åˆ†æ•°è¿‡äºé›†ä¸­å¯¼è‡´åˆ‡åˆ†å¤±è´¥
        df['group'] = df.groupby('date')['score'].transform(
            lambda x: pd.qcut(x, 5, labels=False, duplicates='drop')
        )

        # è®¡ç®—æ¯ç»„æ¯æ—¥çš„å¹³å‡ excess_label
        layer_ret = df.groupby(['date', 'group'])['excess_label'].mean().unstack()

        # [Critical Fix] ä¿®æ­£å¤šæ—¥é¢„æµ‹å¸¦æ¥çš„æ”¶ç›Šé‡å 
        # å¦‚æœé¢„æµ‹çš„æ˜¯æœªæ¥ 5 æ—¥æ”¶ç›Šï¼Œæ¯æ—¥ç´¯ä¹˜ä¼šå¯¼è‡´æ”¶ç›Šè¢«æ”¾å¤§ 5 å€
        # è¿™é‡Œè¿›è¡Œç®€å•çš„çº¿æ€§å¹³æ‘Šï¼Œæ¨¡æ‹Ÿæ—¥é¢‘æ”¶ç›Š
        if Config.PRED_LEN > 1:
            layer_ret = layer_ret / Config.PRED_LEN

        layer_ret = layer_ret.fillna(0)
        cum_ret = (1 + layer_ret).cumprod()

        # ç»˜å›¾é€»è¾‘
        groups = sorted(layer_ret.columns)
        cmap = plt.get_cmap('RdYlGn_r')  # é€†åºï¼šçº¢(Top) -> ç»¿(Bottom)

        for idx, g in enumerate(groups):
            if g == groups[-1]:
                label, c, lw, alpha = "Top 20% (Long)", "red", 2.0, 1.0
            elif g == groups[0]:
                label, c, lw, alpha = "Bottom 20% (Short)", "green", 1.5, 0.8
            else:
                label, c, lw, alpha = f"Group {g}", "gray", 0.8, 0.3

            ax3.plot(cum_ret.index, cum_ret[g], label=label, color=c, linewidth=lw, alpha=alpha)

        # ç»˜åˆ¶å¤šç©ºæ›²çº¿ (Long - Short)
        if len(groups) >= 2:
            ls_ret = layer_ret[groups[-1]] - layer_ret[groups[0]]
            ls_cum = (1 + ls_ret).cumprod()
            ax3.plot(ls_cum.index, ls_cum, label='Long-Short Alpha', color='blue', linestyle='--', linewidth=1.5)

        ax3.set_title(f'Layered Backtest (Avg Daily Return derived from {Config.PRED_LEN}-Day Horizon)', fontsize=12,
                      fontweight='bold')
        ax3.legend(loc='upper left', ncol=2)
        ax3.grid(True, linestyle='--', alpha=0.4)

        plt.tight_layout()
        save_path = os.path.join(Config.OUTPUT_DIR, "factor_comprehensive_report.png")
        plt.savefig(save_path, dpi=150)
        print(f"ğŸ“ˆ å›¾è¡¨å·²ä¿å­˜è‡³: {save_path}")


if __name__ == "__main__":
    # å•å…ƒæµ‹è¯•
    analyzer = BacktestAnalyzer(start_date='2024-01-01', end_date='2024-12-31')
    analyzer.generate_historical_predictions()
    analyzer.analyze_performance()