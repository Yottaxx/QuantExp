import akshare as ak
import pandas as pd
import os
import glob
import numpy as np
import time
import random
import threading
import datetime
import concurrent.futures
import pickle
import warnings
from datasets import Dataset, DatasetDict
from tqdm import tqdm
from .config import Config
from .vpn_rotator import vpn_rotator
from .alpha_lib import AlphaFactory
from pandarallel import pandarallel
import json
# å¿½ç•¥ pandas çš„æ€§èƒ½è­¦å‘Š
warnings.simplefilter(action='ignore', category=pd.errors.PerformanceWarning)

# åˆå§‹åŒ–å¹¶è¡Œè®¡ç®— (åˆ©ç”¨å¤šæ ¸åŠ é€Ÿ pandas apply)
pandarallel.initialize(progress_bar=True, nb_workers=os.cpu_count())




# =========================================================================
# Utils: ä¸‹è½½è®°å½•å™¨ä¸è¡Œä¸šç¼–ç å™¨
# =========================================================================

class DownloadRecorder:
    """è®°å½•ä¸‹è½½å¤±è´¥çš„æ–‡ä»¶ï¼Œæ”¯æŒå¯¼å‡ºä¾›ä¸‹æ¬¡é‡è¯•"""

    def __init__(self, log_path=None):
        self.log_path = log_path or os.path.join(Config.DATA_DIR, "download_failures.json")
        self._lock = threading.Lock()
        self.failed_tasks = {
            "price": [],
            "finance": [],
            "info": []
        }

    def log(self, category, code, reason):
        with self._lock:
            self.failed_tasks[category].append({
                "code": code,
                "reason": str(reason),
                "time": datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            })

    def save(self):
        """ä¿å­˜å¤±è´¥è®°å½•åˆ° JSON"""
        if any(self.failed_tasks.values()):
            with open(self.log_path, 'w', encoding='utf-8') as f:
                json.dump(self.failed_tasks, f, indent=4, ensure_ascii=False)
            print(f"âš ï¸ å­˜åœ¨ä¸‹è½½å¤±è´¥çš„ä»»åŠ¡ï¼Œå·²è®°å½•è‡³: {self.log_path}")
        else:
            if os.path.exists(self.log_path):
                os.remove(self.log_path)  # å¦‚æœå…¨éƒ¨æˆåŠŸï¼Œæ¸…é™¤æ—§æ—¥å¿—


class IndustryEncoder:
    """æŒä¹…åŒ–è¡Œä¸šç¼–ç æ˜ å°„ï¼Œä¿è¯ One-Hot/Embedding ID çš„ä¸€è‡´æ€§"""

    def __init__(self, map_path=None):
        self.map_path = map_path or os.path.join(Config.DATA_DIR, "industry_map.json")
        self.mapping = self._load_mapping()
        self._lock = threading.Lock()

    def _load_mapping(self):
        if os.path.exists(self.map_path):
            with open(self.map_path, 'r', encoding='utf-8') as f:
                return json.load(f)
        return {"Unknown": 0}  # 0 å·ä½ç•™ç»™æœªçŸ¥

    def save_mapping(self):
        with open(self.map_path, 'w', encoding='utf-8') as f:
            json.dump(self.mapping, f, indent=4, ensure_ascii=False)
        print(f"âœ… è¡Œä¸šç¼–ç è¡¨å·²æ›´æ–°: {self.map_path}")

    def encode(self, industries):
        """
        å°†è¡Œä¸šåˆ—è¡¨è½¬æ¢ä¸º IDï¼Œå¦‚æœé‡åˆ°æ–°è¡Œä¸šè‡ªåŠ¨æ·»åŠ åˆ°æ˜ å°„è¡¨
        """
        ids = []
        is_updated = False

        # é¢„æ£€æŸ¥ï¼Œé¿å…é¢‘ç¹åŠ é”
        current_keys = set(self.mapping.keys())
        new_industries = set(industries) - current_keys

        if new_industries:
            with self._lock:
                # äºŒæ¬¡æ£€æŸ¥
                max_id = max(self.mapping.values()) if self.mapping else -1
                for ind in new_industries:
                    if ind not in self.mapping:
                        max_id += 1
                        self.mapping[ind] = max_id
                        is_updated = True

        if is_updated:
            self.save_mapping()

        # è½¬æ¢
        return [self.mapping.get(i, 0) for i in industries]


# =========================================================================
# 1. NetworkManager: ç½‘ç»œä¸åçˆ¬å¯¹æŠ—å±‚
# =========================================================================
class NetworkManager:
    _vpn_lock = threading.Lock()
    _last_switch_time = 0

    @staticmethod
    def setup_proxy_env():
        """é…ç½®ç³»ç»Ÿä»£ç†ç¯å¢ƒå˜é‡ï¼Œç¡®ä¿ akshare è¯·æ±‚èµ°ä»£ç†æ± """
        proxy_url = Config.PROXY_URL
        if proxy_url:
            for k in ['http_proxy', 'https_proxy', 'all_proxy', 'HTTP_PROXY', 'HTTPS_PROXY', 'ALL_PROXY']:
                os.environ[k] = proxy_url

    @classmethod
    def safe_switch_vpn(cls):
        """çº¿ç¨‹å®‰å…¨çš„ VPN åˆ‡æ¢é€»è¾‘ (å†·å´æ—¶é—´ 5s)"""
        with cls._vpn_lock:
            if time.time() - cls._last_switch_time < 5: return
            try:
                print("ğŸ”„ [Network] æ£€æµ‹åˆ°åçˆ¬/å°ç¦ï¼Œæ­£åœ¨åˆ‡æ¢ IP çº¿è·¯ ...")
                vpn_rotator.switch_random()
            except Exception as e:
                print(f"âš ï¸ VPN åˆ‡æ¢å¼‚å¸¸: {e}")
            cls._last_switch_time = time.time()
            time.sleep(3)  # ç­‰å¾…ç½‘ç»œç¨³å®š


# =========================================================================
# 2. DataDownloader: ETL å±‚ (Extract, Transform, Load)
# =========================================================================

def to_em_symbol(x: str) -> str:
    s = (x or "").strip().upper()
    if s.endswith((".SZ", ".SH", ".BJ")):
        return s
    if x[0] in ("6", "9"):
        return f"{x}.SH"
    if x[0] in ("0", "3"):
        return f"{x}.SZ"
    if x[0] in ("8", "4"):
        return f"{x}.BJ"
    return f"{x}.SZ"




class DataDownloader:
    recorder = DownloadRecorder()  # å®ä¾‹åŒ–è®°å½•å™¨
    @staticmethod
    def _get_latest_trading_date():
        """è·å–æœ€è¿‘ä¸€ä¸ªäº¤æ˜“æ—¥ï¼Œç”¨äºåˆ¤æ–­æ•°æ®æ˜¯å¦éœ€è¦æ›´æ–°"""
        try:
            df = ak.stock_zh_index_daily(symbol=Config.MARKET_INDEX_SYMBOL)
            return pd.to_datetime(df['date']).max().date().strftime("%Y-%m-%d")
        except:
            return datetime.date.today().strftime("%Y-%m-%d")

    @staticmethod
    def _read_parquet_safe(path):
        """
        [å®‰å…¨è¯»å–] å°è¯•è¯»å– Parquetï¼Œå¦‚æœæ–‡ä»¶æŸååˆ™åˆ é™¤å¹¶è¿”å› None
        """
        if not os.path.exists(path): return None
        try:
            return pd.read_parquet(path)
        except Exception as e:
            print(f"âš ï¸ å‘ç°æŸåæ–‡ä»¶ï¼Œå·²åˆ é™¤å¹¶å‡†å¤‡é‡è¯•: {path} ({e})")
            try:
                os.remove(path)
            except:
                pass
            return None

    @staticmethod
    def _fetch_pub_date_map(code):
        """
        è·å–è´¢æŠ¥å…¬å‘Šæ—¥æ˜ å°„ (PIT Data Core)
        ç”¨äºå°†è´¢æŠ¥æ•°æ®å¯¹é½åˆ°å…¶çœŸå®çš„å‘å¸ƒæ—¥æœŸï¼Œè€ŒéæŠ¥å‘ŠæœŸæœ«ã€‚
        """
        try:
            df = ak.stock_financial_abstract(symbol=code)
            if df is None or df.empty: return None
            df.columns = [c.strip() for c in df.columns]
            if 'æˆªæ­¢æ—¥æœŸ' in df.columns and 'å…¬å‘Šæ—¥æœŸ' in df.columns:
                res = df[['æˆªæ­¢æ—¥æœŸ', 'å…¬å‘Šæ—¥æœŸ']].copy()
                res.columns = ['date', 'pub_date']
                res['date'] = pd.to_datetime(res['date'], errors='coerce')
                res['pub_date'] = pd.to_datetime(res['pub_date'], errors='coerce')
                return res.dropna()
        except:
            pass
        return None


    @staticmethod
    def _download_finance_worker(code):
        """
        ä¸‹è½½è´¢åŠ¡æ•°æ®ï¼ˆEastmoney/AKShare: stock_financial_analysis_indicator_emï¼‰
        å¯¹é½ FundamentalPipeline çš„ç¨³å®š schemaï¼š
          date, roe, rev_growth, profit_growth, debt_ratio, eps, bps, pub_date
        """
        fund_dir = os.path.join(Config.DATA_DIR, "fundamental")
        if not os.path.exists(fund_dir):
            os.makedirs(fund_dir)
        path = os.path.join(fund_dir, f"{code}.parquet")

        # ç¼“å­˜æ£€æŸ¥: 3å¤©å†…ä¸é‡å¤ä¸‹è½½
        if os.path.exists(path):
            mtime = os.path.getmtime(path)
            if (time.time() - mtime) < 3 * 24 * 3600:
                return code, True, "Skipped"

        # ç¨³å®š schema
        numeric_cols = ["roe", "rev_growth", "profit_growth", "debt_ratio", "eps", "bps"]
        final_cols = ["date"] + numeric_cols + ["pub_date"]

        for attempt in range(5):
            try:
                time.sleep(random.uniform(0.5, 1.2))  # ç¤¼è²Œè¯·æ±‚

                df = ak.stock_financial_analysis_indicator_em(symbol=to_em_symbol(code), indicator="æŒ‰æŠ¥å‘ŠæœŸ")
                if df is None or df.empty:
                    return code, True, "Empty"

                # --- 1) report date -> date ---
                date_col = "REPORT_DATE" if "REPORT_DATE" in df.columns else (
                    "æŠ¥å‘ŠæœŸ" if "æŠ¥å‘ŠæœŸ" in df.columns else None)
                if date_col is None:
                    raise ValueError("Missing report date column (REPORT_DATE/æŠ¥å‘ŠæœŸ)")
                df["date"] = pd.to_datetime(df[date_col], errors="coerce")
                df.dropna(subset=["date"], inplace=True)

                # --- 2) pick fields (pipeline-style keys) ---
                pick = {
                    "roe": ("ROEJQ", "ROEKCJQ"),
                    "rev_growth": ("TOTALOPERATEREVETZ",),
                    "profit_growth": ("PARENTNETPROFITTZ",),
                    "debt_ratio": ("ZCFZL",),
                    "eps": ("EPSJB", "EPSJQ", "EPS"),
                    "bps": ("BPS",),
                }

                for k, cands in pick.items():
                    src = next((c for c in cands if c in df.columns), None)
                    df[k] = pd.to_numeric(df[src], errors="coerce") if src else np.nan

                # need debug

                # --- 3) YoY fallback (only when growth fully missing) ---
                df = df.sort_values("date")
                df = df.drop_duplicates(subset=["date"], keep="last")

                idx = pd.DatetimeIndex(df["date"])

                if df["rev_growth"].isna().all() and "TOTALOPERATEREVE" in df.columns:
                    cur = pd.to_numeric(df["TOTALOPERATEREVE"], errors="coerce").to_numpy(dtype=np.float64)
                    prev = pd.Series(cur, index=idx).reindex(idx - pd.DateOffset(years=1)).to_numpy(dtype=np.float64)
                    prev = np.where(prev == 0.0, np.nan, prev)
                    df["rev_growth"] = ((cur / prev - 1.0) * 100.0).astype(np.float32)

                if df["profit_growth"].isna().all() and "PARENTNETPROFIT" in df.columns:
                    cur = pd.to_numeric(df["PARENTNETPROFIT"], errors="coerce").to_numpy(dtype=np.float64)
                    prev = pd.Series(cur, index=idx).reindex(idx - pd.DateOffset(years=1)).to_numpy(dtype=np.float64)
                    prev = np.where(prev == 0.0, np.nan, prev)
                    df["profit_growth"] = ((cur / prev - 1.0) * 100.0).astype(np.float32)

                # --- 4) pub_date (pipeline: map > NOTICE/UPDATE > estimate) ---
                pub_df = DataDownloader._fetch_pub_date_map(code)
                if pub_df is not None and not pub_df.empty and "pub_date" in pub_df.columns:
                    df = pd.merge(df, pub_df[["date", "pub_date"]], on="date", how="left")
                elif "NOTICE_DATE" in df.columns:
                    df["pub_date"] = pd.to_datetime(df["NOTICE_DATE"], errors="coerce")
                elif "UPDATE_DATE" in df.columns:
                    df["pub_date"] = pd.to_datetime(df["UPDATE_DATE"], errors="coerce")
                else:
                    df["pub_date"] = pd.NaT

                # estimate pub_date for missing (no extra dependency, same idea as pipeline)
                miss = df["pub_date"].isna()
                if miss.any():
                    d = pd.to_datetime(df.loc[miss, "date"], errors="coerce")
                    y, m, day = d.dt.year, d.dt.month, d.dt.day
                    est = pd.Series(pd.NaT, index=d.index, dtype="datetime64[ns]")
                    est.loc[(m == 3) & (day == 31)] = pd.to_datetime(y[(m == 3) & (day == 31)].astype(str) + "-04-30")
                    est.loc[(m == 6) & (day == 30)] = pd.to_datetime(y[(m == 6) & (day == 30)].astype(str) + "-08-31")
                    est.loc[(m == 9) & (day == 30)] = pd.to_datetime(y[(m == 9) & (day == 30)].astype(str) + "-10-31")
                    est.loc[(m == 12) & (day == 31)] = pd.to_datetime(
                        (y[(m == 12) & (day == 31)] + 1).astype(str) + "-04-30")
                    df.loc[miss, "pub_date"] = est

                # --- 5) finalize schema + types ---
                for c in numeric_cols:
                    df[c] = pd.to_numeric(df[c], errors="coerce").astype(np.float32)

                df = df[final_cols].copy()
                df = df.sort_values("date").set_index("date")
                df.to_parquet(path)

                return code, True, "Success"

            except Exception as e:
                err_str = str(e)
                if attempt < 4 and any(
                        k in err_str for k in ["404", "429", "502", "503", "Connection", "timed out", "NoneType"]):
                    NetworkManager.safe_switch_vpn()
                    time.sleep(2)
                    continue
                if attempt == 4:
                    print(f"âš ï¸ [Fail] {code} Finance: {e}")
                continue

        return code, False, "Failed"

    @staticmethod
    def _download_worker(code):
        """
        ä¸‹è½½æ—¥é¢‘è¡Œæƒ… (æ”¯æŒå¢é‡æ›´æ–°æ–­ç‚¹ç»­ä¼ )
        """
        path = os.path.join(Config.DATA_DIR, f"{code}.parquet")
        start_date = Config.START_DATE
        old_df = None

        # --- å¢é‡æ›´æ–°é€»è¾‘ ---
        if os.path.exists(path):
            try:
                old_df = pd.read_parquet(path)
                if not old_df.empty:
                    # è·å–æœ¬åœ°æœ€æ–°æ—¥æœŸ
                    last_date = old_df.index.max()
                    # å¦‚æœæœ¬åœ°æœ€æ–°æ—¥æœŸ >= æ˜¨å¤©ï¼Œå¤§æ¦‚ç‡ä¸éœ€è¦æ›´æ–°ï¼ˆè¿™é‡Œç®€åŒ–åˆ¤æ–­ï¼Œä¸¥è°¨å¯ç”¨ calendarï¼‰
                    if last_date.date() >= (datetime.date.today() - datetime.timedelta(days=1)):
                        return code, True, "Up-to-date"

                    # è®¾ç½®æ–°çš„ä¸‹è½½èµ·ç‚¹ = æœ¬åœ°æœ€åæ—¥æœŸ + 1å¤©
                    start_date = (last_date + datetime.timedelta(days=1)).strftime("%Y%m%d")
            except Exception as e:
                print(f"âš ï¸ æ–‡ä»¶æŸåï¼Œé‡æ–°ä¸‹è½½: {code} ({e})")
                os.remove(path)
                old_df = None

        # å¦‚æœ start_date è¶…è¿‡äº†ä»Šå¤©ï¼Œè¯´æ˜ä¸ç”¨æ›´æ–°
        if start_date > datetime.date.today().strftime("%Y%m%d"):
            return code, True, "Skipped"

        for attempt in range(5):
            try:
                time.sleep(random.uniform(0.05, 0.2))

                # ä¸‹è½½å¢é‡æ•°æ®
                new_df = ak.stock_zh_a_hist(symbol=code, period="daily", start_date=start_date, adjust="qfq")

                if new_df is None or new_df.empty:
                    # å¦‚æœæ²¡æœ‰æ–°æ•°æ®ï¼Œä¸”æœ¬æ¥å°±æœ‰è€æ•°æ®ï¼Œè§†ä½œæˆåŠŸ
                    return code, True, "No New Data" if old_df is not None else "Empty"

                # æ ‡å‡†åŒ–åˆ—å
                new_df.rename(columns={
                    'æ—¥æœŸ': 'date', 'å¼€ç›˜': 'open', 'æ”¶ç›˜': 'close',
                    'æœ€é«˜': 'high', 'æœ€ä½': 'low',
                    'æˆäº¤é‡': 'volume', 'æˆäº¤é¢': 'amount',
                    'æ¢æ‰‹ç‡': 'turnover'
                }, inplace=True)
                new_df['date'] = pd.to_datetime(new_df['date'])
                new_df.set_index('date', inplace=True)

                # ç±»å‹è½¬æ¢
                cols = ['open', 'close', 'high', 'low', 'volume', 'amount', 'turnover']
                for col in cols:
                    if col in new_df.columns:
                        new_df[col] = pd.to_numeric(new_df[col], errors='coerce').astype(np.float32)

                new_df.dropna(inplace=True)

                # åˆå¹¶é€»è¾‘
                if old_df is not None:
                    # è¿‡æ»¤æ‰é‡å æ—¥æœŸ (ä»¥é˜²ä¸‡ä¸€)
                    new_df = new_df[new_df.index > old_df.index.max()]
                    if new_df.empty: return code, True, "Up-to-date"
                    final_df = pd.concat([old_df, new_df])
                else:
                    final_df = new_df

                if 'amount' in final_df.columns: final_df.drop(columns=['amount'], inplace=True)

                if not final_df.empty:
                    final_df.sort_index(inplace=True)
                    # åŸå­å†™å…¥é˜²æ­¢ä¸­æ–­å¯¼è‡´æ–‡ä»¶æŸå
                    temp_path = path + ".tmp"
                    final_df.to_parquet(temp_path)
                    if os.path.exists(path): os.remove(path)
                    os.rename(temp_path, path)

                return code, True, "Success"

            except Exception as e:
                if attempt == 4:
                    # è®°å½•å¤±è´¥
                    DataDownloader.recorder.log("price", code, e)
                NetworkManager.safe_switch_vpn()
                continue

        return code, False, "Failed"

    @staticmethod
    def _download_info_worker(code):
        """
        [æ–°å¢] ä¸‹è½½ä¸ªè‚¡é™æ€ä¿¡æ¯ï¼ˆè¡Œä¸šã€ä¸Šå¸‚æ—¥æœŸã€æ€»å¸‚å€¼ï¼‰
        ç”¨äºè¡Œä¸šä¸­æ€§åŒ– (Sector Neutralization) å’Œ è‚¡ç¥¨æ± ç­›é€‰
        """
        info_dir = os.path.join(Config.DATA_DIR, "info")
        if not os.path.exists(info_dir): os.makedirs(info_dir)
        path = os.path.join(info_dir, f"{code}.parquet")

        # é™æ€æ•°æ®ç¼“å­˜ 30 å¤©
        if os.path.exists(path):
            mtime = os.path.getmtime(path)
            if (time.time() - mtime) < 30 * 24 * 3600:
                return code, True, "Skipped"

        for attempt in range(5):
            try:
                time.sleep(random.uniform(0.1, 0.3))
                df = ak.stock_individual_info_em(symbol=code)
                if df is None or df.empty: return code, True, "Empty"

                # è½¬ç½® kv ä¸º row
                info_dict = dict(zip(df['item'], df['value']))

                clean_data = {
                    'code': code,
                    'name': info_dict.get('è‚¡ç¥¨ç®€ç§°', 'Unknown'),
                    'industry': info_dict.get('è¡Œä¸š', 'Unknown'),
                    'list_date': str(info_dict.get('ä¸Šå¸‚æ—¶é—´', '19900101')),
                    'total_mkt_cap': float(info_dict.get('æ€»å¸‚å€¼', 0))
                }

                res_df = pd.DataFrame([clean_data])
                res_df.to_parquet(path)
                return code, True, "Success"
            except Exception:
                NetworkManager.safe_switch_vpn()
                continue

        return code, False, "Failed"

    @staticmethod
    def _get_stock_list():
        """è·å–å…¨å¸‚åœºè‚¡ç¥¨åˆ—è¡¨ (å¸¦ç¼“å­˜)"""
        cache_file = os.path.join(Config.DATA_DIR, "stock_list.pkl")
        if os.path.exists(cache_file):
            # 12å°æ—¶æœ‰æ•ˆ
            if time.time() - os.path.getmtime(cache_file) < 12 * 3600:
                print(f"âš¡ï¸ [Cache] è¯»å–æœ¬åœ°è‚¡ç¥¨åˆ—è¡¨ç¼“å­˜")
                with open(cache_file, 'rb') as f: return pickle.load(f)

        print("ğŸŒ [Network] è·å–æœ€æ–°è‚¡ç¥¨åˆ—è¡¨...")
        for attempt in range(10):
            try:
                stock_info = ak.stock_zh_a_spot_em()
                if stock_info is not None and not stock_info.empty:
                    codes = stock_info['ä»£ç '].tolist()
                    # è¿‡æ»¤éAè‚¡
                    codes = [c for c in codes if c.startswith(('00', '60', '30', '68'))]
                    with open(cache_file, 'wb') as f: pickle.dump(codes, f)
                    return codes
            except Exception:
                NetworkManager.safe_switch_vpn()
                time.sleep(2)

        # Fallback
        if os.path.exists(cache_file):
            print("âš ï¸ [Fallback] ä½¿ç”¨æ—§ç¼“å­˜åˆ—è¡¨")
            with open(cache_file, 'rb') as f: return pickle.load(f)
        return []

    @staticmethod
    def run():
        """æ‰§è¡Œå…¨é‡ ETL ä»»åŠ¡"""
        print(">>> [ETL] å¯åŠ¨æ•°æ®ä¸‹è½½æµæ°´çº¿...")
        NetworkManager.setup_proxy_env()
        if not os.path.exists(Config.DATA_DIR): os.makedirs(Config.DATA_DIR)

        codes = DataDownloader._get_stock_list()
        if not codes: return

        target_date_str = DataDownloader._get_latest_trading_date()

        # 1. æ£€æŸ¥è¡Œæƒ…æ•°æ®æ›´æ–°æƒ…å†µ
        existing_fresh = set()
        for fname in os.listdir(Config.DATA_DIR):
            if fname.endswith(".parquet"):
                try:
                    mtime = os.path.getmtime(os.path.join(Config.DATA_DIR, fname))
                    if datetime.date.fromtimestamp(mtime).strftime("%Y-%m-%d") >= target_date_str:
                        existing_fresh.add(fname.replace(".parquet", ""))
                except:
                    pass

        todo_price = sorted(list(set(codes) - existing_fresh))
        print(f"ğŸ“Š è‚¡ç¥¨æ± : {len(codes)} | å¾…æ›´æ–°è¡Œæƒ…: {len(todo_price)}")

        # # 2. ä¸‹è½½è¡Œæƒ… (Price)
        # if todo_price:
        #     with concurrent.futures.ThreadPoolExecutor(max_workers=16) as executor:
        #         futures = {executor.submit(DataDownloader._download_worker, c): c for c in todo_price}
        #         for _ in tqdm(concurrent.futures.as_completed(futures), total=len(todo_price), desc="Price"): pass

        # 3. ä¸‹è½½è´¢åŠ¡ (Finance)
        print("åŒæ­¥è´¢åŠ¡æ•°æ®...")
        # DataDownloader._download_finance_worker(codes[0])
        with concurrent.futures.ThreadPoolExecutor(max_workers=8) as executor:
            futures = {executor.submit(DataDownloader._download_finance_worker, c): c for c in codes}
            for _ in tqdm(concurrent.futures.as_completed(futures), total=len(codes), desc="Finance"): pass

        # 4. ä¸‹è½½é™æ€ä¿¡æ¯ (Info/Industry)
        # print("åŒæ­¥è¡Œä¸šé™æ€ä¿¡æ¯...")
        # with concurrent.futures.ThreadPoolExecutor(max_workers=16) as executor:
        #     futures = {executor.submit(DataDownloader._download_info_worker, c): c for c in codes}
        #     for _ in tqdm(concurrent.futures.as_completed(futures), total=len(codes), desc="Info"): pass

        print("âœ… æ•°æ®åŒæ­¥å®Œæˆã€‚")


# =========================================================================
# 3. DataProcessor: ç‰¹å¾å·¥ç¨‹ä¸æ•°æ®åˆå¹¶å±‚
# =========================================================================
class DataProcessor:
    @staticmethod
    def get_cache_path(mode, end_date=None):
        today_str = datetime.date.today().strftime("%Y%m%d")
        end_date_str = end_date.replace("-", "") if end_date else "latest"
        return os.path.join(Config.OUTPUT_DIR, f"panel_cache_{mode}_{end_date_str}_{today_str}.pkl")

    @staticmethod
    def _tag_universe(panel_df):
        """
        æ ‡è®°åŠ¨æ€è‚¡ç¥¨æ± 
        è¿‡æ»¤æ‰: åœç‰Œ(volume=0), ä½ä»·è‚¡(<2å…ƒ),ä»¥æ­¤ä¸Šå¸‚æœªæ»¡60å¤©çš„æ¬¡æ–°è‚¡
        """
        print(">>> [Tagging] æ ‡è®° Universe...")
        panel_df = panel_df.sort_values(['code', 'date'])
        panel_df['list_days_count'] = panel_df.groupby('code')['date'].cumcount() + 1

        mask = (panel_df['volume'] > 0) & \
               (panel_df['close'] >= 2.0) & \
               (panel_df['list_days_count'] > 60)

        panel_df['is_universe'] = mask
        panel_df.drop(columns=['list_days_count'], inplace=True)
        return panel_df

    @staticmethod
    def process(mode='train', end_date=None, force_refresh=False):
        """
        æ„å»ºè®­ç»ƒæ•°æ®çš„æ ¸å¿ƒé€»è¾‘
        Steps: Read -> Merge Info -> Merge Fund(PIT) -> Time Cut -> Factor Calc -> CS Factors -> Cache
        """
        cache_path = DataProcessor.get_cache_path(mode, end_date)
        if not force_refresh and os.path.exists(cache_path):
            print(f"âš¡ï¸ [Cache Hit] {cache_path}")
            with open(cache_path, 'rb') as f: return pickle.load(f)

        print(f"\n>>> [Processing] æ„å»º Panel (Mode: {mode})...")

        # Paths
        price_files = glob.glob(os.path.join(Config.DATA_DIR, "*.parquet"))
        fund_dir = os.path.join(Config.DATA_DIR, "fundamental")
        info_dir = os.path.join(Config.DATA_DIR, "info")

        # --- Step 1: è¯»å–è¡Œæƒ… (Parallel) ---
        def _read_price(f):
            try:
                df = pd.read_parquet(f)
                if isinstance(df.index, pd.DatetimeIndex) and 'date' not in df.columns:
                    df = df.reset_index()
                df['code'] = os.path.basename(f).replace(".parquet", "")
                float_cols = df.select_dtypes(include=['float64']).columns
                df[float_cols] = df[float_cols].astype(np.float32)
                return df
            except:
                return None

        with concurrent.futures.ThreadPoolExecutor(max_workers=8) as executor:
            results = list(tqdm(executor.map(_read_price, price_files), total=len(price_files), desc="Reading Price"))

        data_frames = [df for df in results if df is not None and not df.empty]
        if not data_frames: raise ValueError("æ— æœ‰æ•ˆè¡Œæƒ…æ•°æ®")
        panel_df = pd.concat(data_frames, ignore_index=True)
        del data_frames

        panel_df['code'] = panel_df['code'].astype(str)
        panel_df['date'] = pd.to_datetime(panel_df['date'])

        # --- Step 2: ä¸¥æ ¼çš„æ—¶é—´æˆªæ–­ (Time Travel Prevention) ---
        if end_date:
            print(f"âœ‚ï¸  æ‰§è¡Œæ—¶é—´æˆªæ–­: {end_date}")
            panel_df = panel_df[panel_df['date'] <= pd.to_datetime(end_date)]

        # --- Step 3: è¯»å–å¹¶åˆå¹¶è´¢åŠ¡æ•°æ® (PIT Merge) ---
        fund_files = glob.glob(os.path.join(fund_dir, "*.parquet"))

        def _read_fund(f):
            try:
                df = pd.read_parquet(f)
                df['code'] = os.path.basename(f).replace(".parquet", "")
                return df
            except:
                return None

        with concurrent.futures.ThreadPoolExecutor(max_workers=8) as executor:
            fund_frames = [df for df in executor.map(_read_fund, fund_files) if df is not None]

        if fund_frames:
            fund_df = pd.concat(fund_frames)
            fund_df = fund_df.reset_index().sort_values(['code', 'date'])

            # å¡«å……å‘å¸ƒæ—¥æœŸ
            if 'pub_date' in fund_df.columns:
                fund_df['merge_date'] = fund_df['pub_date']
                mask_na = fund_df['merge_date'].isna()
                # é»˜è®¤å»¶è¿Ÿ90å¤©
                fund_df.loc[mask_na, 'merge_date'] = fund_df.loc[mask_na, 'date'] + pd.Timedelta(days=90)
            else:
                fund_df['merge_date'] = fund_df['date'] + pd.Timedelta(days=90)

            fund_df = fund_df.drop(columns=['date', 'pub_date'], errors='ignore')
            fund_df.rename(columns={'merge_date': 'date'}, inplace=True)

            # è´¢åŠ¡æ•°æ®åŒæ ·éœ€è¦æˆªæ–­
            if end_date: fund_df = fund_df[fund_df['date'] <= pd.to_datetime(end_date)]

            # Merge Asof (Point-in-Time)
            panel_df = panel_df.sort_values(['code', 'date'])
            fund_df = fund_df.sort_values(['code', 'date'])
            panel_df = pd.merge_asof(panel_df, fund_df, on='date', by='code', direction='backward')

            # åŠ¨æ€è®¡ç®— PE/PB (ä½¿ç”¨åˆå¹¶åçš„ Price å’Œ EPS/BPS)
            print("è®¡ç®—ä¼°å€¼æŒ‡æ ‡ (PE/PB)...")
            panel_df['eps'] = panel_df['eps'].fillna(0)
            panel_df['bps'] = panel_df['bps'].fillna(0)

            panel_df['pe_ttm'] = np.where(panel_df['eps'] > 0.001, panel_df['close'] / panel_df['eps'], 0)
            panel_df['pb'] = np.where(panel_df['bps'] > 0.001, panel_df['close'] / panel_df['bps'], 0)

            # å¤„ç† Inf
            panel_df.replace([np.inf, -np.inf], 0, inplace=True)

        if 'turnover' in panel_df.columns:
            panel_df['turnover'] = panel_df['turnover'].fillna(0).astype(np.float32)

        # --- Step 4: åˆå¹¶è¡Œä¸šä¿¡æ¯ (Industry Merge with Persistent Mapping) ---
        print("åˆå¹¶è¡Œä¸šä¿¡æ¯å¹¶ç¼–ç ...")
        info_files = glob.glob(os.path.join(info_dir, "*.parquet"))

        # åˆå§‹åŒ–ç¼–ç å™¨
        ind_encoder = IndustryEncoder()  # è‡ªåŠ¨åŠ è½½ data/industry_map.json

        def _read_info(f):
            try:
                return pd.read_parquet(f)
            except:
                return None

        with concurrent.futures.ThreadPoolExecutor(max_workers=8) as executor:
            info_results = list(executor.map(_read_info, info_files))

        info_frames = [df for df in info_results if df is not None and not df.empty]

        if info_frames:
            info_df = pd.concat(info_frames, ignore_index=True)
            info_df['code'] = info_df['code'].astype(str)

            # å¤„ç†ç¼ºå¤±è¡Œä¸š
            info_df['industry'] = info_df['industry'].fillna('Unknown')

            # è¿™é‡Œçš„è¡Œä¸šåˆ—è¡¨å¯èƒ½æ˜¯ ['é“¶è¡Œ', 'åŒ»è¯', 'é“¶è¡Œ'...]
            # ä½¿ç”¨ç¼–ç å™¨ç»Ÿä¸€è½¬æ¢
            unique_industries = info_df['industry'].unique().tolist()
            # é¢„çƒ­ç¼–ç å™¨ï¼Œç¡®ä¿æ‰€æœ‰è¡Œä¸šéƒ½åœ¨ map ä¸­
            ind_encoder.encode(unique_industries)

            # åº”ç”¨æ˜ å°„
            # å»ºè®®ä½¿ç”¨ map é€Ÿåº¦æ›´å¿«ï¼Œencode æ–¹æ³•å†…éƒ¨å·²å¤„ç†äº†æ›´æ–°é€»è¾‘
            industry_map = ind_encoder.mapping
            info_df['industry_cat'] = info_df['industry'].map(industry_map).fillna(0).astype(int)

            panel_df = pd.merge(panel_df, info_df[['code', 'industry_cat']], on='code', how='left')
            # å¯¹äº panel ä¸­æœ‰ä½† info ä¸­æ²¡æœ‰çš„è‚¡ç¥¨ï¼Œå¡«å…… Unknown (0)
            panel_df['industry_cat'] = panel_df['industry_cat'].fillna(0).astype(int)

        else:
            panel_df['industry_cat'] = 0

        # --- Step 5: è®¡ç®—æ—¶åºå› å­ (AlphaFactory) ---
        # æ­¤æ—¶ panel_df å·²ç»åŒ…å« OHLCV, Turnover, Industry, PE/PB
        if 'date' in panel_df.columns: panel_df = panel_df.set_index('date')
        panel_df = panel_df.reset_index().sort_values(['code', 'date'])

        print("è®¡ç®—æ—¶åºå› å­ (Parallel)...")
        panel_df = panel_df.groupby('code', group_keys=False).parallel_apply(lambda x: AlphaFactory(x).make_factors())

        # --- Step 6: æ„é€  Label (Target) ---
        print("æ„é€  Labels...")
        panel_df['next_open'] = panel_df.groupby('code')['open'].shift(-1)
        panel_df['future_close'] = panel_df.groupby('code')['close'].shift(-Config.PRED_LEN)
        panel_df['target'] = panel_df['future_close'] / panel_df['next_open'] - 1
        panel_df.drop(columns=['next_open', 'future_close'], inplace=True)

        if mode == 'train':
            panel_df.dropna(subset=['target'], inplace=True)

        # æ ‡è®° Universe
        panel_df = DataProcessor._tag_universe(panel_df)

        # --- Step 7: æˆªé¢å¤„ç†ä¸ä¸­æ€§åŒ– (Cross-Section) ---
        print("è®¡ç®—æˆªé¢å› å­ä¸ä¸­æ€§åŒ–...")
        panel_df = panel_df.set_index('date')

        # è°ƒç”¨ AlphaFactory çš„é™æ€æ–¹æ³•ï¼Œè¿›è¡Œè¡Œä¸š/å¸‚åœºä¸­æ€§åŒ–
        panel_df = AlphaFactory.add_cross_sectional_factors(panel_df)

        # --- Step 8: ä¿å­˜ ---
        feature_cols = [c for c in panel_df.columns if any(c.startswith(p) for p in Config.FEATURE_PREFIXES)]

        panel_df = panel_df.reset_index()
        # å°†æ•°æ®è½¬ä¸º float32 èŠ‚çœç©ºé—´
        panel_df[feature_cols] = panel_df[feature_cols].fillna(0).replace([np.inf, -np.inf], 0).astype(np.float32)

        with open(cache_path, 'wb') as f:
            pickle.dump((panel_df, feature_cols), f)

        return panel_df, feature_cols


# =========================================================================
# 4. DatasetBuilder: æ•°æ®é›†æ„å»ºå±‚
# =========================================================================
class DatasetBuilder:
    @staticmethod
    def build(panel_df, feature_cols):
        print(">>> [Dataset] Tensor Split (Train/Valid/Test)...")
        panel_df = panel_df.sort_values(['code', 'date']).reset_index(drop=True)

        feature_matrix = panel_df[feature_cols].values.astype(np.float32)
        # ä½¿ç”¨ rank_label (0~1å‡åŒ€åˆ†å¸ƒ) ä½œä¸ºè®­ç»ƒç›®æ ‡ï¼Œæ¯” raw return æ›´ç¨³å®š
        target_array = panel_df['rank_label'].fillna(0.5).values.astype(np.float32)

        universe_mask = panel_df['is_universe'].values
        dates = panel_df['date'].values
        codes = panel_df['code'].values

        # å¿«é€Ÿè®¡ç®—æ¯ä¸ªè‚¡ç¥¨çš„åˆ‡åˆ†ç‚¹
        code_changes = np.where(codes[:-1] != codes[1:])[0] + 1
        start_indices = np.concatenate(([0], code_changes))
        end_indices = np.concatenate((code_changes, [len(codes)]))

        valid_indices = []
        seq_len = Config.CONTEXT_LEN
        stride = Config.STRIDE

        # ç”Ÿæˆåˆæ³•çš„æ ·æœ¬èµ·å§‹ç´¢å¼• (ä¿è¯ seq_len é•¿åº¦ä¸”å±äº Universe)
        for start, end in zip(start_indices, end_indices):
            if end - start <= seq_len: continue
            for i in range(start + seq_len - 1, end, stride):
                if universe_mask[i]: valid_indices.append(i - seq_len + 1)

        valid_indices = np.array(valid_indices)

        # æŒ‰æ—¶é—´åˆ‡åˆ†æ•°æ®é›†
        unique_dates = np.sort(np.unique(dates))
        n_dates = len(unique_dates)

        train_end_idx = int(n_dates * Config.TRAIN_RATIO)
        val_end_idx = int(n_dates * (Config.TRAIN_RATIO + Config.VAL_RATIO))

        train_date_limit = unique_dates[train_end_idx]
        val_start_date = unique_dates[min(train_end_idx + Config.CONTEXT_LEN, n_dates - 1)]
        val_date_limit = unique_dates[val_end_idx]
        test_start_date = unique_dates[min(val_end_idx + Config.CONTEXT_LEN, n_dates - 1)]

        sample_pred_dates = dates[valid_indices + seq_len - 1]

        idx_train = valid_indices[sample_pred_dates < train_date_limit]
        idx_valid = valid_indices[(sample_pred_dates >= val_start_date) & (sample_pred_dates < val_date_limit)]
        idx_test = valid_indices[sample_pred_dates >= test_start_date]

        print(f"Dataset Size: Train={len(idx_train)}, Valid={len(idx_valid)}, Test={len(idx_test)}")

        def create_gen(indices, shuffle=False):
            def _gen():
                if shuffle: np.random.shuffle(indices)
                for start_idx in indices:
                    yield {
                        "past_values": feature_matrix[start_idx: start_idx + seq_len],
                        "labels": target_array[start_idx + seq_len - 1]
                    }

            return _gen

        return DatasetDict({
            'train': Dataset.from_generator(create_gen(idx_train, shuffle=True)),
            'validation': Dataset.from_generator(create_gen(idx_valid, shuffle=False)),
            'test': Dataset.from_generator(create_gen(idx_test, shuffle=False))
        }), len(feature_cols)


# =========================================================================
# 5. DataProvider: é—¨é¢ (Facade)
# =========================================================================
class DataProvider:
    """
    ç»Ÿä¸€å…¥å£ç±»
    """

    @staticmethod
    def _get_cache_path(mode='train', end_date=None):
        return DataProcessor.get_cache_path(mode=mode,end_date=end_date)

    @staticmethod
    def download_data():
        """æ‰§è¡Œå…¨é‡æ•°æ®ä¸‹è½½"""
        return DataDownloader.run()

    @staticmethod
    def load_and_process_panel(mode='train', end_date=None, force_refresh=False):
        """ç”Ÿæˆå› å­è¡¨"""
        return DataProcessor.process(mode=mode, end_date=end_date, force_refresh=force_refresh)

    @staticmethod
    def make_dataset(panel_df, feature_cols):
        """ç”Ÿæˆå¼ é‡æ•°æ®é›†"""
        return DatasetBuilder.build(panel_df, feature_cols)


# å…¼å®¹æ—§æ¥å£
def get_dataset(force_refresh=False):
    panel_df, feature_cols = DataProvider.load_and_process_panel(mode='train', force_refresh=force_refresh)
    ds, num_features = DataProvider.make_dataset(panel_df, feature_cols)
    return ds, num_features