import akshare as ak
import pandas as pd
import numpy as np
import os
import glob
import time
import random
import threading
import datetime
import concurrent.futures
import pickle
import warnings
import shutil
from typing import Tuple, List, Optional, Union, Dict
from datasets import Dataset, DatasetDict
from tqdm import tqdm
from pandarallel import pandarallel

# å†…éƒ¨æ¨¡å—ä¾èµ–
from .config import Config
from .vpn_rotator import vpn_rotator
from .alpha_lib import AlphaFactory

# --- å…¨å±€é…ç½® ---
# å¿½ç•¥ Pandas çš„ç¢ç‰‡åŒ–è­¦å‘Šå’Œæ€§èƒ½è­¦å‘Š
warnings.simplefilter(action='ignore', category=pd.errors.PerformanceWarning)
warnings.simplefilter(action='ignore', category=FutureWarning)

# åˆå§‹åŒ–å¹¶è¡Œè®¡ç®— (ç”¨äºå› å­è®¡ç®—)
# verbose=0 é™é»˜å¯åŠ¨
pandarallel.initialize(progress_bar=True, nb_workers=os.cpu_count(), verbose=0)


class DataProvider:
    """
    ã€SOTA Data Engine v11.0 - Industrial Gradeã€‘

    Architecture:
    1. ETL Layer: Atomic IO, Smart Caching, Deep Probing.
    2. Data Lake: Parquet with Snappy compression.
    3. Serving Layer: Zero-Copy Lazy Mapping (HuggingFace Arrow).
    """

    _vpn_lock = threading.Lock()
    _last_switch_time = 0

    # ==========================================================================
    # 1. åŸºç¡€ I/O ä¸ç½‘ç»œè®¾æ–½ (Infrastructure)
    # ==========================================================================

    @staticmethod
    def _setup_proxy_env():
        """é…ç½®ç³»ç»Ÿçº§ä»£ç†"""
        if Config.PROXY_URL:
            for k in ['http_proxy', 'https_proxy', 'all_proxy', 'HTTP_PROXY', 'HTTPS_PROXY', 'ALL_PROXY']:
                os.environ[k] = Config.PROXY_URL

    @classmethod
    def _safe_switch_vpn(cls):
        """çº¿ç¨‹å®‰å…¨çš„ VPN è½®è¯¢"""
        with cls._vpn_lock:
            if time.time() - cls._last_switch_time < 5: return
            try:
                # Debug æ¨¡å¼ä¸‹å¯å¼€å¯ print
                # print("ğŸ”„ [Network] Switching Proxy Node...")
                vpn_rotator.switch_random()
            except Exception:
                pass
            cls._last_switch_time = time.time()
            time.sleep(2)

    @staticmethod
    def _atomic_save(df: pd.DataFrame, file_path: str):
        """
        ã€åŸå­å†™å…¥ã€‘
        å…ˆå†™å…¥ .tmpï¼Œå†æ‰§è¡ŒåŸå­æ›¿æ¢ã€‚é˜²æ­¢è¿›ç¨‹å´©æºƒå¯¼è‡´ 0 å­—èŠ‚æ–‡ä»¶ã€‚
        """
        tmp_path = file_path + ".tmp"
        try:
            df.to_parquet(tmp_path, index=True)
            # POSIX åŸå­æ“ä½œ (Windows Python 3.3+ æ”¯æŒ)
            os.replace(tmp_path, file_path)
        except Exception as e:
            if os.path.exists(tmp_path): os.remove(tmp_path)
            raise e

    @staticmethod
    def _is_data_fresh(file_path: str, target_date_str: str) -> bool:
        """
        ã€æ·±åº¦å†…å®¹æ¢é’ˆã€‘Deep Content Probe
        åªè¯»å– Parquet ç´¢å¼•åˆ— (IO å¼€é”€æå°)ï¼Œæ ¡éªŒæ•°æ®æ˜¯å¦ç¡®å®åŒ…å«ç›®æ ‡æ—¥æœŸã€‚
        """
        if not os.path.exists(file_path): return False
        if os.path.getsize(file_path) < 1024: return False

        try:
            # Column Pruning: åªè¯»ä¸€åˆ—è·å– Index
            df_meta = pd.read_parquet(file_path, columns=['close'])
            if df_meta.empty: return False

            last_dt = df_meta.index.max()
            last_date = last_dt.strftime("%Y-%m-%d") if isinstance(last_dt, pd.Timestamp) else str(last_dt)[:10]

            return last_date >= target_date_str
        except Exception:
            return False

    @staticmethod
    def _get_latest_trading_date() -> str:
        """è·å–å…¨å¸‚åœºæœ€æ–°äº¤æ˜“æ—¥ (Benchmark: SH000001)"""
        try:
            df = ak.stock_zh_index_daily(symbol=Config.MARKET_INDEX_SYMBOL)
            return pd.to_datetime(df['date']).max().strftime("%Y-%m-%d")
        except:
            return datetime.date.today().strftime("%Y-%m-%d")

    # ==========================================================================
    # 2. ä¸‹è½½å·¥ä½œæµ (Workers)
    # ==========================================================================

    @staticmethod
    def _download_price_worker(code: str) -> Tuple[str, bool, str]:
        """æ—¥çº¿è¡Œæƒ…ä¸‹è½½å™¨"""
        path = os.path.join(Config.DATA_DIR, f"{code}.parquet")

        for attempt in range(3):
            try:
                time.sleep(random.uniform(0.1, 0.4))  # Jitter

                df = ak.stock_zh_a_hist(symbol=code, period="daily", start_date=Config.START_DATE, adjust="qfq")
                if df is None or df.empty: return code, True, "Empty"

                # è§„èŒƒåŒ–
                rename_map = {'æ—¥æœŸ': 'date', 'å¼€ç›˜': 'open', 'æ”¶ç›˜': 'close',
                              'æœ€é«˜': 'high', 'æœ€ä½': 'low', 'æˆäº¤é‡': 'volume', 'æˆäº¤é¢': 'amount','æ¢æ‰‹ç‡': 'turnover'}
                df.rename(columns=rename_map, inplace=True)
                df['date'] = pd.to_datetime(df['date'])
                df.set_index('date', inplace=True)

                # ç±»å‹å‹ç¼©
                cols = ['open', 'close', 'high', 'low', 'volume', 'amount','turnover']
                for c in cols:
                    if c in df.columns:
                        df[c] = pd.to_numeric(df[c], errors='coerce').astype(np.float32)

                # å•ä½æ¸…æ´— (æ‰‹ vs è‚¡)
                if 'amount' in df.columns and 'volume' in df.columns:
                    sample = df[(df['volume'] > 0) & (df['amount'] > 0)].tail(10)
                    if not sample.empty:
                        vwap = sample['amount'] / sample['volume']
                        ratio = (vwap / sample['close']).median()
                        if 80 < ratio < 120:  # æ¥è¿‘ 100
                            df['volume'] *= 100

                df = df[['open', 'high', 'low', 'close', 'volume','turnover']]
                df.sort_index(inplace=True)

                DataProvider._atomic_save(df, path)
                return code, True, "Success"

            except Exception:
                if attempt < 2: DataProvider._safe_switch_vpn()
                continue

        return code, False, "Failed"

    @staticmethod
    def _download_finance_worker(code: str) -> Tuple[str, bool, str]:
        """è´¢åŠ¡æ•°æ®ä¸‹è½½å™¨ (æ™ºèƒ½ç¼“å­˜)"""
        fund_dir = os.path.join(Config.DATA_DIR, "fundamental")
        os.makedirs(fund_dir, exist_ok=True)
        path = os.path.join(fund_dir, f"{code}.parquet")

        # --- Smart Seasonality Logic ---
        if os.path.exists(path):
            mtime = os.path.getmtime(path)
            curr_month = datetime.date.today().month
            # è´¢æŠ¥æœˆ(4,8,10)ç¼“å­˜12å°æ—¶ï¼Œå¹³æ—¶ç¼“å­˜3å¤©
            ttl_seconds = 12 * 3600 if curr_month in [4, 8, 10] else 72 * 3600

            if (time.time() - mtime) < ttl_seconds:
                return code, True, "Skipped (Cache)"

        for attempt in range(2):
            try:
                time.sleep(random.uniform(0.1, 0.5))
                df = ak.stock_financial_analysis_indicator_em(symbol=code)
                if df is None or df.empty: return code, True, "Empty"

                df['date'] = pd.to_datetime(df['æ—¥æœŸ'])

                col_map = {
                    'åŠ æƒå‡€èµ„äº§æ”¶ç›Šç‡': 'roe', 'ä¸»è¥ä¸šåŠ¡æ”¶å…¥å¢é•¿ç‡(%)': 'rev_growth',
                    'å‡€åˆ©æ¶¦å¢é•¿ç‡(%)': 'profit_growth', 'èµ„äº§è´Ÿå€ºç‡(%)': 'debt_ratio',
                    'å¸‚ç›ˆç‡(åŠ¨æ€)': 'pe_ttm', 'å¸‚å‡€ç‡': 'pb'
                }
                valid_cols = [c for c in col_map.keys() if c in df.columns]
                df = df[['date'] + valid_cols].copy()
                df.rename(columns=col_map, inplace=True)

                for c in df.columns:
                    if c != 'date':
                        df[c] = pd.to_numeric(df[c], errors='coerce').astype(np.float32)

                df.set_index('date', inplace=True)
                df.sort_index(inplace=True)

                DataProvider._atomic_save(df, path)
                return code, True, "Success"
            except:
                DataProvider._safe_switch_vpn()
        return code, False, "Failed"

    # ==========================================================================
    # 3. ETL ä¸»æµç¨‹ (Pipeline)
    # ==========================================================================

    @staticmethod
    def download_data():
        """ETL Entry Point"""
        print(f"\n{'=' * 60}\n>>> [ETL] Data Pipeline Initiated\n{'=' * 60}")
        DataProvider._setup_proxy_env()
        os.makedirs(Config.DATA_DIR, exist_ok=True)

        # 1. Sync List
        try:
            print("â˜ï¸ Syncing Universe List...")
            stock_info = ak.stock_zh_a_spot_em()
            codes = stock_info['ä»£ç '].tolist()
        except Exception as e:
            print(f"âŒ Critical Error: Failed to fetch stock list. {e}")
            return

        target_date = DataProvider._get_latest_trading_date()
        print(f"ğŸ“… Target Trading Date: {target_date}")

        # 2. Parallel Probe (Deep Integrity Scan)
        print("ğŸ” Probing Local Data Integrity...")

        def _check_task(c):
            fpath = os.path.join(Config.DATA_DIR, f"{c}.parquet")
            if not DataProvider._is_data_fresh(fpath, target_date):
                return c
            return None

        scan_workers = min(os.cpu_count() * 4, 64)
        with concurrent.futures.ThreadPoolExecutor(max_workers=scan_workers) as executor:
            results = list(tqdm(executor.map(_check_task, codes), total=len(codes), desc="Scanning"))

        todo_codes = [r for r in results if r is not None]
        print(f"ğŸ“Š Status: Total={len(codes)} | Fresh={len(codes) - len(todo_codes)} | Stale={len(todo_codes)}")

        # 3. Download Execution
        if todo_codes:
            max_workers = 8 if len(todo_codes) < 500 else 16
            print(f"ğŸš€ Launching Download Engine (Workers={max_workers})...")

            with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
                futures = {executor.submit(DataProvider._download_price_worker, c): c for c in todo_codes}
                success_count = 0
                for future in tqdm(concurrent.futures.as_completed(futures), total=len(todo_codes),
                                   desc="Downloading Price"):
                    try:
                        _, status, _ = future.result()
                        if status: success_count += 1
                    except:
                        pass
            print(f"âœ… Price Sync Complete. Success: {success_count}/{len(todo_codes)}")
        else:
            print("âœ… Market Data is Up-to-Date.")

        # 4. Finance Sync
        print("ğŸ“‹ Syncing Fundamental Data...")
        with concurrent.futures.ThreadPoolExecutor(max_workers=8) as executor:
            futures = {executor.submit(DataProvider._download_finance_worker, c): c for c in codes}
            for _ in tqdm(concurrent.futures.as_completed(futures), total=len(codes), desc="Downloading Finance"):
                pass

        print("âœ… ETL Pipeline Completed.")

    # ==========================================================================
    # 4. æ•°æ®åŠ è½½ä¸é¢„å¤„ç† (Processing Layer)
    # ==========================================================================

    @staticmethod
    def _get_cache_path(mode):
        today = datetime.date.today().strftime("%Y%m%d")
        return os.path.join(Config.OUTPUT_DIR, f"panel_cache_{mode}_{today}.pkl")

    @staticmethod
    def _tag_universe(panel_df):
        """Universe Selection Logic"""
        print(">>> [Tagging] æ ‡è®°åŠ¨æ€è‚¡ç¥¨æ±  (Universe Mask)...")
        panel_df = panel_df.sort_values(['code', 'date'])
        panel_df['list_days_count'] = panel_df.groupby('code')['date'].cumcount() + 1

        cond_vol = panel_df['volume'] > 0
        cond_price = panel_df['close'] >= 2.0
        cond_list = panel_df['list_days_count'] > 60

        panel_df['is_universe'] = cond_vol & cond_price & cond_list
        panel_df.drop(columns=['list_days_count'], inplace=True)
        return panel_df

    @staticmethod
    def load_and_process_panel(mode='train', force_refresh=False):
        """
        Build Panel Data: Merge -> Alpha -> Label -> Norm
        """
        cache_path = DataProvider._get_cache_path(mode)
        if not force_refresh and os.path.exists(cache_path):
            print(f"âš¡ï¸ [Cache Hit] Loading from {cache_path}")
            try:
                with open(cache_path, 'rb') as f:
                    return pickle.load(f)
            except:
                pass

        print(f"\n>>> [Processing] Building Panel Data (Mode: {mode})...")

        # 1. Load Price
        price_files = glob.glob(os.path.join(Config.DATA_DIR, "*.parquet"))
        if not price_files:
            raise RuntimeError("âŒ No data found! Run `python main.py --mode download` first.")

        def _read_pq(f):
            try:
                df = pd.read_parquet(f)
                if df.empty: return None
                df['code'] = os.path.basename(f).replace(".parquet", "")
                return df
            except:
                return None

        with concurrent.futures.ThreadPoolExecutor(max_workers=min(16, os.cpu_count() + 4)) as executor:
            dfs = list(tqdm(executor.map(_read_pq, price_files), total=len(price_files), desc="Reading Price"))

        valid_dfs = [d for d in dfs if d is not None and len(d) > Config.CONTEXT_LEN]
        if not valid_dfs: raise ValueError("Not enough valid data.")

        panel_df = pd.concat(valid_dfs, ignore_index=True)
        if 'date' not in panel_df.columns: panel_df = panel_df.reset_index().rename(columns={'index': 'date'})
        panel_df['date'] = pd.to_datetime(panel_df['date'])
        panel_df['code'] = panel_df['code'].astype(str)

        # Optimization: Downcast
        f_cols = panel_df.select_dtypes(include=['float64']).columns
        panel_df[f_cols] = panel_df[f_cols].astype(np.float32)

        # 2. Merge Fundamental (PIT)
        fund_dir = os.path.join(Config.DATA_DIR, "fundamental")
        fund_files = glob.glob(os.path.join(fund_dir, "*.parquet"))

        # --- Explicit Warning ---
        if not fund_files:
            print("\033[93m" + "=" * 60)
            print("âš ï¸  [WARNING] MISSING FUNDAMENTAL DATA!")
            print("   The model will lose all valuation (PE/PB) factors.")
            print("   Please run `python main.py --mode download`.")
            print("=" * 60 + "\033[0m")
        else:
            print(f"ğŸ”— Merging Fundamental Data (PIT Mode)... Coverage: {len(fund_files)}")
            with concurrent.futures.ThreadPoolExecutor(max_workers=8) as ex:
                funds = [f for f in ex.map(_read_pq, fund_files) if f is not None]

            if funds:
                fund_df = pd.concat(funds).sort_values(['code', 'date'])

                # Default visibility lag: 90 days
                fund_df['merge_date'] = fund_df['date'] + pd.Timedelta(days=90)
                if 'pub_date' in fund_df.columns:
                    fund_df['merge_date'] = fund_df['pub_date'].fillna(fund_df['merge_date'])

                fund_df = fund_df.drop(columns=['date', 'pub_date'], errors='ignore').rename(
                    columns={'merge_date': 'date'})

                panel_df = panel_df.sort_values(['code', 'date'])
                fund_df = fund_df.sort_values(['code', 'date'])

                panel_df = pd.merge_asof(panel_df, fund_df, on='date', by='code', direction='backward')

                # Keep NaN for fundamental factors here
                fin_cols = ['roe', 'rev_growth', 'profit_growth', 'debt_ratio', 'pe_ttm', 'pb']
                for c in fin_cols:
                    if c in panel_df.columns:
                        panel_df[c] = panel_df[c].astype(np.float32)

        # 3. Alpha Gen (Parallel)
        print("âš™ï¸  Running AlphaFactory (Parallel)...")
        panel_df.set_index('date', inplace=True)
        panel_df = panel_df.groupby('code', group_keys=False).parallel_apply(lambda x: AlphaFactory(x).make_factors())
        panel_df.reset_index(inplace=True)

        # 4. Labeling
        print("ğŸ·ï¸  Generating Labels...")
        panel_df.sort_values(['code', 'date'], inplace=True)
        panel_df['next_open'] = panel_df.groupby('code')['open'].shift(-1)
        panel_df['future_close'] = panel_df.groupby('code')['close'].shift(-Config.PRED_LEN)
        panel_df['target'] = panel_df['future_close'] / panel_df['next_open'] - 1

        if mode == 'train':
            panel_df.dropna(subset=['target'], inplace=True)

        # Tag Universe
        panel_df = DataProvider._tag_universe(panel_df)

        # 5. CS Norm
        print("ğŸŒ Cross-Sectional Normalization...")
        panel_df.set_index('date', inplace=True)
        panel_df = AlphaFactory.add_cross_sectional_factors(panel_df)
        panel_df.reset_index(inplace=True)

        # 6. Final Clean
        feat_cols = [c for c in panel_df.columns if any(c.startswith(p) for p in Config.FEATURE_PREFIXES)]

        # Fill NaN with 0 for Technical Factors.
        panel_df[feat_cols] = panel_df[feat_cols].fillna(0).replace([np.inf, -np.inf], 0).astype(np.float32)

        with open(cache_path, 'wb') as f:
            pickle.dump((panel_df, feat_cols), f)
        print(f"âœ… Panel Ready. Shape: {panel_df.shape}")
        return panel_df, feat_cols

    # ==========================================================================
    # 5. é«˜æ€§èƒ½æ•°æ®é›†æ„å»º (Lazy Mapping Layer)
    # ==========================================================================

    @staticmethod
    def make_dataset(panel_df, feature_cols):
        """
        ã€Zero-Copy Lazy Datasetã€‘
        ä¸å†ç”Ÿæˆ Sample å¯¹è±¡ï¼Œè€Œæ˜¯å­˜å‚¨ç´¢å¼•ï¼Œä½¿ç”¨ set_transform åŠ¨æ€åˆ‡ç‰‡ã€‚
        æåº¦èŠ‚çœå†…å­˜ï¼Œä¸”åˆå§‹åŒ–æå¿«ã€‚
        """
        print(">>> [Dataset] Constructing Lazy Mapping (Zero-Copy Mode)...")

        # 1. å†…å­˜é”å®š (Memory Locking)
        panel_df = panel_df.sort_values(['code', 'date']).reset_index(drop=True)

        # å…³é”®ï¼šè½¬ä¸º C-contiguous å†…å­˜å—ï¼Œè¿™æ˜¯é«˜æ•ˆåˆ‡ç‰‡çš„å‰æ
        print("    > Locking features into contiguous memory block...")
        feature_matrix = np.ascontiguousarray(
            panel_df[feature_cols].values.astype(np.float32)
        )

        if 'rank_label' in panel_df.columns:
            target_array = panel_df['rank_label'].fillna(0.5).values.astype(np.float32)
        else:
            target_array = panel_df['target'].fillna(0).values.astype(np.float32)

        # 2. ç´¢å¼•è®¡ç®— (Valid Index Calculation)
        universe_mask = panel_df['is_universe'].values
        dates = panel_df['date'].values
        codes = panel_df['code'].values

        # å¿«é€Ÿå‘é‡åŒ–å¯»æ‰¾åˆ‡æ¢ç‚¹
        code_changes = np.where(codes[:-1] != codes[1:])[0] + 1
        start_indices = np.concatenate(([0], code_changes))
        end_indices = np.concatenate((code_changes, [len(codes)]))

        valid_start_indices = []
        seq_len = Config.CONTEXT_LEN
        stride = Config.STRIDE

        # è®¡ç®—æ‰€æœ‰åˆæ³•çš„ Window Start Index
        for start, end in zip(start_indices, end_indices):
            length = end - start
            if length <= seq_len: continue

            # å€™é€‰èµ·ç‚¹
            curr_starts = np.arange(start, end - seq_len + 1, stride)
            # å¯¹åº”çš„é¢„æµ‹ç‚¹ (åˆ‡ç‰‡æœ«å°¾)
            pred_indices = curr_starts + seq_len - 1

            # Universe è¿‡æ»¤
            mask = universe_mask[pred_indices]
            valid_start_indices.extend(curr_starts[mask])

        valid_start_indices = np.array(valid_start_indices, dtype=np.int64)

        # 3. ä¸¥æ ¼æ—¶é—´åˆ‡åˆ† (Strict Time Split)
        unique_dates = np.sort(np.unique(dates))
        n_dates = len(unique_dates)

        train_end_idx = int(n_dates * Config.TRAIN_RATIO)
        val_end_idx = int(n_dates * (Config.TRAIN_RATIO + Config.VAL_RATIO))

        train_date_limit = unique_dates[train_end_idx]
        val_start_date = unique_dates[min(train_end_idx + Config.CONTEXT_LEN, n_dates - 1)]
        val_date_limit = unique_dates[val_end_idx]
        test_start_date = unique_dates[min(val_end_idx + Config.CONTEXT_LEN, n_dates - 1)]

        print(f"\nğŸ“Š Dataset Split (Gap={Config.CONTEXT_LEN} days):")
        print(f"   Train : ~ {train_date_limit}")
        print(f"   Valid : {val_start_date} ~ {val_date_limit}")
        print(f"   Test  : {test_start_date} ~")

        # æ˜ å°„å›æ—¥æœŸè¿›è¡Œç­›é€‰
        sample_pred_dates = dates[valid_start_indices + seq_len - 1]

        idx_train = valid_start_indices[sample_pred_dates < train_date_limit]

        valid_mask = (sample_pred_dates >= val_start_date) & (sample_pred_dates < val_date_limit)
        idx_valid = valid_start_indices[valid_mask]

        idx_test = valid_start_indices[sample_pred_dates >= test_start_date]

        print(f"   Samples: Train={len(idx_train)}, Valid={len(idx_valid)}, Test={len(idx_test)}")

        # 4. é—­åŒ… Transform å‡½æ•° (Lazy Loader)
        # æ­¤å‡½æ•°åœ¨ DataLoader Worker ä¸­è¢«è°ƒç”¨
        def lazy_transform(batch):
            """
            batch: {'start_idx': [id1, id2, ...]}
            """
            start_idxs = batch['start_idx']

            past_values = []
            labels = []

            for start in start_idxs:
                end = start + seq_len
                # è¿™é‡Œåªäº§ç”Ÿ View æˆ–æå°çš„ Copyï¼Œåˆ©ç”¨ Shared Memory
                past_values.append(feature_matrix[start:end])
                labels.append(target_array[end - 1])

            return {
                "past_values": past_values,
                "labels": labels
            }

        # 5. æ„å»º Light-weight Dataset
        ds = DatasetDict({
            'train': Dataset.from_dict({'start_idx': idx_train}),
            'validation': Dataset.from_dict({'start_idx': idx_valid}),
            'test': Dataset.from_dict({'start_idx': idx_test})
        })

        # æ³¨å†Œ On-the-fly Transform
        ds.set_transform(lazy_transform)

        return ds, len(feature_cols)


def get_dataset(force_refresh=False):
    """External API"""
    panel_df, feature_cols = DataProvider.load_and_process_panel(mode='train', force_refresh=force_refresh)
    return DataProvider.make_dataset(panel_df, feature_cols)