import akshare as ak
import pandas as pd
import os
import glob
import numpy as np
import time
import random
import requests
import threading
import datetime
import concurrent.futures
import pickle
from datasets import Dataset
from sklearn.preprocessing import StandardScaler
from tqdm import tqdm
from .config import Config
from .vpn_rotator import vpn_rotator
from .alpha_lib import AlphaFactory


class DataProvider:
    _vpn_lock = threading.Lock()
    _last_switch_time = 0

    # --------------------------------------------------------------------------
    # PART 1: ä¸‹è½½æ¨¡å— (å¤šçº¿ç¨‹ + VPN è½®è¯¢ + æ™ºèƒ½æ—¥å†)
    # --------------------------------------------------------------------------

    @staticmethod
    def _setup_proxy_env():
        """è®¾ç½®å½“å‰è¿›ç¨‹çš„ä»£ç†ç¯å¢ƒå˜é‡ (å¯¹åº” Clash æ··åˆç«¯å£ 7890)"""
        proxy_url = "http://127.0.0.1:7890"
        os.environ['http_proxy'] = proxy_url
        os.environ['https_proxy'] = proxy_url
        os.environ['all_proxy'] = proxy_url
        os.environ['HTTP_PROXY'] = proxy_url
        os.environ['HTTPS_PROXY'] = proxy_url
        os.environ['ALL_PROXY'] = proxy_url

    @classmethod
    def _safe_switch_vpn(cls):
        """çº¿ç¨‹å®‰å…¨çš„ VPN åˆ‡æ¢é€»è¾‘"""
        with cls._vpn_lock:
            # é˜²æ­¢å¤šä¸ªçº¿ç¨‹åŒæ—¶è§¦å‘åˆ‡æ¢ï¼Œè®¾ç½® 5 ç§’å†·å´
            if time.time() - cls._last_switch_time < 5:
                return
            vpn_rotator.switch_random()
            cls._last_switch_time = time.time()
            time.sleep(2)  # ç»™ Clash å»ºç«‹è¿æ¥ç•™å‡ºæ—¶é—´

    @staticmethod
    def _get_latest_trading_date():
        """
        ã€æ–°å¢ã€‘è·å–æœ€è¿‘çš„ä¸€ä¸ªäº¤æ˜“æ—¥
        ä¼˜åŒ–ï¼šé˜²æ­¢å‘¨æœ«/èŠ‚å‡æ—¥è¿è¡Œè„šæœ¬æ—¶é‡å¤ä¸‹è½½å‘¨äº”çš„æ•°æ®
        """
        try:
            # è·å–ä¸Šè¯æŒ‡æ•°çš„æœ€æ–°æ—¥çº¿æ•°æ®ä½œä¸ºå‚è€ƒ
            # symbol="sh000001" æ˜¯ä¸Šè¯æŒ‡æ•°
            df = ak.stock_zh_index_daily(symbol="sh000001")
            if df is not None and not df.empty:
                latest_date = pd.to_datetime(df['date']).max().date()
                return latest_date.strftime("%Y-%m-%d")
        except:
            pass

        # å¦‚æœè·å–å¤±è´¥ï¼ˆæ¯”å¦‚æ–­ç½‘ï¼‰ï¼Œé€€åŒ–ä¸ºä½¿ç”¨ä»Šå¤©
        return datetime.date.today().strftime("%Y-%m-%d")

    @staticmethod
    def _download_worker(code):
        """å•ä¸ªè‚¡ç¥¨ä¸‹è½½ä»»åŠ¡"""
        path = os.path.join(Config.DATA_DIR, f"{code}.parquet")
        for attempt in range(5):
            try:
                # æé€Ÿæ¨¡å¼ï¼šä¿ç•™å¾®å°éšæœºå»¶è¿Ÿæ¨¡æ‹ŸçœŸäºº
                time.sleep(random.uniform(0.05, 0.2))
                df = ak.stock_zh_a_hist(symbol=code, period="daily", start_date=Config.START_DATE, adjust="qfq")

                if df is None or df.empty: return code, True, "Empty"

                # æ ‡å‡†åŒ–åˆ—å
                df.rename(columns={'æ—¥æœŸ': 'date', 'å¼€ç›˜': 'open', 'æ”¶ç›˜': 'close',
                                   'æœ€é«˜': 'high', 'æœ€ä½': 'low', 'æˆäº¤é‡': 'volume'}, inplace=True)
                df['date'] = pd.to_datetime(df['date'])
                df.set_index('date', inplace=True)

                # å†…å­˜ä¼˜åŒ–ï¼šå­˜ç›˜å‰è½¬ float32
                for col in ['open', 'close', 'high', 'low', 'volume']:
                    if col in df.columns: df[col] = df[col].astype(np.float32)

                if len(df) > 0: df.to_parquet(path)
                return code, True, "Success"
            except:
                # é‡åˆ°å°é”ï¼Œç”³è¯·åˆ‡æ¢ VPN
                DataProvider._safe_switch_vpn()
                continue
        return code, False, "Failed"

    @staticmethod
    def download_data():
        """ä¸‹è½½å…¨å¸‚åœºæ•°æ®ä¸»å…¥å£"""
        print(">>> [Phase 1] å¯åŠ¨æ•°æ®ä¸‹è½½ (æ™ºèƒ½å¢é‡æ¨¡å¼)...")
        DataProvider._setup_proxy_env()

        if not os.path.exists(Config.DATA_DIR): os.makedirs(Config.DATA_DIR)

        try:
            stock_info = ak.stock_zh_a_spot_em()
            codes = stock_info['ä»£ç '].tolist()
        except:
            print("âŒ æ— æ³•è·å–è‚¡ç¥¨åˆ—è¡¨ï¼Œè¯·æ£€æŸ¥ç½‘ç»œ/VPN")
            return

        # 1. è·å–ã€çœŸæ­£ã€‘éœ€è¦æ›´æ–°åˆ°çš„æ—¥æœŸ
        print(">>> æ­£åœ¨æ ¡å¯¹äº¤æ˜“æ—¥å†...")
        target_date_str = DataProvider._get_latest_trading_date()
        print(f"ğŸ“… å¸‚åœºæœ€æ–°äº¤æ˜“æ—¥: {target_date_str}")

        # 2. æ™ºèƒ½æ–­ç‚¹ç»­ä¼ 
        # é€»è¾‘ï¼šå¦‚æœæœ¬åœ°æ–‡ä»¶çš„ä¿®æ”¹æ—¥æœŸ >= å¸‚åœºæœ€æ–°äº¤æ˜“æ—¥ï¼Œè¯´æ˜å·²ç»åŒ…å«äº†æœ€æ–°æ•°æ®ï¼Œè·³è¿‡
        existing_fresh = set()
        files = os.listdir(Config.DATA_DIR)

        for fname in files:
            if fname.endswith(".parquet"):
                fpath = os.path.join(Config.DATA_DIR, fname)
                if os.path.getsize(fpath) > 1024:
                    # è·å–æ–‡ä»¶ä¿®æ”¹æ—¶é—´
                    mtime = os.path.getmtime(fpath)
                    file_date = datetime.date.fromtimestamp(mtime).strftime("%Y-%m-%d")

                    # ã€æ ¸å¿ƒä¼˜åŒ–ã€‘åªè¦æ–‡ä»¶æ—¥æœŸ >= ç›®æ ‡äº¤æ˜“æ—¥ï¼Œå°±ç®—æ–°é²œ
                    # ä¾‹å¦‚ï¼šç›®æ ‡æ—¥æ˜¯å‘¨äº”ï¼Œä½ åœ¨å‘¨å…­è¿è¡Œï¼Œæ–‡ä»¶æ—¥æœŸæ˜¯å‘¨äº”ï¼Œæ»¡è¶³ >=ï¼Œè·³è¿‡ä¸‹è½½
                    if file_date >= target_date_str:
                        existing_fresh.add(fname.replace(".parquet", ""))

        todo = list(set(codes) - existing_fresh)
        todo.sort()

        print(f"ğŸ“Š ä»»åŠ¡: æ€»æ•° {len(codes)} | å·²æ˜¯æœ€æ–° {len(existing_fresh)} | å¾…æ›´æ–° {len(todo)}")
        if not todo:
            print("âœ… æ‰€æœ‰æ•°æ®å·²åŒæ­¥è‡³æœ€æ–°äº¤æ˜“æ—¥ï¼Œæ— éœ€ä¸‹è½½ã€‚")
            return

        # å¼€å¯ 16 çº¿ç¨‹å¹¶å‘ä¸‹è½½
        with concurrent.futures.ThreadPoolExecutor(max_workers=16) as executor:
            futures = {executor.submit(DataProvider._download_worker, c): c for c in todo}
            for _ in tqdm(concurrent.futures.as_completed(futures), total=len(todo)):
                pass
        print("ä¸‹è½½å®Œæˆã€‚")

    # --------------------------------------------------------------------------
    # PART 2: æ ¸å¿ƒé‡æ„ - å†…å­˜ Panel å¤„ç† (å« Phase 2 è¿‡æ»¤ä¸é€»è¾‘ä¿®å¤)
    # --------------------------------------------------------------------------

    @staticmethod
    def _get_cache_path(mode):
        today_str = datetime.date.today().strftime("%Y%m%d")
        return os.path.join(Config.OUTPUT_DIR, f"panel_cache_{mode}_{today_str}.pkl")

    @staticmethod
    def _filter_universe(panel_df):
        """
        ã€åŠ¨æ€è‚¡ç¥¨æ± è¿‡æ»¤ã€‘
        æ¸…æ´—æ‰ä¸é€‚åˆäº¤æ˜“çš„è„æ•°æ®ï¼ˆåœç‰Œã€é€€å¸‚ã€æ¬¡æ–°è‚¡ï¼‰ï¼Œé˜²æ­¢æ¨¡å‹å­¦åã€‚
        """
        print(">>> [Filtering] æ­£åœ¨æ‰§è¡ŒåŠ¨æ€è‚¡ç¥¨æ± è¿‡æ»¤...")
        original_len = len(panel_df)

        # 1. å‰”é™¤åœç‰Œ (Volume = 0)
        panel_df = panel_df[panel_df['volume'] > 0]

        # 2. å‰”é™¤åƒåœ¾è‚¡/å‡†é€€å¸‚è‚¡ (Close < 2.0)
        panel_df = panel_df[panel_df['close'] >= 2.0]

        # 3. å‰”é™¤ä¸Šå¸‚ä¸æ»¡ 60 å¤©çš„æ¬¡æ–°è‚¡
        panel_df['list_days'] = panel_df.groupby('code').cumcount()
        panel_df = panel_df[panel_df['list_days'] > 60]
        panel_df = panel_df.drop(columns=['list_days'])

        new_len = len(panel_df)
        print(f"è¿‡æ»¤å®Œæˆã€‚ç§»é™¤æ ·æœ¬: {original_len - new_len} ({1 - new_len / original_len:.2%})")
        return panel_df

    @staticmethod
    def load_and_process_panel(mode='train', force_refresh=False):
        """
        å…¨å†…å­˜åŠ è½½ä¸å¤„ç†æ ¸å¿ƒå‡½æ•°
        :param mode: 'train' (å‰”é™¤æ— æ ‡ç­¾æ•°æ®) | 'predict' (ä¿ç•™æœ€æ–°æ•°æ®ç”¨äºæ¨ç†)
        :param force_refresh: å¼ºåˆ¶ä¸ä½¿ç”¨ç¼“å­˜
        """
        cache_path = DataProvider._get_cache_path(mode)

        # å°è¯•è¯»å–ç¼“å­˜
        if not force_refresh and os.path.exists(cache_path):
            print(f"âš¡ï¸ [Cache Hit] å‘ç°ä»Šæ—¥ç¼“å­˜ï¼Œæ­£åœ¨æé€ŸåŠ è½½: {cache_path}")
            try:
                with open(cache_path, 'rb') as f:
                    panel_df, feature_cols = pickle.load(f)
                print(f"âœ… ç¼“å­˜åŠ è½½æˆåŠŸï¼Œç‰¹å¾æ•°: {len(feature_cols)}")
                return panel_df, feature_cols
            except Exception as e:
                print(f"âš ï¸ ç¼“å­˜è¯»å–å¤±è´¥ ({e})ï¼Œå°†é‡æ–°è®¡ç®—...")

        print(f"\n>>> [Phase 2] å¼€å§‹æ„å»ºå…¨å†…å­˜ Panel æ•°æ® (Mode: {mode})...")

        files = glob.glob(os.path.join(Config.DATA_DIR, "*.parquet"))
        if not files:
            raise ValueError("æ²¡æœ‰æ‰¾åˆ°æ•°æ®æ–‡ä»¶ï¼Œè¯·å…ˆè¿è¡Œ download")

        print(f"æ­£åœ¨åŠ è½½ {len(files)} ä¸ªæ–‡ä»¶åˆ°å†…å­˜...")

        def _read_helper(f):
            try:
                df = pd.read_parquet(f)
                code = os.path.basename(f).replace(".parquet", "")

                # å†…å­˜ä¼˜åŒ–ï¼šå¼ºè½¬ float32
                float_cols = df.select_dtypes(include=['float64']).columns
                df[float_cols] = df[float_cols].astype(np.float32)

                df['code'] = code
                # df['code'] = df['code'].astype('category') # æš‚æ—¶ç¦ç”¨ category ä»¥é˜² groupby å…¼å®¹æ€§é—®é¢˜
                return df
            except:
                return None

        with concurrent.futures.ThreadPoolExecutor(max_workers=8) as executor:
            results = list(tqdm(executor.map(_read_helper, files), total=len(files), desc="Reading"))

        data_frames = [df for df in results if df is not None and len(df) > Config.CONTEXT_LEN + 10]
        if not data_frames: raise ValueError("æœ‰æ•ˆæ•°æ®ä¸ºç©º")

        print("æ­£åœ¨åˆå¹¶ Panel DataFrame...")
        panel_df = pd.concat(data_frames)
        del data_frames

        # é‡ç½®ç´¢å¼•
        panel_df = panel_df.reset_index().sort_values(['code', 'date'])

        # --- Step 2: è®¡ç®—æ—¶åºå› å­ (TS Factors) ---
        print("æ­£åœ¨è®¡ç®—æ—¶åºå› å­ (TS Factors)...")

        def _process_ts(df_sub):
            factory = AlphaFactory(df_sub)
            return factory.make_factors()

        panel_df = panel_df.groupby('code', group_keys=False).apply(_process_ts)

        # --- Step 3: æ„é€  Label ---
        print("æ­£åœ¨æ„é€ é¢„æµ‹ç›®æ ‡ (Future Returns)...")
        panel_df['target'] = panel_df.groupby('code')['close'].shift(-Config.PRED_LEN) / panel_df['close'] - 1

        # ã€æ ¸å¿ƒä¿®å¤é€»è¾‘ï¼šé˜²æ­¢é¢„æµ‹æ—¥è‡ªæ€ã€‘
        if mode == 'train':
            print("è®­ç»ƒæ¨¡å¼ï¼šå‰”é™¤æ— æ ‡ç­¾çš„å°¾éƒ¨æ•°æ®...")
            panel_df.dropna(subset=['target'], inplace=True)
        else:
            print("é¢„æµ‹æ¨¡å¼ï¼šä¿ç•™å°¾éƒ¨æ•°æ®ç”¨äºæ¨ç† (Targetä¸ºNaNæ˜¯æ­£å¸¸çš„)...")
            # ä¸æ‰§è¡Œ dropnaï¼Œä¿ç•™æœ€æ–°çš„æ•°æ®è¡Œ

        # --- Step 4: æ‰§è¡ŒåŠ¨æ€è¿‡æ»¤ ---
        panel_df = DataProvider._filter_universe(panel_df)

        # --- Step 5: è®¡ç®—æˆªé¢å› å­ (Cross-Sectional) ---
        # å¿…é¡»åœ¨è¿‡æ»¤ä¹‹ååšï¼Œç¡®ä¿æ’åæ˜¯åœ¨å¯äº¤æ˜“è‚¡ç¥¨æ± ä¸­è¿›è¡Œçš„
        panel_df = panel_df.set_index('date')
        panel_df = AlphaFactory.add_cross_sectional_factors(panel_df)

        # --- Step 6: æœ€ç»ˆæ¸…æ´— ---
        feature_cols = [c for c in panel_df.columns
                        if any(
                c.startswith(p) for p in ['style_', 'tech_', 'alpha_', 'adv_', 'ind_', 'cs_rank_', 'mkt_', 'rel_'])]

        print(f"å› å­å·¥ç¨‹å®Œæˆã€‚ç‰¹å¾ç»´åº¦: {len(feature_cols)}")
        panel_df[feature_cols] = panel_df[feature_cols].fillna(0).astype(np.float32)

        panel_df = panel_df.reset_index()

        # ä¿å­˜ç¼“å­˜
        print(f"ğŸ’¾ æ­£åœ¨ä¿å­˜è®¡ç®—ç»“æœåˆ°ç¼“å­˜: {cache_path} ...")
        try:
            with open(cache_path, 'wb') as f:
                pickle.dump((panel_df, feature_cols), f)
            print("âœ… ç¼“å­˜ä¿å­˜å®Œæ¯•ã€‚")
        except Exception as e:
            print(f"âš ï¸ ç¼“å­˜ä¿å­˜å¤±è´¥: {e}")

        return panel_df, feature_cols

    @staticmethod
    def make_dataset(panel_df, feature_cols):
        """è½¬æ¢ Dataset (ä»…ç”¨äºè®­ç»ƒ)"""
        print(">>> [Phase 3] è½¬æ¢ Dataset...")
        panel_df = panel_df.sort_values(['code', 'date'])

        feature_matrix = panel_df[feature_cols].values.astype(np.float32)

        # ä¼˜å…ˆä½¿ç”¨è¶…é¢æ”¶ç›Šä½œä¸ºç›®æ ‡
        # ä¼˜å…ˆ rank_label -> excess_label -> target
        if 'rank_label' in panel_df.columns:
            target_col = 'rank_label'
        elif 'excess_label' in panel_df.columns:
            target_col = 'excess_label'
        else:
            target_col = 'target'

        # å¡«å…… NaNï¼Œé˜²æ­¢é¢„æµ‹æ¨¡å¼æŠ¥é”™
        target_array = panel_df[target_col].fillna(0.5).values.astype(np.float32)

        codes = panel_df['code'].values
        code_changes = np.where(codes[:-1] != codes[1:])[0] + 1
        start_indices = np.concatenate(([0], code_changes))
        end_indices = np.concatenate((code_changes, [len(codes)]))

        valid_indices = []
        seq_len = Config.CONTEXT_LEN
        stride = 5

        for start, end in zip(start_indices, end_indices):
            length = end - start
            if length <= seq_len: continue
            for i in range(start, end - seq_len + 1, stride):
                valid_indices.append(i)

        print(f"ç”Ÿæˆçš„æ ·æœ¬æ•°é‡: {len(valid_indices)}")

        # æ—¶é—´åºåˆ—åˆ‡åˆ† (Time-Series Split)
        dates = panel_df['date'].unique()
        dates.sort()
        split_idx = int(len(dates) * 0.9)
        split_date = dates[split_idx]
        print(f"åˆ‡åˆ†æ—¥æœŸ: {split_date}")

        sample_dates = panel_df['date'].values[np.array(valid_indices) + seq_len - 1]
        train_mask = sample_dates < split_date
        train_indices = np.array(valid_indices)[train_mask]
        valid_indices = np.array(valid_indices)[~train_mask]

        print(f"Train: {len(train_indices)} | Valid: {len(valid_indices)}")

        def gen_train():
            np.random.shuffle(train_indices)
            for idx in train_indices:
                yield {
                    "past_values": feature_matrix[idx: idx + seq_len],
                    "labels": target_array[idx + seq_len - 1]
                }

        def gen_valid():
            for idx in valid_indices:
                yield {
                    "past_values": feature_matrix[idx: idx + seq_len],
                    "labels": target_array[idx + seq_len - 1]
                }

        from datasets import DatasetDict
        ds = DatasetDict({
            'train': Dataset.from_generator(gen_train),
            'test': Dataset.from_generator(gen_valid)
        })

        return ds, len(feature_cols)


def get_dataset(force_refresh=False):
    # é»˜è®¤æ˜¯è®­ç»ƒæ¨¡å¼
    panel_df, feature_cols = DataProvider.load_and_process_panel(mode='train', force_refresh=force_refresh)
    ds, num_features = DataProvider.make_dataset(panel_df, feature_cols)
    return ds, num_features