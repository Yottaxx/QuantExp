import akshare as ak
import pandas as pd
import os
import glob
import numpy as np
import time
import random
import requests
import threading
import concurrent.futures
from datasets import Dataset
from sklearn.preprocessing import StandardScaler
from tqdm import tqdm
from .config import Config
# æ³¨æ„ï¼šè¿™é‡Œä¸å†åœ¨é¡¶å±‚å¯¼å…¥ AlphaFactoryï¼Œè¯æ˜ä¸‹è½½é˜¶æ®µå®Œå…¨ä¸ä¾èµ–å®ƒ
# from .alpha_lib import AlphaFactory
from .vpn_rotator import vpn_rotator


class DataProvider:
    # çº¿ç¨‹é”
    _vpn_lock = threading.Lock()
    _last_switch_time = 0

    # ==============================================================================
    #   Phase 1: æ•°æ®ä¸‹è½½ (çº¯ IO æ“ä½œ)
    #   ç›®æ ‡: åªè´Ÿè´£æŠŠæ•°æ®æ¬è¿åˆ°ç¡¬ç›˜ï¼Œä¸åšä»»ä½•å¤æ‚çš„æ•°å­¦è®¡ç®—
    # ==============================================================================

    @staticmethod
    def _setup_proxy_env():
        """è®¾ç½®ä»£ç†ç¯å¢ƒ"""
        proxy_url = "http://127.0.0.1:7890"
        os.environ['http_proxy'] = proxy_url
        os.environ['https_proxy'] = proxy_url
        os.environ['all_proxy'] = proxy_url
        os.environ['HTTP_PROXY'] = proxy_url
        os.environ['HTTPS_PROXY'] = proxy_url
        os.environ['ALL_PROXY'] = proxy_url

    @classmethod
    def _safe_switch_vpn(cls):
        """çº¿ç¨‹å®‰å…¨çš„ VPN åˆ‡æ¢"""
        with cls._vpn_lock:
            if time.time() - cls._last_switch_time < 5:
                return
            vpn_rotator.switch_random()
            cls._last_switch_time = time.time()
            time.sleep(2)

    @staticmethod
    def _download_worker(code):
        """
        ä¸‹è½½å•å…ƒ
        æ³¨æ„ï¼šè¿™é‡Œåªåšã€æœ€å°åŒ–æ ¼å¼æ¸…æ´—ã€‘ï¼Œç»å¯¹ä¸è®¡ç®— Alpha
        """
        path = os.path.join(Config.DATA_DIR, f"{code}.parquet")

        for attempt in range(5):
            try:
                time.sleep(random.uniform(0.05, 0.2))

                # 1. ç½‘ç»œè¯·æ±‚ (è¿™æ˜¯æœ€è€—æ—¶çš„)
                df = ak.stock_zh_a_hist(symbol=code, period="daily", start_date=Config.START_DATE, adjust="qfq")

                if df is None or df.empty:
                    return code, True, "Empty"

                # 2. æœ€å°åŒ–æ ¼å¼æ¸…æ´— (Standardization)
                # è¿™ä¸æ˜¯â€œå¤„ç†â€ï¼Œè¿™æ˜¯ä¸ºäº†è®©æ•°æ®å­˜ä¸‹æ¥åæ›´å¥½ç”¨ã€‚è€—æ—¶ < 0.001ç§’ã€‚
                # å¦‚æœä¸æ”¹åï¼Œä»¥åè¯»å–æ—¶å…¨æ˜¯ä¸­æ–‡åˆ—åä¼šå¾ˆéº»çƒ¦ã€‚
                df.rename(columns={'æ—¥æœŸ': 'date', 'å¼€ç›˜': 'open', 'æ”¶ç›˜': 'close',
                                   'æœ€é«˜': 'high', 'æœ€ä½': 'low', 'æˆäº¤é‡': 'volume'}, inplace=True)
                df['date'] = pd.to_datetime(df['date'])
                df.set_index('date', inplace=True)

                # 3. å­˜ç›˜ (IO)
                if len(df) > 0:
                    df.to_parquet(path)

                return code, True, "Success"

            except Exception as e:
                DataProvider._safe_switch_vpn()
                continue

        return code, False, "Failed"

    @staticmethod
    def download_data():
        """ä¸‹è½½å…¥å£"""
        print(">>> [Phase 1] åˆå§‹åŒ–ä¸‹è½½å¼•æ“...")
        DataProvider._setup_proxy_env()

        # è·å–åˆ—è¡¨
        codes = []
        for _ in range(5):
            try:
                stock_info = ak.stock_zh_a_spot_em()
                codes = stock_info['ä»£ç '].tolist()
                break
            except:
                vpn_rotator.switch_random()
                time.sleep(2)

        if not codes:
            print("âŒ æ— æ³•è·å–è‚¡ç¥¨åˆ—è¡¨")
            return

        if not os.path.exists(Config.DATA_DIR):
            os.makedirs(Config.DATA_DIR)

        # æé€Ÿæ–­ç‚¹ç»­ä¼ è®¡ç®—
        files = os.listdir(Config.DATA_DIR)
        existing_codes = {f.replace(".parquet", "") for f in files if f.endswith(".parquet")}

        all_codes_set = set(codes)
        todo_codes = list(all_codes_set - existing_codes)
        todo_codes.sort()

        print(f"ğŸ“Š ä»»åŠ¡ç»Ÿè®¡: æ€»æ•° {len(codes)} | å·²å­˜ç›˜ {len(existing_codes)} | å¾…ä¸‹è½½ {len(todo_codes)}")

        if not todo_codes:
            print("âœ… æ‰€æœ‰æ•°æ®å·²ä¸‹è½½å®Œæ¯•ã€‚")
            return

        MAX_WORKERS = 16
        print(f"ğŸš€ å¯åŠ¨ {MAX_WORKERS} çº¿ç¨‹å¹¶å‘ä¸‹è½½...")

        failed_codes = []
        with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
            future_to_code = {executor.submit(DataProvider._download_worker, code): code for code in todo_codes}
            progress_bar = tqdm(concurrent.futures.as_completed(future_to_code), total=len(todo_codes), unit="it")

            for future in progress_bar:
                code = future_to_code[future]
                try:
                    _, is_success, _ = future.result()
                    if not is_success:
                        failed_codes.append(code)
                except:
                    failed_codes.append(code)

        print(f"ä¸‹è½½ç»“æŸã€‚å¤±è´¥æ•°: {len(failed_codes)}")

    # ==============================================================================
    #   Phase 2: æ•°æ®å¤„ç† (CPU å¯†é›†å‹)
    #   ç›®æ ‡: è¯»å–ç¡¬ç›˜æ•°æ®ï¼Œè®¡ç®— Alpha å› å­ï¼Œç”Ÿæˆè®­ç»ƒé›†
    #   æ³¨æ„: è¿™éƒ¨åˆ†ä»£ç åªåœ¨è®­ç»ƒæ—¶è¿è¡Œ (main.py --mode train)
    # ==============================================================================

    @staticmethod
    def process_single_stock(df):
        """
        å•åªè‚¡ç¥¨çš„ Alpha è®¡ç®—
        åªæœ‰åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬æ‰å¼•å…¥ AlphaFactory è¿›è¡Œç¹é‡çš„æ•°å­¦è®¡ç®—
        """
        # å»¶è¿Ÿå¯¼å…¥ï¼šè¯æ˜ä¸‹è½½é˜¶æ®µç»å¯¹æ²¡ç”¨åˆ°å®ƒ
        from .alpha_lib import AlphaFactory

        df['target'] = df['close'].shift(-Config.PRED_LEN) / df['close'] - 1
        factory = AlphaFactory(df)
        df = factory.make_factors()
        factor_cols = [c for c in df.columns if c.startswith('alpha_')]
        keep_cols = factor_cols + ['target']
        df.dropna(subset=keep_cols, inplace=True)
        return df, factor_cols

    def generator(self):
        """
        è®­ç»ƒæ•°æ®ç”Ÿæˆå™¨
        å®ƒä¼š 'Lazily' (æ‡’åŠ è½½) åœ°ä»ç¡¬ç›˜è¯»å– Parquetï¼Œå¤„ç†å®Œä¸€ä¸ªä¸¢ç»™ GPUï¼Œå†è¯»ä¸‹ä¸€ä¸ª
        """
        files = glob.glob(os.path.join(Config.DATA_DIR, "*.parquet"))
        # ç”Ÿäº§ç¯å¢ƒè¯·å»æ‰åˆ‡ç‰‡
        target_files = files[:500]
        for fpath in target_files:
            try:
                df = pd.read_parquet(fpath)
                if len(df) < 100: continue

                # è¿™é‡Œæ‰å¼€å§‹å¤„ç†æ•°æ®
                df_proc, factor_cols = self.process_single_stock(df)

                scaler = StandardScaler()
                x_data = scaler.fit_transform(df_proc[factor_cols].values)
                y_data = df_proc['target'].values
                for i in range(0, len(x_data) - Config.CONTEXT_LEN, 5):
                    yield {
                        "past_values": x_data[i: i + Config.CONTEXT_LEN].astype(np.float32),
                        "labels": y_data[i + Config.CONTEXT_LEN - 1].astype(np.float32)
                    }
            except:
                continue


def get_dataset():
    provider = DataProvider()
    try:
        pass
    except:
        pass

    ds = Dataset.from_generator(provider.generator)
    ds = ds.train_test_split(test_size=0.1)
    temp_gen = provider.generator()
    try:
        first = next(temp_gen)
        num_features = first['past_values'].shape[1]
    except:
        num_features = 12

    return ds, num_features