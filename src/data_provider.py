import akshare as ak
import pandas as pd
import os
import glob
import numpy as np
import time
import random
import threading
import datetime
import concurrent.futures
import pickle
import warnings
from datasets import Dataset
from tqdm import tqdm
from .config import Config
from .vpn_rotator import vpn_rotator
from .alpha_lib import AlphaFactory
from pandarallel import pandarallel

# å¿½ç•¥æ€§èƒ½è­¦å‘Š
warnings.simplefilter(action='ignore', category=pd.errors.PerformanceWarning)

# 1. åˆå§‹åŒ–å¹¶è¡Œç¯å¢ƒ (progress_bar=True å¼€å¯è¿›åº¦æ¡)
# nb_workers æ ¹æ®ä½ çš„CPUæ ¸æ•°è°ƒæ•´ï¼Œé»˜è®¤æ˜¯å…¨éƒ¨
pandarallel.initialize(progress_bar=True, nb_workers=os.cpu_count())


class DataProvider:
    _vpn_lock = threading.Lock()
    _last_switch_time = 0

    @staticmethod
    def _setup_proxy_env():
        """è®¾ç½®ä»£ç†ç¯å¢ƒ"""
        proxy_url = Config.PROXY_URL
        for k in ['http_proxy', 'https_proxy', 'all_proxy', 'HTTP_PROXY', 'HTTPS_PROXY', 'ALL_PROXY']:
            os.environ[k] = proxy_url

    @classmethod
    def _safe_switch_vpn(cls):
        """çº¿ç¨‹å®‰å…¨çš„ VPN åˆ‡æ¢"""
        with cls._vpn_lock:
            # é˜²æ­¢åˆ‡æ¢è¿‡äºé¢‘ç¹ï¼Œå†·å´æ—¶é—´ 5 ç§’
            if time.time() - cls._last_switch_time < 5:
                return
            try:
                vpn_rotator.switch_random()
            except Exception as e:
                print(f"VPN Switch Warning: {e}")
            cls._last_switch_time = time.time()
            time.sleep(2)

    @staticmethod
    def _get_latest_trading_date():
        """è·å–æœ€è¿‘äº¤æ˜“æ—¥"""
        try:
            df = ak.stock_zh_index_daily(symbol=Config.MARKET_INDEX_SYMBOL)
            return pd.to_datetime(df['date']).max().date().strftime("%Y-%m-%d")
        except:
            return datetime.date.today().strftime("%Y-%m-%d")

    @staticmethod
    def _fetch_pub_date_map(code):
        """è·å–è´¢æŠ¥å…¬å‘Šæ—¥æœŸï¼Œç”¨äº PIT (Point-in-Time) å¯¹é½"""
        try:
            df = ak.stock_financial_abstract(symbol=code)
            if df is None or df.empty:
                return None

            if 'æˆªæ­¢æ—¥æœŸ' in df.columns and 'å…¬å‘Šæ—¥æœŸ' in df.columns:
                res = df[['æˆªæ­¢æ—¥æœŸ', 'å…¬å‘Šæ—¥æœŸ']].copy()
                res.columns = ['date', 'pub_date']
                res['date'] = pd.to_datetime(res['date'], errors='coerce')
                res['pub_date'] = pd.to_datetime(res['pub_date'], errors='coerce')
                return res.dropna()
        except:
            pass
        return None

    @staticmethod
    def _download_finance_worker(code):
        """ä¸‹è½½å•åªè‚¡ç¥¨è´¢åŠ¡æ•°æ®"""
        fund_dir = os.path.join(Config.DATA_DIR, "fundamental")
        if not os.path.exists(fund_dir):
            os.makedirs(fund_dir)
        path = os.path.join(fund_dir, f"{code}.parquet")

        # å¢é‡æ›´æ–°æ£€æŸ¥ï¼šå¦‚æœæ˜¯æœ€è¿‘ 3 å¤©å†…æ›´æ–°è¿‡çš„ï¼Œè·³è¿‡
        if os.path.exists(path):
            mtime = os.path.getmtime(path)
            if (time.time() - mtime) < 3 * 24 * 3600:
                return code, True, "Skipped"

        for attempt in range(3):
            try:
                time.sleep(random.uniform(0.1, 0.5))
                df = ak.stock_financial_analysis_indicator_em(symbol=code)
                if df is None or df.empty:
                    return code, True, "Empty"

                df['date'] = pd.to_datetime(df['æ—¥æœŸ'])
                cols_map = {
                    'åŠ æƒå‡€èµ„äº§æ”¶ç›Šç‡': 'roe',
                    'ä¸»è¥ä¸šåŠ¡æ”¶å…¥å¢é•¿ç‡(%)': 'rev_growth',
                    'å‡€åˆ©æ¶¦å¢é•¿ç‡(%)': 'profit_growth',
                    'èµ„äº§è´Ÿå€ºç‡(%)': 'debt_ratio',
                    'å¸‚ç›ˆç‡(åŠ¨æ€)': 'pe_ttm',
                    'å¸‚å‡€ç‡': 'pb'
                }
                valid_cols = [c for c in cols_map.keys() if c in df.columns]
                df = df[['date'] + valid_cols].copy()
                df.rename(columns=cols_map, inplace=True)

                # è·å–å…¬å‘Šæ—¥è¿›è¡Œåˆå¹¶
                pub_df = DataProvider._fetch_pub_date_map(code)
                if pub_df is not None:
                    df = pd.merge(df, pub_df, on='date', how='left')
                else:
                    df['pub_date'] = pd.NaT

                for c in df.columns:
                    if c not in ['date', 'pub_date']:
                        df[c] = pd.to_numeric(df[c], errors='coerce').astype(np.float32)

                df.set_index('date', inplace=True)
                df.to_parquet(path)
                return code, True, "Success"
            except:
                DataProvider._safe_switch_vpn()
                continue
        return code, False, "Failed"

    @staticmethod
    def _download_worker(code):
        """ä¸‹è½½å•åªè‚¡ç¥¨æ—¥çº¿è¡Œæƒ…"""
        path = os.path.join(Config.DATA_DIR, f"{code}.parquet")
        for attempt in range(5):
            try:
                time.sleep(random.uniform(0.05, 0.2))
                # ä½¿ç”¨å‰å¤æƒæ•°æ®
                df = ak.stock_zh_a_hist(symbol=code, period="daily", start_date=Config.START_DATE, adjust="qfq")

                if df is None or df.empty:
                    return code, True, "Empty"

                df.rename(columns={
                    'æ—¥æœŸ': 'date', 'å¼€ç›˜': 'open', 'æ”¶ç›˜': 'close',
                    'æœ€é«˜': 'high', 'æœ€ä½': 'low',
                    'æˆäº¤é‡': 'volume', 'æˆäº¤é¢': 'amount'
                }, inplace=True)

                df['date'] = pd.to_datetime(df['date'])
                df.set_index('date', inplace=True)

                for col in ['open', 'close', 'high', 'low', 'volume', 'amount']:
                    if col in df.columns:
                        df[col] = pd.to_numeric(df[col], errors='coerce').astype(np.float32)

                df.dropna(inplace=True)

                # ç®€å•æ¸…æ´—ï¼šå¤„ç† volume å•ä½å¼‚å¸¸ (éƒ¨åˆ†æ¥å£è¿”å›çš„æ˜¯æ‰‹ï¼Œéƒ¨åˆ†æ˜¯è‚¡)
                if 'amount' in df.columns and 'volume' in df.columns and 'close' in df.columns:
                    valid_sample = df[(df['volume'] > 0) & (df['amount'] > 0)].tail(20)
                    if not valid_sample.empty:
                        # ä¼°ç®—å‡ä»·
                        multiplier = (
                                    valid_sample['amount'] / (valid_sample['close'] * valid_sample['volume'])).median()
                        # å¦‚æœ multiplier æ¥è¿‘ 100ï¼Œè¯´æ˜ volume æ˜¯æ‰‹ï¼Œéœ€è¦ä¹˜ 100
                        if multiplier > 50:
                            df['volume'] = df['volume'] * 100

                if 'amount' in df.columns:
                    df.drop(columns=['amount'], inplace=True)

                if not df.empty:
                    df.sort_index(inplace=True)

                if len(df) > 0:
                    df.to_parquet(path)
                return code, True, "Success"
            except:
                DataProvider._safe_switch_vpn()
                continue
        return code, False, "Failed"

    @staticmethod
    def download_data():
        """ä¸»ä¸‹è½½å…¥å£"""
        print(">>> [ETL] å¯åŠ¨æ•°æ®ä¸‹è½½æµæ°´çº¿...")
        DataProvider._setup_proxy_env()
        if not os.path.exists(Config.DATA_DIR):
            os.makedirs(Config.DATA_DIR)

        try:
            stock_info = ak.stock_zh_a_spot_em()
            codes = stock_info['ä»£ç '].tolist()
        except:
            print("âŒ æ— æ³•è·å–è‚¡ç¥¨åˆ—è¡¨ï¼Œè¯·æ£€æŸ¥ç½‘ç»œæˆ–ä»£ç†é…ç½®")
            return

        target_date_str = DataProvider._get_latest_trading_date()

        # å¢é‡æ£€æŸ¥
        existing_fresh = set()
        for fname in os.listdir(Config.DATA_DIR):
            if fname.endswith(".parquet"):
                fpath = os.path.join(Config.DATA_DIR, fname)
                if os.path.getsize(fpath) > 1024:
                    mtime = os.path.getmtime(fpath)
                    file_date = datetime.date.fromtimestamp(mtime).strftime("%Y-%m-%d")
                    if file_date >= target_date_str:
                        existing_fresh.add(fname.replace(".parquet", ""))

        todo_price = sorted(list(set(codes) - existing_fresh))

        print(f"ğŸ“Š è‚¡ç¥¨æ± æ€»æ•°: {len(codes)} | å¾…æ›´æ–°: {len(todo_price)}")

        # ä¸‹è½½è¡Œæƒ…
        if todo_price:
            with concurrent.futures.ThreadPoolExecutor(max_workers=16) as executor:
                futures = {executor.submit(DataProvider._download_worker, c): c for c in todo_price}
                for _ in tqdm(concurrent.futures.as_completed(futures), total=len(todo_price),
                              desc="Downloading Price"):
                    pass

        # ä¸‹è½½è´¢åŠ¡
        print("æ­£åœ¨åŒæ­¥è´¢åŠ¡æ•°æ®...")
        with concurrent.futures.ThreadPoolExecutor(max_workers=8) as executor:
            futures = {executor.submit(DataProvider._download_finance_worker, c): c for c in codes}
            for _ in tqdm(concurrent.futures.as_completed(futures), total=len(codes), desc="Downloading Finance"):
                pass

        print("âœ… æ•°æ®åŒæ­¥å®Œæˆã€‚")

    @staticmethod
    def _get_cache_path(mode):
        today_str = datetime.date.today().strftime("%Y%m%d")
        return os.path.join(Config.OUTPUT_DIR, f"panel_cache_{mode}_{today_str}.pkl")

    @staticmethod
    def _tag_universe(panel_df):
        """
        [Tagging] æ ‡è®°åŠ¨æ€è‚¡ç¥¨æ± 
        CRITICAL FIX: ä¿®å¤æœªæ¥æ•°æ®æ³„æ¼
        """
        print(">>> [Tagging] æ ‡è®°åŠ¨æ€è‚¡ç¥¨æ±  (Universe Mask)...")

        # 1. ç¡®ä¿æŒ‰æ—¥æœŸæ’åº
        panel_df = panel_df.sort_values(['code', 'date'])

        # 2. è®¡ç®—ç´¯è®¡ä¸Šå¸‚å¤©æ•° (Expanding Window)
        # é”™è¯¯å†™æ³•: transform('count') -> çœ‹äº†æœªæ¥æ•°æ®
        # æ­£ç¡®å†™æ³•: cumcount() -> åªçœ‹è¿‡å»æ•°æ®
        panel_df['list_days_count'] = panel_df.groupby('code')['date'].cumcount() + 1

        # 3. ç­›é€‰æ¡ä»¶
        cond_vol = panel_df['volume'] > 0
        cond_price = panel_df['close'] >= 2.0
        # å¿…é¡»ä¸Šå¸‚è¶…è¿‡ 60 å¤©æ‰çº³å…¥ (å‰”é™¤æ¬¡æ–°è‚¡)
        cond_list = panel_df['list_days_count'] > 60

        panel_df['is_universe'] = cond_vol & cond_price & cond_list

        # æ¸…ç†ä¸´æ—¶åˆ—
        panel_df.drop(columns=['list_days_count'], inplace=True)

        valid_count = panel_df['is_universe'].sum()
        total_count = len(panel_df)
        print(f"Universe è¦†ç›–ç‡: {valid_count}/{total_count} ({valid_count / total_count:.2%})")
        return panel_df

    @staticmethod
    def load_and_process_panel(mode='train', force_refresh=False):
        cache_path = DataProvider._get_cache_path(mode)
        if not force_refresh and os.path.exists(cache_path):
            print(f"âš¡ï¸ [Cache Hit] {cache_path}")
            with open(cache_path, 'rb') as f:
                return pickle.load(f)

        print(f"\n>>> [Processing] æ„å»º Panel æ•°æ® (Mode: {mode})...")
        price_files = glob.glob(os.path.join(Config.DATA_DIR, "*.parquet"))
        fund_dir = os.path.join(Config.DATA_DIR, "fundamental")

        def _read_price(f):
            try:
                df = pd.read_parquet(f)
                if isinstance(df.index, pd.DatetimeIndex) and 'date' not in df.columns:
                    df = df.reset_index()

                df['code'] = os.path.basename(f).replace(".parquet", "")
                float_cols = df.select_dtypes(include=['float64']).columns
                df[float_cols] = df[float_cols].astype(np.float32)
                return df
            except:
                return None

        # å¹¶è¡Œè¯»å–è¡Œæƒ…
        with concurrent.futures.ThreadPoolExecutor(max_workers=8) as executor:
            results = list(tqdm(executor.map(_read_price, price_files), total=len(price_files), desc="Reading Price"))

        data_frames = [df for df in results if df is not None and len(df) > Config.CONTEXT_LEN]
        if not data_frames:
            raise ValueError("æ²¡æœ‰è¶³å¤Ÿçš„æœ‰æ•ˆè¡Œæƒ…æ•°æ®ï¼Œè¯·å…ˆè¿è¡Œ download_data()")

        panel_df = pd.concat(data_frames, ignore_index=True)
        del data_frames  # é‡Šæ”¾å†…å­˜

        panel_df['code'] = panel_df['code'].astype(str)
        panel_df['date'] = pd.to_datetime(panel_df['index'] if 'index' in panel_df.columns else panel_df['date'])

        # è¯»å–è´¢åŠ¡æ•°æ®
        fund_files = glob.glob(os.path.join(fund_dir, "*.parquet"))

        def _read_fund(f):
            try:
                df = pd.read_parquet(f)
                df['code'] = os.path.basename(f).replace(".parquet", "")
                return df
            except:
                return None

        with concurrent.futures.ThreadPoolExecutor(max_workers=8) as executor:
            fund_frames = [df for df in executor.map(_read_fund, fund_files) if df is not None]

        if fund_frames:
            fund_df = pd.concat(fund_frames)
            fund_df = fund_df.reset_index().sort_values(['code', 'date'])

            # PIT å¯¹é½é€»è¾‘
            if 'pub_date' in fund_df.columns:
                # ä¼˜å…ˆä½¿ç”¨å…¬å‘Šæ—¥ä½œä¸º merge_date
                fund_df['merge_date'] = fund_df['pub_date']
                # å¦‚æœå…¬å‘Šæ—¥ç¼ºå¤±ï¼Œå›é€€åˆ° æŠ¥å‘ŠæœŸ + æ»åæ—¶é—´
                mask_na = fund_df['merge_date'].isna()
                report_months = fund_df.loc[mask_na, 'date'].dt.month
                delays = report_months.apply(lambda m: 120 if m == 12 else 60)  # å¹´æŠ¥å»¶è¿Ÿé•¿ï¼Œå­£æŠ¥å»¶è¿ŸçŸ­
                fund_df.loc[mask_na, 'merge_date'] = fund_df.loc[mask_na, 'date'] + pd.to_timedelta(delays, unit='D')
            else:
                fund_df['merge_date'] = fund_df['date'] + pd.Timedelta(days=90)

            fund_df = fund_df.drop(columns=['date', 'pub_date'], errors='ignore')
            fund_df.rename(columns={'merge_date': 'date'}, inplace=True)

            panel_df = panel_df.reset_index().sort_values(['code', 'date'])
            # ä½¿ç”¨ merge_asof è¿›è¡Œ PIT åˆå¹¶ (Backward direction)
            panel_df = pd.merge_asof(panel_df, fund_df, on='date', by='code', direction='backward')

            # å¡«å……è´¢åŠ¡ç¼ºå¤±å€¼
            for c in ['roe', 'rev_growth', 'profit_growth', 'debt_ratio', 'pe_ttm', 'pb']:
                if c in panel_df.columns:
                    panel_df[c] = panel_df[c].fillna(0).astype(np.float32)
            print("âœ… è´¢åŠ¡æ•°æ® PIT å¯¹é½å®Œæˆã€‚")

        if 'date' in panel_df.columns:
            panel_df = panel_df.set_index('date')

        panel_df = panel_df.reset_index().sort_values(['code', 'date'])

        # è®¡ç®—æ—¶åºå› å­
        print("è®¡ç®—æ—¶åºå› å­...")
        # ä½¿ç”¨ pandarallel è¿›è¡Œå¹¶è¡Œè®¡ç®—
        panel_df = panel_df.groupby('code', group_keys=False).parallel_apply(lambda x: AlphaFactory(x).make_factors())

        # æ„é€ é¢„æµ‹ç›®æ ‡ (Labels)
        print("æ„é€ é¢„æµ‹ç›®æ ‡ (Labels)...")
        panel_df['next_open'] = panel_df.groupby('code')['open'].shift(-1)
        panel_df['future_close'] = panel_df.groupby('code')['close'].shift(-Config.PRED_LEN)
        panel_df['target'] = panel_df['future_close'] / panel_df['next_open'] - 1
        panel_df.drop(columns=['next_open', 'future_close'], inplace=True)

        if mode == 'train':
            # è®­ç»ƒæ¨¡å¼ä¸‹ï¼Œå¿…é¡»å»é™¤ Label ä¸ºç©ºçš„è¡Œ
            panel_df.dropna(subset=['target'], inplace=True)

        # æ ‡è®° Universe (åŒ…å« Future Leakage Fix)
        panel_df = DataProvider._tag_universe(panel_df)

        # è®¡ç®—æˆªé¢å› å­ä¸æ ‡å‡†åŒ–
        print("è®¡ç®—æˆªé¢å› å­ä¸æ ‡å‡†åŒ–...")
        panel_df = panel_df.set_index('date')
        panel_df = AlphaFactory.add_cross_sectional_factors(panel_df)

        # æå–ç‰¹å¾åˆ—
        feature_cols = [c for c in panel_df.columns
                        if any(c.startswith(p) for p in Config.FEATURE_PREFIXES)]

        # æœ€ç»ˆæ¸…æ´—
        panel_df[feature_cols] = panel_df[feature_cols].fillna(0).replace([np.inf, -np.inf], 0).astype(np.float32)
        panel_df = panel_df.reset_index()

        # ç¼“å­˜
        with open(cache_path, 'wb') as f:
            pickle.dump((panel_df, feature_cols), f)

        return panel_df, feature_cols

    @staticmethod
    def make_dataset(panel_df, feature_cols):
        print(">>> [Dataset] è½¬æ¢å¼ é‡æ ¼å¼...")
        panel_df = panel_df.sort_values(['code', 'date']).reset_index(drop=True)

        feature_matrix = panel_df[feature_cols].values.astype(np.float32)
        if 'rank_label' in panel_df.columns:
            target_array = panel_df['rank_label'].fillna(0.5).values.astype(np.float32)
        else:
            target_array = panel_df['target'].fillna(0).values.astype(np.float32)

        universe_mask = panel_df['is_universe'].values
        dates = panel_df['date'].values
        codes = panel_df['code'].values

        # å¿«é€Ÿå®šä½æ¯åªè‚¡ç¥¨çš„èµ·å§‹/ç»“æŸä½ç½®
        code_changes = np.where(codes[:-1] != codes[1:])[0] + 1
        start_indices = np.concatenate(([0], code_changes))
        end_indices = np.concatenate((code_changes, [len(codes)]))

        valid_indices = []
        seq_len = Config.CONTEXT_LEN
        stride = Config.STRIDE

        # ç”Ÿæˆæœ‰æ•ˆçš„æ—¶é—´çª—å£ç´¢å¼•
        for start, end in zip(start_indices, end_indices):
            if end - start <= seq_len:
                continue
            # æ»‘åŠ¨çª—å£é‡‡æ ·
            for i in range(start + seq_len - 1, end, stride):
                # åªæœ‰å½“é¢„æµ‹ç‚¹å±äº Universe æ—¶æ‰åŠ å…¥æ ·æœ¬
                if universe_mask[i]:
                    valid_indices.append(i - seq_len + 1)

        valid_indices = np.array(valid_indices)

        # æŒ‰æ—¶é—´åˆ‡åˆ† Train/Test (Time Series Split)
        unique_dates = np.sort(np.unique(dates))
        split_idx = int(len(unique_dates) * 0.9)
        split_date = unique_dates[split_idx]

        # è·å–æ ·æœ¬å¯¹åº”çš„é¢„æµ‹æ—¥æœŸ (T)
        sample_pred_dates = dates[valid_indices + seq_len - 1]

        train_mask = sample_pred_dates < split_date
        # Test é›†éœ€ç•™å‡º Gapï¼Œé˜²æ­¢æ•°æ®é‡å 
        gap_date = unique_dates[min(split_idx + Config.CONTEXT_LEN, len(unique_dates) - 1)]
        test_mask = sample_pred_dates > gap_date

        train_indices = valid_indices[train_mask]
        test_indices = valid_indices[test_mask]

        print(f"æ ·æœ¬åˆ†å‰²: Train={len(train_indices)}, Test={len(test_indices)}")

        def gen_train():
            np.random.shuffle(train_indices)  # è®­ç»ƒé›†æ‰“ä¹±
            for start_idx in train_indices:
                end_idx = start_idx + seq_len
                yield {
                    "past_values": feature_matrix[start_idx: end_idx],
                    "labels": target_array[end_idx - 1]
                }

        def gen_valid():
            for start_idx in test_indices:
                end_idx = start_idx + seq_len
                yield {
                    "past_values": feature_matrix[start_idx: end_idx],
                    "labels": target_array[end_idx - 1]
                }

        from datasets import DatasetDict
        ds = DatasetDict({
            'train': Dataset.from_generator(gen_train),
            'test': Dataset.from_generator(gen_valid)
        })
        return ds, len(feature_cols)


def get_dataset(force_refresh=False):
    """
    å¯¹å¤–æš´éœ²çš„æ•°æ®è·å–æ¥å£
    """
    panel_df, feature_cols = DataProvider.load_and_process_panel(mode='train', force_refresh=force_refresh)
    ds, num_features = DataProvider.make_dataset(panel_df, feature_cols)
    return ds, num_features