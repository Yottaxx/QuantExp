import akshare as ak
import pandas as pd
import os
import glob
import numpy as np
import time
import random
import requests
import threading
import datetime
import concurrent.futures
from datasets import Dataset
from sklearn.preprocessing import StandardScaler
from tqdm import tqdm
from .config import Config
from .vpn_rotator import vpn_rotator
from .alpha_lib import AlphaFactory


class DataProvider:
    _vpn_lock = threading.Lock()
    _last_switch_time = 0

    # --------------------------------------------------------------------------
    # PART 1: ä¸‹è½½æ¨¡å— (ä¿æŒä¸å˜)
    # --------------------------------------------------------------------------

    @staticmethod
    def _setup_proxy_env():
        proxy_url = "http://127.0.0.1:7890"
        os.environ['http_proxy'] = proxy_url
        os.environ['https_proxy'] = proxy_url
        os.environ['all_proxy'] = proxy_url
        os.environ['HTTP_PROXY'] = proxy_url
        os.environ['HTTPS_PROXY'] = proxy_url
        os.environ['ALL_PROXY'] = proxy_url

    @classmethod
    def _safe_switch_vpn(cls):
        with cls._vpn_lock:
            if time.time() - cls._last_switch_time < 5:
                return
            vpn_rotator.switch_random()
            cls._last_switch_time = time.time()
            time.sleep(2)

    @staticmethod
    def _download_worker(code):
        path = os.path.join(Config.DATA_DIR, f"{code}.parquet")
        for attempt in range(5):
            try:
                time.sleep(random.uniform(0.05, 0.2))
                df = ak.stock_zh_a_hist(symbol=code, period="daily", start_date=Config.START_DATE, adjust="qfq")
                if df is None or df.empty: return code, True, "Empty"

                df.rename(columns={'æ—¥æœŸ': 'date', 'å¼€ç›˜': 'open', 'æ”¶ç›˜': 'close',
                                   'æœ€é«˜': 'high', 'æœ€ä½': 'low', 'æˆäº¤é‡': 'volume'}, inplace=True)
                df['date'] = pd.to_datetime(df['date'])
                df.set_index('date', inplace=True)

                if len(df) > 0: df.to_parquet(path)
                return code, True, "Success"
            except:
                DataProvider._safe_switch_vpn()
                continue
        return code, False, "Failed"

    @staticmethod
    def download_data():
        """ä¸‹è½½å…¨å¸‚åœºæ•°æ®"""
        print(">>> [Phase 1] å¯åŠ¨æ•°æ®ä¸‹è½½...")
        DataProvider._setup_proxy_env()

        if not os.path.exists(Config.DATA_DIR): os.makedirs(Config.DATA_DIR)

        try:
            stock_info = ak.stock_zh_a_spot_em()
            codes = stock_info['ä»£ç '].tolist()
        except:
            print("âŒ æ— æ³•è·å–è‚¡ç¥¨åˆ—è¡¨ï¼Œè¯·æ£€æŸ¥ç½‘ç»œ/VPN")
            return

        files = os.listdir(Config.DATA_DIR)
        existing = {f.replace(".parquet", "") for f in files if
                    f.endswith(".parquet") and os.path.getsize(os.path.join(Config.DATA_DIR, f)) > 1024}
        todo = list(set(codes) - existing)
        todo.sort()

        print(f"ğŸ“Š ä»»åŠ¡: æ€»æ•° {len(codes)} | å¾…ä¸‹è½½ {len(todo)}")
        if not todo:
            print("âœ… æ•°æ®å·²æœ€æ–°ã€‚")
            return

        with concurrent.futures.ThreadPoolExecutor(max_workers=16) as executor:
            futures = {executor.submit(DataProvider._download_worker, c): c for c in todo}
            for _ in tqdm(concurrent.futures.as_completed(futures), total=len(todo)):
                pass
        print("ä¸‹è½½å®Œæˆã€‚")

    # --------------------------------------------------------------------------
    # PART 2: æ ¸å¿ƒé‡æ„ - å†…å­˜ Panel å¤„ç† (å« Phase 2 è¿‡æ»¤)
    # --------------------------------------------------------------------------

    @staticmethod
    def _filter_universe(panel_df):
        """
        ã€Phase 2 æ ¸å¿ƒã€‘åŠ¨æ€å®‡å®™è¿‡æ»¤
        ç›®çš„ï¼šæ¸…æ´—æ‰ä¸é€‚åˆäº¤æ˜“çš„è„æ•°æ®ï¼Œé˜²æ­¢æ¨¡å‹å­¦åã€‚
        æ³¨æ„ï¼šå¿…é¡»åœ¨æ—¶åºå› å­è®¡ç®—å®Œæˆåè°ƒç”¨ï¼Œä½†åœ¨æˆªé¢å› å­è®¡ç®—å‰è°ƒç”¨ã€‚
        """
        print(">>> [Filtering] æ­£åœ¨æ‰§è¡ŒåŠ¨æ€è‚¡ç¥¨æ± è¿‡æ»¤...")
        original_len = len(panel_df)

        # 1. å‰”é™¤åœç‰Œ (Volume = 0)
        # åœç‰ŒæœŸé—´æ— æ³•äº¤æ˜“ï¼Œä¸”å¤ç‰Œåå¾€å¾€ä¼šæœ‰å‰§çƒˆè·³ç©ºï¼Œæ˜¯æå¤§çš„å™ªéŸ³
        panel_df = panel_df[panel_df['volume'] > 0]

        # 2. å‰”é™¤åƒåœ¾è‚¡/å‡†é€€å¸‚è‚¡ (Close < 2.0)
        # ä½ä»·è‚¡å¾€å¾€ä¼´éšæµåŠ¨æ€§é™·é˜±æˆ–é€€å¸‚é£é™©ï¼Œé‡åŒ–ç­–ç•¥åº”å°½é‡é¿å¼€
        panel_df = panel_df[panel_df['close'] >= 2.0]

        # 3. å‰”é™¤ä¸Šå¸‚ä¸æ»¡ 60 å¤©çš„æ¬¡æ–°è‚¡
        # é€»è¾‘ï¼šæŒ‰ code åˆ†ç»„ï¼Œè®¡ç®—ç´¯è®¡äº¤æ˜“å¤©æ•°ã€‚å‰ 60 å¤©çš„æ•°æ®ä¸ç¨³ï¼Œå‰”é™¤ã€‚
        # ä½¿ç”¨ cumcount() é«˜æ•ˆç”Ÿæˆåºå·
        panel_df['list_days'] = panel_df.groupby('code').cumcount()
        panel_df = panel_df[panel_df['list_days'] > 60]

        # æ¸…ç†ä¸´æ—¶åˆ—
        panel_df = panel_df.drop(columns=['list_days'])

        new_len = len(panel_df)
        print(f"è¿‡æ»¤å®Œæˆã€‚ç§»é™¤æ ·æœ¬: {original_len - new_len} ({1 - new_len / original_len:.2%})")
        return panel_df

    @staticmethod
    def load_and_process_panel():
        """
        å…¨å†…å­˜åŠ è½½ä¸å¤„ç†æ ¸å¿ƒå‡½æ•°
        """
        print("\n>>> [Phase 2] å¼€å§‹æ„å»ºå…¨å†…å­˜ Panel æ•°æ®...")

        files = glob.glob(os.path.join(Config.DATA_DIR, "*.parquet"))
        if not files:
            raise ValueError("æ²¡æœ‰æ‰¾åˆ°æ•°æ®æ–‡ä»¶ï¼Œè¯·å…ˆè¿è¡Œ download")

        # --- Step 1: å¹¶è¡Œè¯»å– ---
        print(f"æ­£åœ¨åŠ è½½ {len(files)} ä¸ªæ–‡ä»¶åˆ°å†…å­˜...")

        def _read_helper(f):
            try:
                df = pd.read_parquet(f)
                code = os.path.basename(f).replace(".parquet", "")
                df['code'] = code
                return df
            except:
                return None

        with concurrent.futures.ThreadPoolExecutor(max_workers=8) as executor:
            # ä½¿ç”¨ list() å¼ºåˆ¶æ‰§è¡Œ map
            results = list(tqdm(executor.map(_read_helper, files), total=len(files), desc="Reading"))

        # è¿‡æ»¤æ— æ•ˆæ•°æ®ï¼Œåˆå¹¶
        data_frames = [df for df in results if df is not None and len(df) > Config.CONTEXT_LEN + 10]
        if not data_frames: raise ValueError("æœ‰æ•ˆæ•°æ®ä¸ºç©º")

        print("æ­£åœ¨åˆå¹¶ Panel DataFrame...")
        panel_df = pd.concat(data_frames)
        del data_frames  # é‡Šæ”¾å†…å­˜

        # é‡ç½®ç´¢å¼•ï¼Œç¡®ä¿ 'date' æ˜¯åˆ—
        panel_df = panel_df.reset_index().sort_values(['code', 'date'])

        # --- Step 2: è®¡ç®—æ—¶åºå› å­ (TS Factors) ---
        # æ³¨æ„ï¼šå¿…é¡»åœ¨è¿‡æ»¤ä¹‹å‰è®¡ç®—ï¼Œå¦åˆ™å› ä¸ºæŸäº›å¤©è¢«å‰”é™¤å¯¼è‡´ rolling è®¡ç®—ä¸­æ–­
        print("æ­£åœ¨è®¡ç®—æ—¶åºå› å­ (TS Factors)...")

        def _process_ts(df_sub):
            factory = AlphaFactory(df_sub)
            return factory.make_factors()

        # ä¼˜åŒ–ï¼šåªå¯¹éœ€è¦çš„åˆ—è¿›è¡Œ groupby è¿ç®—ï¼Œé˜²æ­¢å†…å­˜çˆ†ç‚¸
        # group_keys=False é¿å…ç´¢å¼•å±‚çº§å¢åŠ 
        panel_df = panel_df.groupby('code', group_keys=False).apply(_process_ts)

        # --- Step 3: æ„é€  Label ---
        # é¢„æµ‹æœªæ¥ N å¤©æ”¶ç›Š
        print("æ­£åœ¨æ„é€ é¢„æµ‹ç›®æ ‡ (Future Returns)...")
        panel_df['target'] = panel_df.groupby('code')['close'].shift(-Config.PRED_LEN) / panel_df['close'] - 1

        # å‰”é™¤ label ä¸ºç©ºçš„è¡Œ (æœ€å N å¤©)
        panel_df.dropna(subset=['target'], inplace=True)

        # --- Step 4: æ‰§è¡ŒåŠ¨æ€è¿‡æ»¤ (Filtering) ---
        # ã€å…³é”®ã€‘åœ¨è¿™é‡Œåˆ‡é™¤åƒåœ¾æ•°æ®ï¼Œç¡®ä¿åç»­çš„æˆªé¢æ’ååªåœ¨ä¼˜è´¨è‚¡ç¥¨ä¸­è¿›è¡Œ
        panel_df = DataProvider._filter_universe(panel_df)

        # --- Step 5: è®¡ç®—æˆªé¢å› å­ & è¶…é¢æ”¶ç›Š Label ---
        # æ­¤æ—¶ panel_df å·²ç»å¾ˆå¹²å‡€äº†ï¼Œè®¡ç®— cs_rank ä¼šæ›´å‡†ç¡®
        # é‡ç½®ç´¢å¼•ä¸º dateï¼Œæ–¹ä¾¿ AlphaFactory å¤„ç†
        panel_df = panel_df.set_index('date')
        panel_df = AlphaFactory.add_cross_sectional_factors(panel_df)

        # --- Step 6: æœ€ç»ˆæ¸…æ´— ---
        feature_cols = [c for c in panel_df.columns
                        if any(c.startswith(p) for p in ['style_', 'tech_', 'alpha_', 'adv_', 'cs_rank_'])]

        print(f"å› å­å·¥ç¨‹å®Œæˆã€‚ç‰¹å¾ç»´åº¦: {len(feature_cols)}")
        # å¡«å…… NaN
        panel_df[feature_cols] = panel_df[feature_cols].fillna(0)

        # é‡ç½®ç´¢å¼•å›æ¥ï¼Œæ–¹ä¾¿åç»­æ’åº
        panel_df = panel_df.reset_index()

        return panel_df, feature_cols

    @staticmethod
    def make_dataset(panel_df, feature_cols):
        """
        å°† Panel DataFrame è½¬æ¢ä¸º PyTorch å‹å¥½çš„ Dataset
        """
        print(">>> [Phase 3] è½¬æ¢ Dataset...")

        # 1. æ’åº: å¿…é¡»æŒ‰ (code, date) æ’åºä»¥ä¿è¯æ»‘åŠ¨çª—å£æ­£ç¡®
        panel_df = panel_df.sort_values(['code', 'date'])

        # 2. æå– numpy æ•°ç»„ (ä½¿ç”¨ float32 å‹ç¼©å†…å­˜)
        feature_matrix = panel_df[feature_cols].values.astype(np.float32)

        # ã€å…³é”®ã€‘ä½¿ç”¨ 'excess_label' (è¶…é¢æ”¶ç›Š) ä½œä¸ºè®­ç»ƒç›®æ ‡
        # å¦‚æœæ²¡æœ‰ excess_labelï¼Œå›é€€åˆ° target
        target_col = 'excess_label' if 'excess_label' in panel_df.columns else 'target'
        print(f"ä½¿ç”¨è®­ç»ƒç›®æ ‡: {target_col}")
        target_array = panel_df[target_col].values.astype(np.float32)

        # 3. æ„å»ºæ ·æœ¬ç´¢å¼•
        codes = panel_df['code'].values
        code_changes = np.where(codes[:-1] != codes[1:])[0] + 1
        start_indices = np.concatenate(([0], code_changes))
        end_indices = np.concatenate((code_changes, [len(codes)]))

        valid_indices = []
        seq_len = Config.CONTEXT_LEN
        stride = 5

        for start, end in zip(start_indices, end_indices):
            length = end - start
            if length <= seq_len: continue

            # æ»‘åŠ¨çª—å£åˆ‡ç‰‡
            for i in range(start, end - seq_len + 1, stride):
                valid_indices.append(i)

        print(f"ç”Ÿæˆçš„æ ·æœ¬æ•°é‡: {len(valid_indices)}")

        def gen():
            for idx in valid_indices:
                yield {
                    "past_values": feature_matrix[idx: idx + seq_len],
                    "labels": target_array[idx + seq_len - 1]
                }

        ds = Dataset.from_generator(gen)
        ds = ds.train_test_split(test_size=0.1, shuffle=True)

        return ds, len(feature_cols)


# å¯¹å¤–æ¥å£
def get_dataset():
    panel_df, feature_cols = DataProvider.load_and_process_panel()
    ds, num_features = DataProvider.make_dataset(panel_df, feature_cols)
    return ds, num_features