import akshare as ak
import pandas as pd
import os
import glob
import numpy as np
import time
import random
import requests
import threading
import datetime
import concurrent.futures
from datasets import Dataset
from sklearn.preprocessing import StandardScaler
from tqdm import tqdm
from .config import Config
from .vpn_rotator import vpn_rotator
from .alpha_lib import AlphaFactory


class DataProvider:
    _vpn_lock = threading.Lock()
    _last_switch_time = 0

    # --------------------------------------------------------------------------
    # PART 1: Âü∫Á°ÄËÆæÊñΩ‰∏éËæÖÂä©ÂáΩÊï∞
    # --------------------------------------------------------------------------

    @staticmethod
    def _setup_proxy_env():
        proxy_url = "http://127.0.0.1:7890"
        for k in ['http_proxy', 'https_proxy', 'all_proxy', 'HTTP_PROXY', 'HTTPS_PROXY', 'ALL_PROXY']:
            os.environ[k] = proxy_url

    @classmethod
    def _safe_switch_vpn(cls):
        with cls._vpn_lock:
            if time.time() - cls._last_switch_time < 5: return
            vpn_rotator.switch_random()
            cls._last_switch_time = time.time()
            time.sleep(2)

    @staticmethod
    def _get_latest_trading_date():
        """
        „ÄêÊñ∞Â¢û„ÄëËé∑ÂèñÊúÄËøëÁöÑ‰∏Ä‰∏™‰∫§ÊòìÊó•
        Èò≤Ê≠¢Âë®Êú´/ËäÇÂÅáÊó•ËøêË°åËÑöÊú¨Êó∂ÈáçÂ§ç‰∏ãËΩΩ
        """
        try:
            # Ëé∑Âèñ‰∏äËØÅÊåáÊï∞ÁöÑÊúÄÊñ∞Êó•Á∫øÊï∞ÊçÆ‰Ωú‰∏∫ÂèÇËÄÉ
            # ËøôÈáåÁöÑ symbol ÊòØ sh000001 (‰∏äËØÅÊåáÊï∞)
            df = ak.stock_zh_index_daily(symbol="sh000001")
            latest_date = pd.to_datetime(df['date']).max().date()
            return latest_date.strftime("%Y-%m-%d")
        except:
            # Â¶ÇÊûúËé∑ÂèñÂ§±Ë¥•ÔºåÈÄÄÂåñ‰∏∫‰ΩøÁî®‰ªäÂ§©
            return datetime.date.today().strftime("%Y-%m-%d")

    # --------------------------------------------------------------------------
    # PART 2: ‰∏ãËΩΩÊ®°Âùó (Memory & Calendar Optimized)
    # --------------------------------------------------------------------------

    @staticmethod
    def _download_worker(code):
        path = os.path.join(Config.DATA_DIR, f"{code}.parquet")
        for attempt in range(5):
            try:
                time.sleep(random.uniform(0.05, 0.2))
                df = ak.stock_zh_a_hist(symbol=code, period="daily", start_date=Config.START_DATE, adjust="qfq")
                if df is None or df.empty: return code, True, "Empty"

                df.rename(columns={'Êó•Êúü': 'date', 'ÂºÄÁõò': 'open', 'Êî∂Áõò': 'close',
                                   'ÊúÄÈ´ò': 'high', 'ÊúÄ‰Ωé': 'low', 'Êàê‰∫§Èáè': 'volume'}, inplace=True)
                df['date'] = pd.to_datetime(df['date'])
                df.set_index('date', inplace=True)

                # „Äê‰ºòÂåñ„ÄëÂ≠òÁõòÂâçËΩ¨‰∏∫ float32 ‰ª•ËäÇÁúÅÁ£ÅÁõòÁ©∫Èó¥
                for col in ['open', 'close', 'high', 'low', 'volume']:
                    if col in df.columns:
                        df[col] = df[col].astype(np.float32)

                if len(df) > 0: df.to_parquet(path)
                return code, True, "Success"
            except:
                DataProvider._safe_switch_vpn()
                continue
        return code, False, "Failed"

    @staticmethod
    def download_data():
        print(">>> [Phase 1] ÂêØÂä®Êï∞ÊçÆ‰∏ãËΩΩ (Êô∫ËÉΩÂ¢ûÈáèÊ®°Âºè)...")
        DataProvider._setup_proxy_env()

        if not os.path.exists(Config.DATA_DIR): os.makedirs(Config.DATA_DIR)

        try:
            stock_info = ak.stock_zh_a_spot_em()
            codes = stock_info['‰ª£Á†Å'].tolist()
        except:
            print("‚ùå Êó†Ê≥ïËé∑ÂèñËÇ°Á•®ÂàóË°®ÔºåËØ∑Ê£ÄÊü•ÁΩëÁªú/VPN")
            return

        # 1. Ëé∑ÂèñÂ∏ÇÂú∫ÊúÄÊñ∞ÁöÑ‰∫§ÊòìÊó• (‰æãÂ¶Ç‰ªäÂ§©ÊòØÂë®ÂÖ≠Ôºåtarget_date Â∫îËØ•ÊòØÂë®‰∫î)
        print(">>> Ê≠£Âú®Ê†°ÂØπ‰∫§ÊòìÊó•ÂéÜ...")
        target_date_str = DataProvider._get_latest_trading_date()
        print(f"üìÖ ÊúÄËøë‰∫§ÊòìÊó•ÈîÅÂÆö‰∏∫: {target_date_str}")

        # 2. Êô∫ËÉΩËøáÊª§
        existing_fresh = set()
        files = os.listdir(Config.DATA_DIR)

        for fname in files:
            if fname.endswith(".parquet"):
                fpath = os.path.join(Config.DATA_DIR, fname)
                # Ê£ÄÊü•1: Êñá‰ª∂‰∏ç‰∏∫Á©∫
                if os.path.getsize(fpath) > 1024:
                    # Ê£ÄÊü•2: ‰øÆÊîπÊó∂Èó¥ >= ÁõÆÊ†á‰∫§ÊòìÊó•
                    # Âè™Ë¶ÅÊñá‰ª∂ÁöÑ‰øÆÊîπÊó•ÊúüÊòØÂú®ÁõÆÊ†á‰∫§ÊòìÊó•‰πãÂêé(Âê´)ÔºåËØ¥ÊòéÂåÖÂê´‰∫ÜÊúÄÊñ∞Êï∞ÊçÆ
                    mtime = os.path.getmtime(fpath)
                    file_date = datetime.date.fromtimestamp(mtime).strftime("%Y-%m-%d")
                    if file_date >= target_date_str:
                        existing_fresh.add(fname.replace(".parquet", ""))

        todo = list(set(codes) - existing_fresh)
        todo.sort()

        print(f"üìä ‰ªªÂä°: ÊÄªÊï∞ {len(codes)} | Â∑≤ÊòØÊúÄÊñ∞ {len(existing_fresh)} | ÂæÖÊõ¥Êñ∞ {len(todo)}")
        if not todo:
            print("‚úÖ ÊâÄÊúâÊï∞ÊçÆÂ∑≤ÂêåÊ≠•Ëá≥ÊúÄÊñ∞‰∫§ÊòìÊó•„ÄÇ")
            return

        with concurrent.futures.ThreadPoolExecutor(max_workers=16) as executor:
            futures = {executor.submit(DataProvider._download_worker, c): c for c in todo}
            for _ in tqdm(concurrent.futures.as_completed(futures), total=len(todo)):
                pass
        print("‰∏ãËΩΩÂÆåÊàê„ÄÇ")

    # --------------------------------------------------------------------------
    # PART 3: ÂÜÖÂ≠ò Panel Â§ÑÁêÜ (Float32 ÂÜÖÂ≠ò‰ºòÂåñÁâà)
    # --------------------------------------------------------------------------

    @staticmethod
    def _filter_universe(panel_df):
        print(">>> [Filtering] Âä®ÊÄÅËøáÊª§...")
        original_len = len(panel_df)
        panel_df = panel_df[panel_df['volume'] > 0]
        panel_df = panel_df[panel_df['close'] >= 2.0]
        # ‰ΩøÁî® transform Êõø‰ª£ groupby.cumcount Á®çÂæÆÂø´‰∏ÄÁÇπÁÇπÔºåÊàñËÄÖ‰øùÊåÅÂéüÊ†∑
        panel_df['list_days'] = panel_df.groupby('code')['close'].transform('count')
        # Ê≥®ÊÑèÔºö‰∏äÈù¢ÁöÑ list_days ÈÄªËæëÂèò‰∫ÜÔºåÂèòÊàêÊÄªÂ§©Êï∞ÔºåËøô‰∏çÂØπ„ÄÇ
        # ËøòÊòØ‰øùÊåÅ cumcount Ê≠£Á°Æ
        panel_df['list_days'] = panel_df.groupby('code').cumcount()

        panel_df = panel_df[panel_df['list_days'] > 60]
        panel_df = panel_df.drop(columns=['list_days'])
        print(f"ËøáÊª§ÁßªÈô§: {original_len - new_len} ({1 - len(panel_df) / original_len:.2%})")
        return panel_df

    @staticmethod
    def load_and_process_panel(mode='train'):
        print(f"\n>>> [Phase 2] ÊûÑÂª∫ÂÖ®ÂÜÖÂ≠ò Panel (Mode: {mode}, Opt: Float32)...")

        files = glob.glob(os.path.join(Config.DATA_DIR, "*.parquet"))
        if not files: raise ValueError("Êó†Êï∞ÊçÆÊñá‰ª∂")

        print(f"Ê≠£Âú®Âä†ËΩΩ {len(files)} ‰∏™Êñá‰ª∂...")

        def _read_helper(f):
            try:
                # „Äê‰ºòÂåñ„ÄëËØªÂèñÊó∂Áõ¥Êé•ÊåáÂÆöÂàóÁ±ªÂûãÔºåÂ§ßÂπÖÂáèÂ∞ëÂÜÖÂ≠òÂºÄÈîÄ
                df = pd.read_parquet(f)
                code = os.path.basename(f).replace(".parquet", "")
                # Âº∫Âà∂ËΩ¨ float32
                float_cols = df.select_dtypes(include=['float64']).columns
                df[float_cols] = df[float_cols].astype(np.float32)
                df['code'] = code
                # Â∞Ü code ËΩ¨‰∏∫ category Á±ªÂûãËøõ‰∏ÄÊ≠•ÁúÅÂÜÖÂ≠ò
                df['code'] = df['code'].astype('category')
                return df
            except:
                return None

        with concurrent.futures.ThreadPoolExecutor(max_workers=8) as executor:
            results = list(tqdm(executor.map(_read_helper, files), total=len(files), desc="Reading"))

        data_frames = [df for df in results if df is not None and len(df) > Config.CONTEXT_LEN + 10]
        if not data_frames: raise ValueError("ÊúâÊïàÊï∞ÊçÆ‰∏∫Á©∫")

        print("ÂêàÂπ∂ DataFrame...")
        panel_df = pd.concat(data_frames, ignore_index=False)
        del data_frames

        # ÊÅ¢Â§ç code ‰∏∫ string ‰ª•‰æøÂêéÁª≠Â§ÑÁêÜ (pandas category Âú® groupby apply ÊúâÊó∂‰ºöÊúâÂùë)
        panel_df['code'] = panel_df['code'].astype(str)
        panel_df = panel_df.reset_index().sort_values(['code', 'date'])

        print("ËÆ°ÁÆóÊó∂Â∫èÂõ†Â≠ê...")
        # ËøôÈáåÁöÑ apply ‰æùÁÑ∂ÊòØÂÜÖÂ≠òÁì∂È¢àÔºåÊó†Ê≥ïÈÅøÂÖçÔºå‰ΩÜÁî±‰∫éËæìÂÖ•Â∑≤ÁªèÊòØ float32Ôºå‰ºöÂ•ΩÂæàÂ§ö
        panel_df = panel_df.groupby('code', group_keys=False).apply(lambda x: AlphaFactory(x).make_factors())

        print("ÊûÑÈÄ† Target...")
        panel_df['target'] = panel_df.groupby('code')['close'].shift(-Config.PRED_LEN) / panel_df['close'] - 1

        if mode == 'train':
            panel_df.dropna(subset=['target'], inplace=True)

        # Âä®ÊÄÅËøáÊª§
        print("Âä®ÊÄÅËøáÊª§...")
        # Â§çÁî®‰ª£Á†ÅÈÄªËæë...
        original_len = len(panel_df)
        panel_df = panel_df[panel_df['volume'] > 0]
        panel_df = panel_df[panel_df['close'] >= 2.0]
        panel_df['list_days'] = panel_df.groupby('code').cumcount()
        panel_df = panel_df[panel_df['list_days'] > 60]
        panel_df.drop(columns=['list_days'], inplace=True)
        print(f"ËøáÊª§ÁßªÈô§: {original_len - len(panel_df)}")

        # Êà™Èù¢Âõ†Â≠ê
        print("ËÆ°ÁÆóÊà™Èù¢Âõ†Â≠ê...")
        panel_df = panel_df.set_index('date')
        panel_df = AlphaFactory.add_cross_sectional_factors(panel_df)

        feature_cols = [c for c in panel_df.columns
                        if any(c.startswith(p) for p in ['style_', 'tech_', 'alpha_', 'adv_', 'cs_rank_'])]

        # ÊúÄÁªàËΩ¨ float32
        panel_df[feature_cols] = panel_df[feature_cols].fillna(0).astype(np.float32)

        panel_df = panel_df.reset_index()
        return panel_df, feature_cols

    # ... [make_dataset Á≠â‰øùÊåÅ‰∏çÂèò] ...
    @staticmethod
    def make_dataset(panel_df, feature_cols):
        print(">>> [Phase 3] ËΩ¨Êç¢ Dataset...")
        panel_df = panel_df.sort_values(['code', 'date'])

        feature_matrix = panel_df[feature_cols].values  # Â∑≤ÁªèÊòØ float32

        target_col = 'excess_label' if 'excess_label' in panel_df.columns else 'target'
        target_array = panel_df[target_col].fillna(0).values.astype(np.float32)

        codes = panel_df['code'].values
        code_changes = np.where(codes[:-1] != codes[1:])[0] + 1
        start_indices = np.concatenate(([0], code_changes))
        end_indices = np.concatenate((code_changes, [len(codes)]))

        valid_indices = []
        seq_len = Config.CONTEXT_LEN
        stride = 5

        for start, end in zip(start_indices, end_indices):
            length = end - start
            if length <= seq_len: continue
            for i in range(start, end - seq_len + 1, stride):
                valid_indices.append(i)

        print(f"Ê†∑Êú¨Êï∞: {len(valid_indices)}")

        # Êó∂Èó¥ÂàáÂàÜ (Time-Series Split)
        dates = panel_df['date'].unique()
        dates.sort()
        split_idx = int(len(dates) * 0.9)
        split_date = dates[split_idx]
        print(f"ÂàáÂàÜÊó•Êúü: {split_date}")

        sample_dates = panel_df['date'].values[np.array(valid_indices) + seq_len - 1]
        train_mask = sample_dates < split_date
        train_indices = np.array(valid_indices)[train_mask]
        valid_indices = np.array(valid_indices)[~train_mask]

        def gen_train():
            np.random.shuffle(train_indices)
            for idx in train_indices:
                yield {"past_values": feature_matrix[idx: idx + seq_len], "labels": target_array[idx + seq_len - 1]}

        def gen_valid():
            for idx in valid_indices:
                yield {"past_values": feature_matrix[idx: idx + seq_len], "labels": target_array[idx + seq_len - 1]}

        from datasets import DatasetDict
        ds = DatasetDict({
            'train': Dataset.from_generator(gen_train),
            'test': Dataset.from_generator(gen_valid)
        })

        return ds, len(feature_cols)


def get_dataset():
    panel_df, feature_cols = DataProvider.load_and_process_panel(mode='train')
    ds, num_features = DataProvider.make_dataset(panel_df, feature_cols)
    return ds, num_features