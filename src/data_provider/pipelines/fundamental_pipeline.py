from __future__ import annotations

import concurrent.futures
import os
import re
import time
from collections import deque
from dataclasses import dataclass
from typing import Iterable, List, Optional, Tuple

import numpy as np
import pandas as pd
import akshare as ak
from tqdm.auto import tqdm

from ..clients.ak_client import AkClient
from ..core.config import DPConfig
from ..utils.code import normalize_code
from ..utils.io import atomic_save_parquet
from ..stores.paths import fundamental_dir, fundamental_path

# åŒ¹é… YYYYMMDD æ ¼å¼çš„åˆ—å
DATE_COL_RE = re.compile(r"^\d{8}$")
# å®½è¡¨ä¸­å¯èƒ½åŒ…å«æŒ‡æ ‡åç§°çš„åˆ—
IND_COL_CANDIDATES = ("æŒ‡æ ‡", "é¡¹ç›®", "ç§‘ç›®", "æŒ‡æ ‡åç§°")
# é•¿è¡¨ä¸­å¯èƒ½åŒ…å«æ—¥æœŸçš„åˆ—
DATE_COL_CANDIDATES = ("æ—¥æœŸ", "æŠ¥å‘ŠæœŸ", "æˆªæ­¢æ—¥æœŸ", "date")


@dataclass(frozen=True)
class MetricSpec:
    key: str
    # æ­£åˆ™è¡¨è¾¾å¼å…ƒç»„ï¼Œç”¨äºåŒ¹é…è¡Œåï¼ˆå®½è¡¨ï¼‰æˆ–åˆ—åï¼ˆé•¿è¡¨ï¼‰
    patterns: Tuple[str, ...]


# é’ˆå¯¹ ak.stock_financial_abstract è¿”å›çš„æŒ‡æ ‡åç§°è¿›è¡Œé€‚é…
# è¯¥æ¥å£è¿”å›çš„æ•°æ®é€šå¸¸åŒ…å«ï¼šå‡€èµ„äº§æ”¶ç›Šç‡ã€æ€»èµ„äº§å‡€åˆ©ç‡ã€é”€å”®å‡€åˆ©ç‡ã€ä»¥åŠå„ç±»å¢é•¿ç‡
METRICS: Tuple[MetricSpec, ...] = (
    MetricSpec("roe", (r"å‡€èµ„äº§æ”¶ç›Šç‡", r"åŠ æƒ.*å‡€èµ„äº§æ”¶ç›Šç‡")),
    MetricSpec("rev_growth", (r"è¥ä¸š(æ€»)?æ”¶å…¥(åŒæ¯”)?å¢é•¿ç‡", r"ä¸»è¥ä¸šåŠ¡æ”¶å…¥å¢é•¿ç‡")),
    MetricSpec("profit_growth", (r"(å½’æ¯)?å‡€åˆ©æ¶¦(åŒæ¯”)?å¢é•¿ç‡",)),
    MetricSpec("debt_ratio", (r"èµ„äº§è´Ÿå€ºç‡",)),
    MetricSpec("eps", (r"åŸºæœ¬æ¯è‚¡æ”¶ç›Š", r"æ¯è‚¡æ”¶ç›Š")),
    MetricSpec("bps", (r"æ¯è‚¡å‡€èµ„äº§",)),
)

# è¾“å‡ºæ–‡ä»¶çš„æ ‡å‡†åˆ—åº
OUT_COLS = ("date",) + tuple(m.key for m in METRICS) + ("pub_date",)


def _coerce_dt(x) -> pd.Series:
    """å¼ºåˆ¶è½¬æ¢ä¸º datetimeï¼Œæ— æ•ˆå€¼è®¾ä¸º NaT"""
    return pd.to_datetime(x, errors="coerce")


def _to_float32(s: pd.Series) -> pd.Series:
    """æ¸…æ´—åŒ…å« '%' æˆ–éæ•°å€¼å­—ç¬¦çš„æ•°æ®ï¼Œå¹¶è½¬æ¢ä¸º float32"""
    if s.dtype == object:
        # å»é™¤ç™¾åˆ†å·ã€é€—å·ï¼Œå¤„ç† 'nan', '--' ç­‰æƒ…å†µ
        s = s.astype(str).str.replace("%", "", regex=False).str.replace(",", "", regex=False)
        # akshare æœ‰æ—¶è¿”å› 'None' å­—ç¬¦ä¸²
        s = s.replace({"None": np.nan, "--": np.nan, "nan": np.nan})
    return pd.to_numeric(s, errors="coerce").astype(np.float32)


def _estimate_pub_date(series_dates: pd.Series) -> pd.Series:
    """
    æ ¹æ®æŠ¥å‘ŠæœŸ(report_date)ä¼°ç®—æ³•å®šæŠ«éœ²æˆªæ­¢æ—¥(pub_date)ã€‚
    è¿™æ˜¯ä¸ºäº†é¿å… Look-ahead Bias çš„ä¿å®ˆç­–ç•¥ã€‚

    Aè‚¡æ³•å®šæŠ«éœ²æˆªæ­¢æ—¥è§„åˆ™ï¼š
    1å­£æŠ¥(03-31) -> 04-30
    ä¸­æŠ¥(06-30)  -> 08-31
    3å­£æŠ¥(09-30) -> 10-31
    å¹´æŠ¥(12-31)  -> æ¬¡å¹´ 04-30
    """

    def _map_one(d):
        if pd.isna(d):
            return pd.NaT
        try:
            m = d.month
            y = d.year
            if m == 3:
                return pd.Timestamp(year=y, month=4, day=30)
            elif m == 6:
                return pd.Timestamp(year=y, month=8, day=31)
            elif m == 9:
                return pd.Timestamp(year=y, month=10, day=31)
            elif m == 12:
                return pd.Timestamp(year=y + 1, month=4, day=30)
            else:
                # éå¸¸è§„æŠ¥å‘ŠæœŸï¼Œé»˜è®¤å»¶å 60 å¤©
                return d + pd.Timedelta(days=60)
        except Exception:
            return pd.NaT

    # ä½¿ç”¨ apply å¯¹ Series è¿›è¡Œé€ä¸ªå¤„ç†
    return series_dates.apply(_map_one)


def _empty_frame() -> pd.DataFrame:
    """è¿”å›æ ‡å‡†çš„ç©º DataFrame"""
    return pd.DataFrame(columns=list(OUT_COLS))


def _detect_wide_date_cols(cols: Iterable[str]) -> List[str]:
    """æå–å½¢å¦‚ 20231231 çš„æ—¥æœŸåˆ—"""
    return [c for c in cols if DATE_COL_RE.match(str(c))]


def _detect_indicator_col(df: pd.DataFrame) -> Optional[str]:
    """å¯»æ‰¾å­˜æ”¾æŒ‡æ ‡åç§°çš„åˆ—"""
    for c in IND_COL_CANDIDATES:
        if c in df.columns:
            return c
    return None


def _detect_date_col_long(df: pd.DataFrame) -> Optional[str]:
    """å¯»æ‰¾å­˜æ”¾æ—¥æœŸçš„åˆ—ï¼ˆç”¨äºé•¿è¡¨æ¨¡å¼ï¼‰"""
    for c in DATE_COL_CANDIDATES:
        if c in df.columns:
            return c
    return None


def _wide_to_metrics(df: pd.DataFrame) -> pd.DataFrame:
    """
    å¤„ç† ak.stock_financial_abstract è¿”å›çš„å®½è¡¨æ•°æ®
    ç»“æ„ç¤ºä¾‹:
      é€‰é¡¹ | æŒ‡æ ‡ | 20250930 | 20250630 ...
    """
    date_cols = _detect_wide_date_cols(df.columns)
    ind_col = _detect_indicator_col(df)

    if not date_cols or not ind_col:
        return _empty_frame()

    # åªä¿ç•™æŒ‡æ ‡åˆ—å’Œæ—¥æœŸåˆ—ï¼Œä¸¢å¼ƒ 'é€‰é¡¹' åˆ—ä»¥é˜²æ­¢å¹²æ‰°
    # ä½¿ç”¨ copy é¿å… SettingWithCopyWarning
    m = df[[ind_col] + date_cols].copy()

    # æ¸…æ´—æŒ‡æ ‡åç§°ï¼šå»ç©ºæ ¼ã€è½¬å­—ç¬¦ä¸²
    m[ind_col] = m[ind_col].astype(str).str.strip()

    # å®½è¡¨è½¬é•¿è¡¨ (Melt)
    # var_name="date_str", value_name="raw_value"
    long = m.melt(id_vars=[ind_col], value_vars=date_cols, var_name="date_str", value_name="raw_value")

    # è½¬æ¢æ—¥æœŸ
    long["date"] = _coerce_dt(long["date_str"])
    long = long.dropna(subset=["date"])

    parts = []
    # éå†æ¯ä¸ªéœ€è¦çš„æŒ‡æ ‡ï¼Œä» long è¡¨ä¸­æå–å¯¹åº”çš„è¡Œ
    for spec in METRICS:
        # æ„å»ºæ­£åˆ™ï¼šå¿½ç•¥å¤§å°å†™
        pat = re.compile("|".join(spec.patterns), re.IGNORECASE)

        # ç­›é€‰ç¬¦åˆå½“å‰æŒ‡æ ‡æ­£åˆ™çš„è¡Œ
        mask = long[ind_col].str.contains(pat, na=False)
        sub = long[mask][["date", "raw_value"]].copy()

        if sub.empty:
            continue

        # å¦‚æœåŒ¹é…åˆ°å¤šè¡Œï¼ˆä¾‹å¦‚'æ¯è‚¡æ”¶ç›Š'åŒ¹é…äº†'åŸºæœ¬æ¯è‚¡æ”¶ç›Š'å’Œ'ç¨€é‡Šæ¯è‚¡æ”¶ç›Š'ï¼‰ï¼Œ
        # è¿™é‡Œçš„ç®€å•é€»è¾‘æ˜¯ä¿ç•™æœ€åå‡ºç°çš„ï¼ˆé€šå¸¸æ›´å…·ä½“ï¼‰æˆ–å–å‡å€¼ï¼Ÿ
        # åœ¨é‡‘èæŠ¥è¡¨ä¸­ï¼Œé€šå¸¸å–ç¬¬ä¸€ä¸ªåŒ¹é…é¡¹æˆ–æŒ‰ä¼˜å…ˆçº§åŒ¹é…ã€‚
        # è¿™é‡Œä¸ºäº†é˜²æ­¢ duplicate index errorï¼Œæˆ‘ä»¬åœ¨ pivot å‰å»é‡
        # æ¯”å¦‚ï¼šæŒ‰æ—¥æœŸå»é‡ï¼Œä¿ç•™ç¬¬ä¸€ä¸ªåŒ¹é…åˆ°çš„
        sub = sub.drop_duplicates(subset=["date"], keep="first")

        sub["metric"] = spec.key
        parts.append(sub)

    if not parts:
        return _empty_frame()

    # åˆå¹¶æ‰€æœ‰æŒ‡æ ‡ç‰‡æ®µ
    got = pd.concat(parts, ignore_index=True)
    got["value"] = _to_float32(got["raw_value"])

    # é€è§†è¡¨ï¼šIndex=Date, Columns=Metric
    out = (
        got.pivot_table(index="date", columns="metric", values="value", aggfunc="last")
        .reset_index()
        .sort_values("date")
    )

    # è¡¥å…¨ç¼ºå¤±çš„æŒ‡æ ‡åˆ—ï¼Œå¡«å…… NaN
    for spec in METRICS:
        if spec.key not in out.columns:
            out[spec.key] = np.nan

    # è¡¥å…… pub_date åˆ—
    # ä½¿ç”¨æ³•å®šæˆªæ­¢æ—¥æœŸè¿›è¡Œä¼°ç®—ï¼Œé˜²æ­¢å›æµ‹å‰è§†åå·®
    out["pub_date"] = _estimate_pub_date(out["date"])

    # æ•´ç†æœ€ç»ˆåˆ—åºå¹¶å»é‡
    out = out[list(OUT_COLS)].drop_duplicates("date", keep="last").reset_index(drop=True)
    return out


def normalize_fundamental_frame(raw: pd.DataFrame) -> pd.DataFrame:
    """
    ç»Ÿä¸€æ•°æ®æ¸…æ´—å…¥å£
    """
    if raw is None or raw.empty:
        return _empty_frame()

    df = raw.copy()
    # è§„èŒƒåŒ–åˆ—åï¼šè½¬å­—ç¬¦ä¸²å¹¶å»é™¤ç©ºæ ¼
    df.columns = [str(c).strip() for c in df.columns]

    # ç­–ç•¥ 1: å®½è¡¨æ¨¡å¼ (stock_financial_abstract å±äºæ­¤ç±»)
    # ç‰¹å¾ï¼šåˆ—åä¸­åŒ…å« YYYYMMDD æ ¼å¼çš„æ—¥æœŸ
    if _detect_wide_date_cols(df.columns):
        return _wide_to_metrics(df)

    # ç­–ç•¥ 2: é•¿è¡¨æ¨¡å¼ (å¤‡ç”¨ï¼Œéƒ¨åˆ†å†å²æ¥å£å¯èƒ½è¿”å›æ­¤æ ¼å¼)
    # ç‰¹å¾ï¼šæœ‰ä¸€åˆ—å« "date" æˆ– "æŠ¥å‘ŠæœŸ"
    if _detect_date_col_long(df) is not None:
        # è¿™é‡Œä¸ºäº†ä»£ç ç®€æ´ï¼Œæš‚æ—¶ç§»é™¤æœªä½¿ç”¨çš„ _long_to_metrics å®ç°ï¼Œ
        # å¦‚æœæœªæ¥éœ€è¦æ”¯æŒé•¿è¡¨æ¥å£ï¼Œå¯åœ¨æ­¤å¤„æ¢å¤é€»è¾‘ã€‚
        # ç›®å‰ stock_financial_abstract 100% è¿”å›å®½è¡¨ã€‚
        pass

    return _empty_frame()


class FundamentalPipeline:
    """
    ä¸‹è½½å¹¶ç¼“å­˜ä¸ªè‚¡è´¢åŠ¡æ‘˜è¦æ•°æ® (Quarterly Fundamentals)
    Output: {DATA_DIR}/fundamental/{code}.parquet
    Columns: date, roe, rev_growth, profit_growth, debt_ratio, eps, bps, pub_date
    """
    SCHEMA_VER = 2  # Schema ç‰ˆæœ¬å‡çº§

    def __init__(self, cfg: DPConfig, ak_client: AkClient, logger):
        self.cfg = cfg
        self.ak_client = ak_client
        self.logger = logger
        os.makedirs(fundamental_dir(cfg), exist_ok=True)

    def _should_skip(self, path: str) -> bool:
        """æ£€æŸ¥ç¼“å­˜æ˜¯å¦æœ‰æ•ˆ"""
        days = int(self.cfg.get("FUND_TTL_DAYS", 5) or 5)
        ttl = max(1, days) * 24 * 3600
        # æ£€æŸ¥æ–‡ä»¶å­˜åœ¨ä¸”ä¸è¿‡æœŸï¼Œä¸”å¤§å°æ­£å¸¸
        return os.path.exists(path) and os.path.getsize(path) > 512 and (time.time() - os.path.getmtime(path)) < ttl

    def _download_one(self, code: str) -> Tuple[str, bool, str, int]:
        """ä¸‹è½½å•ä¸ªè‚¡ç¥¨çš„è´¢åŠ¡æ•°æ®"""
        c = normalize_code(code)
        if not c:
            return str(code), True, "BadCode", 0

        path = fundamental_path(self.cfg, c)
        if self._should_skip(path):
            return c, True, "Skipped", -1

        # æ³¨æ„: stock_financial_abstract æ¥å£é€šå¸¸ä¸éœ€è¦ start_yearï¼Œå®ƒè¿”å›æ‰€æœ‰æ‘˜è¦æ•°æ®
        try:
            # ä½¿ç”¨æ–°æ¥å£: ak.stock_financial_abstract
            raw = self.ak_client.call(ak.stock_financial_abstract, symbol=c)
            out = normalize_fundamental_frame(raw)

            if out.empty:
                return c, True, "Empty", 0

            atomic_save_parquet(
                out,
                path,
                index=False,
                compression=str(self.cfg.get("PARQUET_COMPRESSION", "zstd") or "zstd"),
            )
            return c, True, "Success", int(len(out))
        except Exception as e:
            # æ•è·å¼‚å¸¸ï¼Œé˜²æ­¢å•åªè‚¡ç¥¨å¤±è´¥å½±å“æ•´ä½“
            return c, False, f"Failed({type(e).__name__})", 0

    def download(self, codes) -> None:
        """æ‰¹é‡ä¸‹è½½å…¥å£"""
        if not bool(self.cfg.get("SYNC_FUNDAMENTAL", False)):
            self.logger.info("ğŸŸ¦ [Fundamental] SYNC_FUNDAMENTAL=False; skip.")
            return

        codes = [normalize_code(c) for c in codes]
        codes = [c for c in codes if c]
        if not codes:
            self.logger.warning("ğŸŸ¦ [Fundamental] empty codes; skip.")
            return

        workers = int(self.cfg.get("FIN_WORKERS", 8) or 8)
        # é™åˆ¶æœ€å¤§æ’é˜Ÿä»»åŠ¡æ•°ï¼Œé˜²æ­¢å†…å­˜çˆ†ç‚¸
        max_inflight = int(self.cfg.get("FIN_MAX_INFLIGHT", workers * 4) or (workers * 4))

        self.logger.info(f"ğŸŸ¦ [Fundamental] syncing {len(codes)} codes ... workers={workers} inflight={max_inflight}")

        q = deque(codes)
        stats = {"ok": 0, "bad": 0, "empty": 0, "skipped": 0}

        def submit_more(ex, inflight_dict):
            """å¡«å……ä»»åŠ¡é˜Ÿåˆ—"""
            while q and len(inflight_dict) < max_inflight:
                c = q.popleft()
                inflight_dict[ex.submit(self._download_one, c)] = c

        with concurrent.futures.ThreadPoolExecutor(max_workers=workers) as ex:
            inflight = {}
            submit_more(ex, inflight)

            with tqdm(total=len(codes), dynamic_ncols=True, desc="Fundamental", unit="code") as pbar:
                while inflight:
                    # ç­‰å¾…ä»»æ„ä¸€ä¸ªä»»åŠ¡å®Œæˆ
                    done, _ = concurrent.futures.wait(
                        inflight.keys(),
                        return_when=concurrent.futures.FIRST_COMPLETED,
                    )
                    for fut in done:
                        _ = inflight.pop(fut, None)
                        try:
                            code, success, msg, rows = fut.result()
                            if success:
                                if msg == "Skipped":
                                    stats["skipped"] += 1
                                elif msg == "Empty":
                                    stats["empty"] += 1
                                else:
                                    stats["ok"] += 1
                            else:
                                stats["bad"] += 1
                                # å¯ä»¥åœ¨è¿™é‡Œè®°å½•å…·ä½“é”™è¯¯æ—¥å¿—: self.logger.debug(f"{code} failed: {msg}")
                        except Exception as e:
                            stats["bad"] += 1
                            self.logger.error(f"Unexpected error in future: {e}")

                        pbar.update(1)
                        pbar.set_postfix(**stats, last=code if 'code' in locals() else "")

                    submit_more(ex, inflight)

        self.logger.info(f"ğŸŸ¦ [Fundamental] done. {stats}")