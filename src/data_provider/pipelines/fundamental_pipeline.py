
#åé¢çš„ä»£ç è¿è¡Œå¤ªæ…¢ å‰é¢çš„estimateè¦å¥½ç‚¹
# from __future__ import annotations
#
# import concurrent.futures
# import os
# import re
# import time
# from collections import deque
# from dataclasses import dataclass
# from typing import Iterable, List, Optional, Tuple
#
# import numpy as np
# import pandas as pd
# import akshare as ak
# from tqdm.auto import tqdm
#
# from ..clients.ak_client import AkClient
# from ..core.config import DPConfig
# from ..utils.code import normalize_code
# from ..utils.io import atomic_save_parquet
# from ..stores.paths import fundamental_dir, fundamental_path
#
# # åŒ¹é… YYYYMMDD æ ¼å¼çš„åˆ—å
# DATE_COL_RE = re.compile(r"^\d{8}$")
# # å®½è¡¨ä¸­å¯èƒ½åŒ…å«æŒ‡æ ‡åç§°çš„åˆ—
# IND_COL_CANDIDATES = ("æŒ‡æ ‡", "é¡¹ç›®", "ç§‘ç›®", "æŒ‡æ ‡åç§°")
# # é•¿è¡¨ä¸­å¯èƒ½åŒ…å«æ—¥æœŸçš„åˆ—
# DATE_COL_CANDIDATES = ("æ—¥æœŸ", "æŠ¥å‘ŠæœŸ", "æˆªæ­¢æ—¥æœŸ", "date")
#
#
# @dataclass(frozen=True)
# class MetricSpec:
#     key: str
#     # æ­£åˆ™è¡¨è¾¾å¼å…ƒç»„ï¼Œç”¨äºåŒ¹é…è¡Œåï¼ˆå®½è¡¨ï¼‰æˆ–åˆ—åï¼ˆé•¿è¡¨ï¼‰
#     patterns: Tuple[str, ...]
#
#
# # é’ˆå¯¹ ak.stock_financial_abstract è¿”å›çš„æŒ‡æ ‡åç§°è¿›è¡Œé€‚é…
# # è¯¥æ¥å£è¿”å›çš„æ•°æ®é€šå¸¸åŒ…å«ï¼šå‡€èµ„äº§æ”¶ç›Šç‡ã€æ€»èµ„äº§å‡€åˆ©ç‡ã€é”€å”®å‡€åˆ©ç‡ã€ä»¥åŠå„ç±»å¢é•¿ç‡
# METRICS: Tuple[MetricSpec, ...] = (
#     MetricSpec("roe", (r"å‡€èµ„äº§æ”¶ç›Šç‡", r"åŠ æƒ.*å‡€èµ„äº§æ”¶ç›Šç‡")),
#     MetricSpec("rev_growth", (r"è¥ä¸š(æ€»)?æ”¶å…¥(åŒæ¯”)?å¢é•¿ç‡", r"ä¸»è¥ä¸šåŠ¡æ”¶å…¥å¢é•¿ç‡")),
#     MetricSpec("profit_growth", (r"(å½’æ¯)?å‡€åˆ©æ¶¦(åŒæ¯”)?å¢é•¿ç‡",)),
#     MetricSpec("debt_ratio", (r"èµ„äº§è´Ÿå€ºç‡",)),
#     MetricSpec("eps", (r"åŸºæœ¬æ¯è‚¡æ”¶ç›Š", r"æ¯è‚¡æ”¶ç›Š")),
#     MetricSpec("bps", (r"æ¯è‚¡å‡€èµ„äº§",)),
# )
#
# # è¾“å‡ºæ–‡ä»¶çš„æ ‡å‡†åˆ—åº
# OUT_COLS = ("date",) + tuple(m.key for m in METRICS) + ("pub_date",)
#
#
# def _coerce_dt(x) -> pd.Series:
#     """å¼ºåˆ¶è½¬æ¢ä¸º datetimeï¼Œæ— æ•ˆå€¼è®¾ä¸º NaT"""
#     return pd.to_datetime(x, errors="coerce")
#
#
# def _to_float32(s: pd.Series) -> pd.Series:
#     """æ¸…æ´—åŒ…å« '%' æˆ–éæ•°å€¼å­—ç¬¦çš„æ•°æ®ï¼Œå¹¶è½¬æ¢ä¸º float32"""
#     if s.dtype == object:
#         # å»é™¤ç™¾åˆ†å·ã€é€—å·ï¼Œå¤„ç† 'nan', '--' ç­‰æƒ…å†µ
#         s = s.astype(str).str.replace("%", "", regex=False).str.replace(",", "", regex=False)
#         # akshare æœ‰æ—¶è¿”å› 'None' å­—ç¬¦ä¸²
#         s = s.replace({"None": np.nan, "--": np.nan, "nan": np.nan})
#     return pd.to_numeric(s, errors="coerce").astype(np.float32)
#
#
# def _estimate_pub_date(series_dates: pd.Series) -> pd.Series:
#     """
#     æ ¹æ®æŠ¥å‘ŠæœŸ(report_date)ä¼°ç®—æ³•å®šæŠ«éœ²æˆªæ­¢æ—¥(pub_date)ã€‚
#     è¿™æ˜¯ä¸ºäº†é¿å… Look-ahead Bias çš„ä¿å®ˆç­–ç•¥ã€‚
#
#     Aè‚¡æ³•å®šæŠ«éœ²æˆªæ­¢æ—¥è§„åˆ™ï¼š
#     1å­£æŠ¥(03-31) -> 04-30
#     ä¸­æŠ¥(06-30)  -> 08-31
#     3å­£æŠ¥(09-30) -> 10-31
#     å¹´æŠ¥(12-31)  -> æ¬¡å¹´ 04-30
#     """
#
#     def _map_one(d):
#         if pd.isna(d):
#             return pd.NaT
#         try:
#             m = d.month
#             y = d.year
#             if m == 3:
#                 return pd.Timestamp(year=y, month=4, day=30)
#             elif m == 6:
#                 return pd.Timestamp(year=y, month=8, day=31)
#             elif m == 9:
#                 return pd.Timestamp(year=y, month=10, day=31)
#             elif m == 12:
#                 return pd.Timestamp(year=y + 1, month=4, day=30)
#             else:
#                 # éå¸¸è§„æŠ¥å‘ŠæœŸï¼Œé»˜è®¤å»¶å 60 å¤©
#                 return d + pd.Timedelta(days=60)
#         except Exception:
#             return pd.NaT
#
#     # ä½¿ç”¨ apply å¯¹ Series è¿›è¡Œé€ä¸ªå¤„ç†
#     return series_dates.apply(_map_one)
#
#
# def _empty_frame() -> pd.DataFrame:
#     """è¿”å›æ ‡å‡†çš„ç©º DataFrame"""
#     return pd.DataFrame(columns=list(OUT_COLS))
#
#
# def _detect_wide_date_cols(cols: Iterable[str]) -> List[str]:
#     """æå–å½¢å¦‚ 20231231 çš„æ—¥æœŸåˆ—"""
#     return [c for c in cols if DATE_COL_RE.match(str(c))]
#
#
# def _detect_indicator_col(df: pd.DataFrame) -> Optional[str]:
#     """å¯»æ‰¾å­˜æ”¾æŒ‡æ ‡åç§°çš„åˆ—"""
#     for c in IND_COL_CANDIDATES:
#         if c in df.columns:
#             return c
#     return None
#
#
# def _detect_date_col_long(df: pd.DataFrame) -> Optional[str]:
#     """å¯»æ‰¾å­˜æ”¾æ—¥æœŸçš„åˆ—ï¼ˆç”¨äºé•¿è¡¨æ¨¡å¼ï¼‰"""
#     for c in DATE_COL_CANDIDATES:
#         if c in df.columns:
#             return c
#     return None
#
#
# def _wide_to_metrics(df: pd.DataFrame) -> pd.DataFrame:
#     """
#     å¤„ç† ak.stock_financial_abstract è¿”å›çš„å®½è¡¨æ•°æ®
#     ç»“æ„ç¤ºä¾‹:
#       é€‰é¡¹ | æŒ‡æ ‡ | 20250930 | 20250630 ...
#     """
#     date_cols = _detect_wide_date_cols(df.columns)
#     ind_col = _detect_indicator_col(df)
#
#     if not date_cols or not ind_col:
#         return _empty_frame()
#
#     # åªä¿ç•™æŒ‡æ ‡åˆ—å’Œæ—¥æœŸåˆ—ï¼Œä¸¢å¼ƒ 'é€‰é¡¹' åˆ—ä»¥é˜²æ­¢å¹²æ‰°
#     # ä½¿ç”¨ copy é¿å… SettingWithCopyWarning
#     m = df[[ind_col] + date_cols].copy()
#
#     # æ¸…æ´—æŒ‡æ ‡åç§°ï¼šå»ç©ºæ ¼ã€è½¬å­—ç¬¦ä¸²
#     m[ind_col] = m[ind_col].astype(str).str.strip()
#
#     # å®½è¡¨è½¬é•¿è¡¨ (Melt)
#     # var_name="date_str", value_name="raw_value"
#     long = m.melt(id_vars=[ind_col], value_vars=date_cols, var_name="date_str", value_name="raw_value")
#
#     # è½¬æ¢æ—¥æœŸ
#     long["date"] = _coerce_dt(long["date_str"])
#     long = long.dropna(subset=["date"])
#
#     parts = []
#     # éå†æ¯ä¸ªéœ€è¦çš„æŒ‡æ ‡ï¼Œä» long è¡¨ä¸­æå–å¯¹åº”çš„è¡Œ
#     for spec in METRICS:
#         # æ„å»ºæ­£åˆ™ï¼šå¿½ç•¥å¤§å°å†™
#         pat = re.compile("|".join(spec.patterns), re.IGNORECASE)
#
#         # ç­›é€‰ç¬¦åˆå½“å‰æŒ‡æ ‡æ­£åˆ™çš„è¡Œ
#         mask = long[ind_col].str.contains(pat, na=False)
#         sub = long[mask][["date", "raw_value"]].copy()
#
#         if sub.empty:
#             continue
#
#         # å¦‚æœåŒ¹é…åˆ°å¤šè¡Œï¼ˆä¾‹å¦‚'æ¯è‚¡æ”¶ç›Š'åŒ¹é…äº†'åŸºæœ¬æ¯è‚¡æ”¶ç›Š'å’Œ'ç¨€é‡Šæ¯è‚¡æ”¶ç›Š'ï¼‰ï¼Œ
#         # è¿™é‡Œçš„ç®€å•é€»è¾‘æ˜¯ä¿ç•™æœ€åå‡ºç°çš„ï¼ˆé€šå¸¸æ›´å…·ä½“ï¼‰æˆ–å–å‡å€¼ï¼Ÿ
#         # åœ¨é‡‘èæŠ¥è¡¨ä¸­ï¼Œé€šå¸¸å–ç¬¬ä¸€ä¸ªåŒ¹é…é¡¹æˆ–æŒ‰ä¼˜å…ˆçº§åŒ¹é…ã€‚
#         # è¿™é‡Œä¸ºäº†é˜²æ­¢ duplicate index errorï¼Œæˆ‘ä»¬åœ¨ pivot å‰å»é‡
#         # æ¯”å¦‚ï¼šæŒ‰æ—¥æœŸå»é‡ï¼Œä¿ç•™ç¬¬ä¸€ä¸ªåŒ¹é…åˆ°çš„
#         sub = sub.drop_duplicates(subset=["date"], keep="first")
#
#         sub["metric"] = spec.key
#         parts.append(sub)
#
#     if not parts:
#         return _empty_frame()
#
#     # åˆå¹¶æ‰€æœ‰æŒ‡æ ‡ç‰‡æ®µ
#     got = pd.concat(parts, ignore_index=True)
#     got["value"] = _to_float32(got["raw_value"])
#
#     # é€è§†è¡¨ï¼šIndex=Date, Columns=Metric
#     out = (
#         got.pivot_table(index="date", columns="metric", values="value", aggfunc="last")
#         .reset_index()
#         .sort_values("date")
#     )
#
#     # è¡¥å…¨ç¼ºå¤±çš„æŒ‡æ ‡åˆ—ï¼Œå¡«å…… NaN
#     for spec in METRICS:
#         if spec.key not in out.columns:
#             out[spec.key] = np.nan
#
#     # è¡¥å…… pub_date åˆ—
#     # ä½¿ç”¨æ³•å®šæˆªæ­¢æ—¥æœŸè¿›è¡Œä¼°ç®—ï¼Œé˜²æ­¢å›æµ‹å‰è§†åå·®
#     out["pub_date"] = _estimate_pub_date(out["date"])
#
#     # æ•´ç†æœ€ç»ˆåˆ—åºå¹¶å»é‡
#     out = out[list(OUT_COLS)].drop_duplicates("date", keep="last").reset_index(drop=True)
#     return out
#
#
# def normalize_fundamental_frame(raw: pd.DataFrame) -> pd.DataFrame:
#     """
#     ç»Ÿä¸€æ•°æ®æ¸…æ´—å…¥å£
#     """
#     if raw is None or raw.empty:
#         return _empty_frame()
#
#     df = raw.copy()
#     # è§„èŒƒåŒ–åˆ—åï¼šè½¬å­—ç¬¦ä¸²å¹¶å»é™¤ç©ºæ ¼
#     df.columns = [str(c).strip() for c in df.columns]
#
#     # ç­–ç•¥ 1: å®½è¡¨æ¨¡å¼ (stock_financial_abstract å±äºæ­¤ç±»)
#     # ç‰¹å¾ï¼šåˆ—åä¸­åŒ…å« YYYYMMDD æ ¼å¼çš„æ—¥æœŸ
#     if _detect_wide_date_cols(df.columns):
#         return _wide_to_metrics(df)
#
#     # ç­–ç•¥ 2: é•¿è¡¨æ¨¡å¼ (å¤‡ç”¨ï¼Œéƒ¨åˆ†å†å²æ¥å£å¯èƒ½è¿”å›æ­¤æ ¼å¼)
#     # ç‰¹å¾ï¼šæœ‰ä¸€åˆ—å« "date" æˆ– "æŠ¥å‘ŠæœŸ"
#     if _detect_date_col_long(df) is not None:
#         # è¿™é‡Œä¸ºäº†ä»£ç ç®€æ´ï¼Œæš‚æ—¶ç§»é™¤æœªä½¿ç”¨çš„ _long_to_metrics å®ç°ï¼Œ
#         # å¦‚æœæœªæ¥éœ€è¦æ”¯æŒé•¿è¡¨æ¥å£ï¼Œå¯åœ¨æ­¤å¤„æ¢å¤é€»è¾‘ã€‚
#         # ç›®å‰ stock_financial_abstract 100% è¿”å›å®½è¡¨ã€‚
#         pass
#
#     return _empty_frame()
#
#
# class FundamentalPipeline:
#     """
#     ä¸‹è½½å¹¶ç¼“å­˜ä¸ªè‚¡è´¢åŠ¡æ‘˜è¦æ•°æ® (Quarterly Fundamentals)
#     Output: {DATA_DIR}/fundamental/{code}.parquet
#     Columns: date, roe, rev_growth, profit_growth, debt_ratio, eps, bps, pub_date
#     """
#     SCHEMA_VER = 2  # Schema ç‰ˆæœ¬å‡çº§
#
#     def __init__(self, cfg: DPConfig, ak_client: AkClient, logger):
#         self.cfg = cfg
#         self.ak_client = ak_client
#         self.logger = logger
#         os.makedirs(fundamental_dir(cfg), exist_ok=True)
#
#     def _should_skip(self, path: str) -> bool:
#         """æ£€æŸ¥ç¼“å­˜æ˜¯å¦æœ‰æ•ˆ"""
#         days = int(self.cfg.get("FUND_TTL_DAYS", 5) or 5)
#         ttl = max(1, days) * 24 * 3600
#         # æ£€æŸ¥æ–‡ä»¶å­˜åœ¨ä¸”ä¸è¿‡æœŸï¼Œä¸”å¤§å°æ­£å¸¸
#         return os.path.exists(path) and os.path.getsize(path) > 512 and (time.time() - os.path.getmtime(path)) < ttl
#
#     def _download_one(self, code: str) -> Tuple[str, bool, str, int]:
#         """ä¸‹è½½å•ä¸ªè‚¡ç¥¨çš„è´¢åŠ¡æ•°æ®"""
#         c = normalize_code(code)
#         if not c:
#             return str(code), True, "BadCode", 0
#
#         path = fundamental_path(self.cfg, c)
#         if self._should_skip(path):
#             return c, True, "Skipped", -1
#
#         # æ³¨æ„: stock_financial_abstract æ¥å£é€šå¸¸ä¸éœ€è¦ start_yearï¼Œå®ƒè¿”å›æ‰€æœ‰æ‘˜è¦æ•°æ®
#         try:
#             # ä½¿ç”¨æ–°æ¥å£: ak.stock_financial_abstract
#             raw = self.ak_client.call(ak.stock_financial_abstract, symbol=c)
#             out = normalize_fundamental_frame(raw)
#
#             if out.empty:
#                 return c, True, "Empty", 0
#
#             atomic_save_parquet(
#                 out,
#                 path,
#                 index=False,
#                 compression=str(self.cfg.get("PARQUET_COMPRESSION", "zstd") or "zstd"),
#             )
#             return c, True, "Success", int(len(out))
#         except Exception as e:
#             # æ•è·å¼‚å¸¸ï¼Œé˜²æ­¢å•åªè‚¡ç¥¨å¤±è´¥å½±å“æ•´ä½“
#             return c, False, f"Failed({type(e).__name__})", 0
#
#     def download(self, codes) -> None:
#         """æ‰¹é‡ä¸‹è½½å…¥å£"""
#         if not bool(self.cfg.get("SYNC_FUNDAMENTAL", False)):
#             self.logger.info("ğŸŸ¦ [Fundamental] SYNC_FUNDAMENTAL=False; skip.")
#             return
#
#         codes = [normalize_code(c) for c in codes]
#         codes = [c for c in codes if c]
#         if not codes:
#             self.logger.warning("ğŸŸ¦ [Fundamental] empty codes; skip.")
#             return
#
#         workers = int(self.cfg.get("FIN_WORKERS", 8) or 8)
#         # é™åˆ¶æœ€å¤§æ’é˜Ÿä»»åŠ¡æ•°ï¼Œé˜²æ­¢å†…å­˜çˆ†ç‚¸
#         max_inflight = int(self.cfg.get("FIN_MAX_INFLIGHT", workers * 4) or (workers * 4))
#
#         self.logger.info(f"ğŸŸ¦ [Fundamental] syncing {len(codes)} codes ... workers={workers} inflight={max_inflight}")
#
#         q = deque(codes)
#         stats = {"ok": 0, "bad": 0, "empty": 0, "skipped": 0}
#
#         def submit_more(ex, inflight_dict):
#             """å¡«å……ä»»åŠ¡é˜Ÿåˆ—"""
#             while q and len(inflight_dict) < max_inflight:
#                 c = q.popleft()
#                 inflight_dict[ex.submit(self._download_one, c)] = c
#
#         with concurrent.futures.ThreadPoolExecutor(max_workers=workers) as ex:
#             inflight = {}
#             submit_more(ex, inflight)
#
#             with tqdm(total=len(codes), dynamic_ncols=True, desc="Fundamental", unit="code") as pbar:
#                 while inflight:
#                     # ç­‰å¾…ä»»æ„ä¸€ä¸ªä»»åŠ¡å®Œæˆ
#                     done, _ = concurrent.futures.wait(
#                         inflight.keys(),
#                         return_when=concurrent.futures.FIRST_COMPLETED,
#                     )
#                     for fut in done:
#                         _ = inflight.pop(fut, None)
#                         try:
#                             code, success, msg, rows = fut.result()
#                             if success:
#                                 if msg == "Skipped":
#                                     stats["skipped"] += 1
#                                 elif msg == "Empty":
#                                     stats["empty"] += 1
#                                 else:
#                                     stats["ok"] += 1
#                             else:
#                                 stats["bad"] += 1
#                                 # å¯ä»¥åœ¨è¿™é‡Œè®°å½•å…·ä½“é”™è¯¯æ—¥å¿—: self.logger.debug(f"{code} failed: {msg}")
#                         except Exception as e:
#                             stats["bad"] += 1
#                             self.logger.error(f"Unexpected error in future: {e}")
#
#                         pbar.update(1)
#                         pbar.set_postfix(**stats, last=code if 'code' in locals() else "")
#
#                     submit_more(ex, inflight)
#
#         self.logger.info(f"ğŸŸ¦ [Fundamental] done. {stats}")
#


from __future__ import annotations

import concurrent.futures
import os
import re
import time
from collections import deque
from dataclasses import dataclass
from typing import Dict, Iterable, List, Optional, Tuple

import akshare as ak
import numpy as np
import pandas as pd
from tqdm.auto import tqdm

from ..clients.ak_client import AkClient
from ..core.config import DPConfig
from ..stores.paths import fundamental_dir, fundamental_path
from ..utils.code import normalize_code
from ..utils.io import atomic_save_parquet


DATE_COL_RE = re.compile(r"^\d{8}$")  # e.g. 20250930

IND_COL_CANDIDATES = ("æŒ‡æ ‡", "é¡¹ç›®", "ç§‘ç›®", "æŒ‡æ ‡åç§°")
DATE_COL_CANDIDATES = ("æ—¥æœŸ", "æŠ¥å‘ŠæœŸ", "æˆªæ­¢æ—¥æœŸ", "æˆªæ­¢æ—¥æœŸ", "date", "report_date")


# ----------------------------
# 1) Stable schema
# ----------------------------
@dataclass(frozen=True)
class MetricSpec:
    key: str
    patterns: Tuple[str, ...]


# æ ¸å¿ƒè¾“å‡ºå­—æ®µï¼ˆç¨³å®š schemaï¼‰
METRICS: Tuple[MetricSpec, ...] = (
    MetricSpec("roe", (r"å‡€èµ„äº§æ”¶ç›Šç‡", r"\bROE\b", r"åŠ æƒ.*å‡€èµ„äº§æ”¶ç›Šç‡", r"å‡€èµ„äº§æ”¶ç›Šç‡.*\(ROE\)")),
    MetricSpec("debt_ratio", (r"èµ„äº§è´Ÿå€ºç‡", r"è´Ÿå€º.*èµ„äº§", r"Debt\s*Ratio")),
    MetricSpec("eps", (r"æ¯è‚¡æ”¶ç›Š", r"åŸºæœ¬æ¯è‚¡æ”¶ç›Š", r"\bEPS\b")),
    MetricSpec("bps", (r"æ¯è‚¡å‡€èµ„äº§", r"\bBPS\b")),
    # growthï¼šä¼˜å…ˆåŒ¹é…åŒæ¯”/å¢é•¿ç‡ï¼›åŒ¹é…ä¸åˆ°åˆ™ç”¨æ”¶å…¥/åˆ©æ¶¦è‡ªå·±ç®— YoY
    MetricSpec("rev_growth", (r"(è¥|ä¸»).*æ”¶å…¥.*(åŒæ¯”|å¢é•¿ç‡|å¢é•¿)", r"æ”¶å…¥.*åŒæ¯”", r"è¥ä¸šæ€»æ”¶å…¥.*åŒæ¯”", r"è¥ä¸šæ”¶å…¥.*åŒæ¯”")),
    MetricSpec("profit_growth", (r"(å½’æ¯)?å‡€åˆ©æ¶¦.*(åŒæ¯”|å¢é•¿ç‡|å¢é•¿)", r"åˆ©æ¶¦.*åŒæ¯”", r"å½’æ¯å‡€åˆ©æ¶¦.*åŒæ¯”", r"å‡€åˆ©æ¶¦.*åŒæ¯”")),
)

# ç”¨æ¥è®¡ç®— YoY çš„è¾…åŠ©è¡Œï¼ˆå½“å¢é•¿ç‡æ‰¾ä¸åˆ°æ—¶ï¼‰
AUX_ABS = {
    "revenue": (r"è¥ä¸šæ€»æ”¶å…¥", r"è¥ä¸šæ”¶å…¥", r"ä¸»è¥ä¸šåŠ¡æ”¶å…¥"),
    "profit": (r"å½’æ¯å‡€åˆ©æ¶¦", r"å‡€åˆ©æ¶¦"),
}

OUT_COLS = ("date",) + tuple(m.key for m in METRICS) + ("pub_date",)


# ----------------------------
# 2) utilities
# ----------------------------
def _coerce_dt(x) -> pd.Series:
    return pd.to_datetime(x, errors="coerce")


def _clean_numeric_series(s: pd.Series) -> pd.Series:
    """
    Robust numeric cleanup:
      - strip % / å…ƒ / commas
      - handle '--'
    """
    if s is None:
        return pd.Series(dtype=np.float32)
    if not isinstance(s, pd.Series):
        s = pd.Series(s)

    if s.dtype == object:
        x = (
            s.astype(str)
            .str.replace(",", "", regex=False)
            .str.replace("å…ƒ", "", regex=False)
            .str.replace("%", "", regex=False)
            .str.replace("--", "", regex=False)
            .str.strip()
        )
        s = x

    return pd.to_numeric(s, errors="coerce").astype(np.float32)


def _empty_frame() -> pd.DataFrame:
    return pd.DataFrame(columns=list(OUT_COLS))


def _detect_wide_date_cols(cols: Iterable[str]) -> List[str]:
    return [c for c in cols if DATE_COL_RE.match(str(c))]


def _detect_indicator_col(df: pd.DataFrame) -> Optional[str]:
    for c in IND_COL_CANDIDATES:
        if c in df.columns:
            return c
    return None


def _detect_date_col_long(df: pd.DataFrame) -> Optional[str]:
    for c in DATE_COL_CANDIDATES:
        if c in df.columns:
            return c
    # æœ‰äº›æ¥å£ä¼šå«â€œæˆªæ­¢æ—¥æœŸâ€
    if "æˆªæ­¢æ—¥æœŸ" in df.columns:
        return "æˆªæ­¢æ—¥æœŸ"
    return None


def _best_effort_call_financial_abstract(ak_client: AkClient, code: str) -> pd.DataFrame:
    """
    ak.stock_financial_abstract è€ç‰ˆæœ¬å‚æ•°åå« stockï¼Œæ–°ç‰ˆæœ¬æœ‰æ—¶å« symbolï¼›
    åšä¸€ä¸ªåŒå°è¯•ï¼Œé¿å…è¢« AkShare å‚æ•°æ”¹åŠ¨å¡æ­»ã€‚
    """
    return ak_client.call(ak.stock_financial_abstract, symbol=code)


def _melt_wide(df: pd.DataFrame, ind_col: str, date_cols: List[str]) -> pd.DataFrame:
    m = df[[ind_col] + date_cols].copy()
    m[ind_col] = m[ind_col].astype(str).str.strip()
    long = m.melt(id_vars=[ind_col], value_vars=date_cols, var_name="date", value_name="value")
    long["date"] = _coerce_dt(long["date"])
    long = long.dropna(subset=["date"])
    return long


def _extract_metric_from_long(long: pd.DataFrame, ind_col: str, patterns: Tuple[str, ...]) -> pd.Series:
    """
    Return Series indexed by date -> value (float32) for best match rows.
    If multiple rows match, we just keep the last per date after melt (stable-ish).
    """
    pat = re.compile("|".join(patterns), re.IGNORECASE)
    sub = long[long[ind_col].str.contains(pat, na=False, regex=True)][["date", "value"]].copy()
    if sub.empty:
        return pd.Series(dtype=np.float32)
    sub["value"] = _clean_numeric_series(sub["value"])
    sub = sub.dropna(subset=["date"]).sort_values("date")
    return sub.groupby("date")["value"].last()


def _yoy_growth_from_abs(abs_series: pd.Series) -> pd.Series:
    """
    YoY on endpoints (quarterly/half/annual cumulative):
      growth(date) = value(date) / value(date - 1y) - 1
    """
    if abs_series is None or abs_series.empty:
        return pd.Series(dtype=np.float32)
    s = abs_series.sort_index()
    idx = s.index
    prev_idx = (idx - pd.DateOffset(years=1)).to_list()
    prev = pd.Series([s.get(d, np.nan) for d in prev_idx], index=idx, dtype=np.float32)
    g = (s / prev) - 1.0
    return g.astype(np.float32)


# ----------------------------
# 3) normalize frames
# ----------------------------
def _wide_to_metrics(df: pd.DataFrame) -> pd.DataFrame:
    """
    Wide table (your real output):
      cols: é€‰é¡¹, æŒ‡æ ‡, 20250930, 20250630, ...
      rows: æŒ‡æ ‡åç§°
    """
    date_cols = _detect_wide_date_cols(df.columns)
    ind_col = _detect_indicator_col(df)
    if not date_cols or not ind_col:
        return _empty_frame()

    long = _melt_wide(df, ind_col, date_cols)

    series_map: Dict[str, pd.Series] = {}
    for spec in METRICS:
        series_map[spec.key] = _extract_metric_from_long(long, ind_col, spec.patterns)

    # å¦‚æœåŒæ¯”å¢é•¿ç‡æ²¡å–åˆ°ï¼Œå°±å°è¯•ç”¨æ”¶å…¥/åˆ©æ¶¦è‡ªå·±ç®—
    if series_map["rev_growth"].empty:
        rev = _extract_metric_from_long(long, ind_col, AUX_ABS["revenue"])
        series_map["rev_growth"] = _yoy_growth_from_abs(rev)

    if series_map["profit_growth"].empty:
        prof = _extract_metric_from_long(long, ind_col, AUX_ABS["profit"])
        series_map["profit_growth"] = _yoy_growth_from_abs(prof)

    # assemble
    all_dates = pd.Index(sorted({d for s in series_map.values() for d in s.index if pd.notna(d)}))
    if all_dates.empty:
        return _empty_frame()

    out = pd.DataFrame({"date": all_dates})
    for spec in METRICS:
        s = series_map.get(spec.key, pd.Series(dtype=np.float32))
        out[spec.key] = out["date"].map(s).astype(np.float32)

    out["pub_date"] = pd.NaT
    out = out[list(OUT_COLS)].drop_duplicates("date", keep="last").sort_values("date").reset_index(drop=True)
    return out


def _long_to_metrics(df: pd.DataFrame) -> pd.DataFrame:
    """
    Long table (some akshare versions show this kind):
      rows: reporting dates
      columns: many indicators
    We select columns by regex; growth can be computed if needed.
    """
    date_col = _detect_date_col_long(df)
    if not date_col:
        return _empty_frame()

    x = df.copy()
    x.columns = [str(c).strip() for c in x.columns]
    x = x.rename(columns={date_col: "date"})
    x["date"] = _coerce_dt(x["date"])
    x = x.dropna(subset=["date"]).sort_values("date")

    out = pd.DataFrame({"date": x["date"].values})
    col_names = list(x.columns)

    # direct matches
    for spec in METRICS:
        pat = re.compile("|".join(spec.patterns), re.IGNORECASE)
        cand = next((c for c in col_names if c != "date" and pat.search(str(c))), None)
        out[spec.key] = _clean_numeric_series(x[cand]) if cand else np.nan

    # fallback compute YoY if growth empty
    if out["rev_growth"].isna().all():
        rev_cand = None
        for p in AUX_ABS["revenue"]:
            pat = re.compile(p, re.IGNORECASE)
            rev_cand = next((c for c in col_names if c != "date" and pat.search(str(c))), None)
            if rev_cand:
                break
        if rev_cand:
            rev = pd.Series(_clean_numeric_series(x[rev_cand]).values, index=out["date"])
            out["rev_growth"] = _yoy_growth_from_abs(rev).reindex(out["date"]).values

    if out["profit_growth"].isna().all():
        prof_cand = None
        for p in AUX_ABS["profit"]:
            pat = re.compile(p, re.IGNORECASE)
            prof_cand = next((c for c in col_names if c != "date" and pat.search(str(c))), None)
            if prof_cand:
                break
        if prof_cand:
            prof = pd.Series(_clean_numeric_series(x[prof_cand]).values, index=out["date"])
            out["profit_growth"] = _yoy_growth_from_abs(prof).reindex(out["date"]).values

    out["pub_date"] = pd.NaT
    out = out[list(OUT_COLS)].drop_duplicates("date", keep="last").reset_index(drop=True)
    return out


def normalize_fundamental_frame(raw: pd.DataFrame) -> pd.DataFrame:
    """
    Unified normalizer:
      - If detect YYYYMMDD columns => wide
      - Else if detect date column => long
      - Else empty
    """
    if raw is None or raw.empty:
        return _empty_frame()

    df = raw.copy()
    df.columns = [str(c).strip() for c in df.columns]

    if _detect_wide_date_cols(df.columns):
        return _wide_to_metrics(df)

    if _detect_date_col_long(df) is not None:
        return _long_to_metrics(df)

    return _empty_frame()


# ----------------------------
# 4) pub_date mapping via cninfo disclosure
# ----------------------------
# ä½¿ç”¨éæ•è·ç»„ (?:...) é¿å… pandas UserWarning
_TITLE_BAD_RE = re.compile(r"(?:æ›´æ­£|ä¿®è®¢|æ›´æ–°|è¡¥å……|æ›´æ­£å|ä¿®æ­£|å–æ¶ˆ|æ‘˜è¦æ›´æ­£|æ¾„æ¸…)", re.IGNORECASE)


def _title_to_report_end(title: str) -> Optional[pd.Timestamp]:
    """
    Parse cninfo announcement title -> report_end_date.
    We only map: Q1(0331), H1(0630), Q3(0930), Annual(1231).
    """
    t = str(title or "").strip()
    m = re.search(r"(\d{4})å¹´", t)
    if not m:
        return None
    year = int(m.group(1))

    if re.search(r"(å¹´åº¦æŠ¥å‘Š|å¹´æŠ¥)", t):
        return pd.to_datetime(f"{year}1231", errors="coerce")
    if re.search(r"(åŠå¹´åº¦æŠ¥å‘Š|åŠå¹´æŠ¥|ä¸­æœŸæŠ¥å‘Š)", t):
        return pd.to_datetime(f"{year}0630", errors="coerce")
    if re.search(r"(ç¬¬ä¸€å­£åº¦æŠ¥å‘Š|ä¸€å­£åº¦æŠ¥å‘Š|ä¸€å­£æŠ¥)", t):
        return pd.to_datetime(f"{year}0331", errors="coerce")
    if re.search(r"(ç¬¬ä¸‰å­£åº¦æŠ¥å‘Š|ä¸‰å­£åº¦æŠ¥å‘Š|ä¸‰å­£æŠ¥)", t):
        return pd.to_datetime(f"{year}0930", errors="coerce")
    return None


def _fetch_pub_date_map_cninfo(
    ak_client: AkClient,
    code: str,
    start_year: str,
    logger,
) -> pd.DataFrame:
    """
    Use cninfo disclosure interface to build (report_end_date -> pub_date) map.

    Interface: stock_zh_a_disclosure_report_cninfo
      outputs include å…¬å‘Šæ ‡é¢˜ / å…¬å‘Šæ—¶é—´.
    """
    start_date = f"{start_year}0101"
    end_date = time.strftime("%Y%m%d")

    cats = ("å¹´æŠ¥", "åŠå¹´æŠ¥", "ä¸€å­£æŠ¥", "ä¸‰å­£æŠ¥")
    frames: List[pd.DataFrame] = []

    for cat in cats:
        try:
            df = ak_client.call(
                ak.stock_zh_a_disclosure_report_cninfo,
                symbol=code,
                market="æ²ªæ·±äº¬",
                category=cat,
                start_date=start_date,
                end_date=end_date,
                keyword="",
            )
        except TypeError:
            # å…¼å®¹å°‘æ•°ç‰ˆæœ¬ keyword å¯èƒ½ä¸æ˜¯å¿…å¡«
            df = ak_client.call(
                ak.stock_zh_a_disclosure_report_cninfo,
                symbol=code,
                market="æ²ªæ·±äº¬",
                category=cat,
                start_date=start_date,
                end_date=end_date,
            )
        except Exception as e:
            logger.debug(f"[Fundamental][PubDate] cninfo failed code={code} cat={cat}: {e}")
            continue

        if df is None or df.empty:
            continue

        df = df.copy()
        df.columns = [str(c).strip() for c in df.columns]

        title_col = next((c for c in ("å…¬å‘Šæ ‡é¢˜", "æ ‡é¢˜", "å…¬å‘Šåç§°") if c in df.columns), None)
        time_col = next((c for c in ("å…¬å‘Šæ—¶é—´", "å…¬å‘Šæ—¥æœŸ", "å‘å¸ƒæ—¶é—´") if c in df.columns), None)
        if not title_col or not time_col:
            continue

        df = df[[title_col, time_col]].rename(columns={title_col: "title", time_col: "pub_date"})
        df["pub_date"] = pd.to_datetime(df["pub_date"], errors="coerce")
        df = df.dropna(subset=["pub_date"])
        df["report_end"] = df["title"].map(_title_to_report_end)
        df = df.dropna(subset=["report_end"])

        # Fix: explicitly use regex=True and ensure _TITLE_BAD_RE is non-capturing
        good = df[~df["title"].astype(str).str.contains(_TITLE_BAD_RE, na=False, regex=True)]
        use = good if not good.empty else df

        frames.append(use[["report_end", "pub_date"]])

    if not frames:
        return pd.DataFrame(columns=["date", "pub_date"])

    x = pd.concat(frames, ignore_index=True)
    # For each report_end, take earliest pub_date (closest to first disclosure)
    m = x.groupby("report_end")["pub_date"].min().reset_index()
    m = m.rename(columns={"report_end": "date"})
    return m


def _estimate_pub_date(report_date: pd.Timestamp) -> pd.Timestamp:
    """
    Estimate pub_date based on statutory deadlines if real date is missing.
    Rules (A-Share):
      Q1 (03-31) -> 04-30
      H1 (06-30) -> 08-31
      Q3 (09-30) -> 10-31
      FY (12-31) -> 04-30 (next year)
    """
    if pd.isna(report_date):
        return pd.NaT

    try:
        m, d = report_date.month, report_date.day
        y = report_date.year
        if m == 3 and d == 31:
            return pd.Timestamp(f"{y}-04-30")
        elif m == 6 and d == 30:
            return pd.Timestamp(f"{y}-08-31")
        elif m == 9 and d == 30:
            return pd.Timestamp(f"{y}-10-31")
        elif m == 12 and d == 31:
            return pd.Timestamp(f"{y + 1}-04-30")
    except Exception:
        pass

    return pd.NaT


def _attach_pub_dates(
    ak_client: AkClient,
    out: pd.DataFrame,
    code: str,
    start_year: str,
    cfg: DPConfig,
    logger,
) -> pd.DataFrame:
    if out is None or out.empty:
        return out

    sync_pub = bool(cfg.get("SYNC_FUNDAMENTAL_PUBDATE", True))

    out2 = out.copy()
    if "pub_date" not in out2.columns:
        out2["pub_date"] = pd.NaT

    if sync_pub:
        # 1. Try real fetch
        mp = _fetch_pub_date_map_cninfo(ak_client, code, start_year=start_year, logger=logger)
        if mp is not None and not mp.empty:
            # Drop old (empty) pub_date and merge new one
            out2 = out2.drop(columns=["pub_date"], errors="ignore").merge(mp, on="date", how="left")

    # 2. Fill missing with estimate
    # æ— è®ºæ˜¯æ²¡å¼€å¯åŒæ­¥ã€åŒæ­¥å¤±è´¥ã€è¿˜æ˜¯åŒæ­¥äº†ä½†ç¼ºæŸå‡ æœŸï¼Œéƒ½å¯¹ NaT è¿›è¡Œè¡¥å…¨
    mask_missing = out2["pub_date"].isna()
    if mask_missing.any():
        out2.loc[mask_missing, "pub_date"] = out2.loc[mask_missing, "date"].apply(_estimate_pub_date)

    # ensure schema order
    for c in OUT_COLS:
        if c not in out2.columns:
            out2[c] = np.nan
    return out2[list(OUT_COLS)]


# ----------------------------
# 5) pipeline
# ----------------------------
class FundamentalPipeline:
    """
    Download & cache quarterly fundamentals per-code.
    Output: {DATA_DIR}/fundamental/{code}.parquet
      columns: date(report_end), roe, rev_growth, profit_growth, debt_ratio, eps, bps, pub_date

    Fundamentals from: ak.stock_financial_abstract
    Pub dates from: ak.stock_zh_a_disclosure_report_cninfo
    """
    SCHEMA_VER = 2

    def __init__(self, cfg: DPConfig, ak_client: AkClient, logger):
        self.cfg = cfg
        self.ak_client = ak_client
        self.logger = logger
        os.makedirs(fundamental_dir(cfg), exist_ok=True)

    def _should_skip(self, path: str) -> bool:
        days = int(self.cfg.get("FUND_TTL_DAYS", 5) or 5)
        ttl = max(1, days) * 24 * 3600
        return (
            os.path.exists(path)
            and os.path.getsize(path) > 512
            and (time.time() - os.path.getmtime(path)) < ttl
        )

    def _download_one(self, code: str) -> Tuple[str, bool, str, int]:
        c = normalize_code(code)
        if not c:
            return str(code), True, "BadCode", 0

        path = fundamental_path(self.cfg, c)
        if self._should_skip(path):
            return c, True, "Skipped", -1

        start_year = str(self.cfg.get("FUNDAMENTAL_START_YEAR", "2010") or "2010")

        try:
            raw = _best_effort_call_financial_abstract(self.ak_client, c)
            out = normalize_fundamental_frame(raw)
            out = _attach_pub_dates(self.ak_client, out, code=c, start_year=start_year, cfg=self.cfg, logger=self.logger)

            atomic_save_parquet(
                out,
                path,
                index=False,
                compression=str(self.cfg.get("PARQUET_COMPRESSION", "zstd") or "zstd"),
            )
            return c, True, ("Empty" if out.empty else "Success"), int(len(out))
        except Exception as e:
            return c, False, f"Failed({type(e).__name__})", 0

    def download(self, codes) -> None:
        if not bool(self.cfg.get("SYNC_FUNDAMENTAL", False)):
            self.logger.info("ğŸŸ¦ [Fundamental] SYNC_FUNDAMENTAL=False; skip.")
            return

        codes = [normalize_code(c) for c in codes]
        codes = [c for c in codes if c]
        if not codes:
            self.logger.warning("ğŸŸ¦ [Fundamental] empty codes; skip.")
            return

        workers = int(self.cfg.get("FIN_WORKERS", 8) or 8)
        max_inflight = int(self.cfg.get("FIN_MAX_INFLIGHT", workers * 4) or (workers * 4))

        self.logger.info(f"ğŸŸ¦ [Fundamental] syncing {len(codes)} codes ... workers={workers} inflight={max_inflight}")

        q = deque(codes)
        ok = bad = empty = skipped = 0

        def submit_more(ex, inflight):
            while q and len(inflight) < max_inflight:
                c = q.popleft()
                inflight[ex.submit(self._download_one, c)] = c

        with concurrent.futures.ThreadPoolExecutor(max_workers=workers) as ex:
            inflight = {}
            submit_more(ex, inflight)

        with tqdm(total=len(codes), dynamic_ncols=True, desc="Fundamental", unit="code") as pbar:
            while inflight:
                done, _ = concurrent.futures.wait(
                    inflight.keys(),
                    return_when=concurrent.futures.FIRST_COMPLETED,
                )
                for fut in done:
                    _ = inflight.pop(fut, None)
                    code, success, msg, rows = fut.result()
                    if success:
                        ok += 1
                        if msg == "Empty":
                            empty += 1
                        if msg == "Skipped":
                            skipped += 1
                    else:
                        bad += 1

                    pbar.update(1)
                    pbar.set_postfix(ok=ok, bad=bad, empty=empty, skipped=skipped, last=code)

                submit_more(ex, inflight)

        self.logger.info(f"ğŸŸ¦ [Fundamental] done. ok={ok}, fail={bad}, empty={empty}, skipped={skipped}")