import pytest
import os
import shutil
import pandas as pd
import numpy as np
import torch
import pickle
from src.config import Config
from src.alpha_lib import AlphaFactory
from src.data_provider import DataProvider
from src.model import PatchTSTForStock, SotaConfig
from src.backtest import run_single_backtest
from transformers import TrainingArguments, Trainer


# ==============================================================================
#  Helper: ä¸´æ—¶é…ç½®è¦†ç›–
# ==============================================================================
class MockConfig:
    """ä¸ºé›†æˆæµ‹è¯•åˆ›å»ºçš„å¾®å‹é…ç½®"""

    def __init__(self, tmp_path):
        self.BASE_DIR = str(tmp_path)
        self.DATA_DIR = os.path.join(str(tmp_path), "data")
        self.OUTPUT_DIR = os.path.join(str(tmp_path), "output")
        self.CONTEXT_LEN = 10
        self.PRED_LEN = 2
        self.PATCH_LEN = 2
        self.STRIDE = 2
        self.FEATURE_PREFIXES = ['style_', 'tech_']  # åªæµ‹è¯•å°‘é‡å› å­ä»¥åŠ é€Ÿ
        self.BATCH_SIZE = 4
        self.INFERENCE_BATCH_SIZE = 4
        self.DEVICE = "cpu"  # å¼ºåˆ¶ CPU ç¡®ä¿ CI/CD å…¼å®¹æ€§

        os.makedirs(self.DATA_DIR, exist_ok=True)
        os.makedirs(self.OUTPUT_DIR, exist_ok=True)


# ==============================================================================
#  Integration Test Suite
# ==============================================================================
@pytest.fixture
def mock_env(tmp_path):
    """Fixture: å‡†å¤‡éš”ç¦»çš„æµ‹è¯•ç¯å¢ƒ"""
    # 1. è¦†ç›– Config (Monkeypatching)
    original_data_dir = Config.DATA_DIR
    original_output_dir = Config.OUTPUT_DIR

    mock_cfg = MockConfig(tmp_path)
    Config.DATA_DIR = mock_cfg.DATA_DIR
    Config.OUTPUT_DIR = mock_cfg.OUTPUT_DIR
    Config.CONTEXT_LEN = mock_cfg.CONTEXT_LEN
    Config.FEATURE_PREFIXES = mock_cfg.FEATURE_PREFIXES
    Config.DEVICE = "cpu"

    yield mock_cfg

    # Teardown: æ¢å¤é…ç½®
    Config.DATA_DIR = original_data_dir
    Config.OUTPUT_DIR = original_output_dir


def create_synthetic_data(mock_cfg, days=300):
    """
    ç”Ÿæˆè¶³å¤Ÿç”¨äº Training + Gap + Inference çš„æ•°æ®
    [Fix] å¢åŠ åˆ° 300 å¤©ï¼Œç¡®ä¿ 90% Split (270å¤©) åï¼Œå‰©ä½™ 30 å¤©æ‰£é™¤ Gap (10å¤©) è¿˜æœ‰ 20 å¤©ç»™ Test é›†ã€‚
    """
    dates = pd.date_range("2024-01-01", periods=days, freq="B")
    codes = ["000001", "000002"]

    frames = []
    for code in codes:
        # åˆ¶é€ ç¡®å®šæ€§è¶‹åŠ¿: 000001 ä¸€ç›´æ¶¨, 000002 ä¸€ç›´è·Œ
        trend = 1 if code == "000001" else -1
        base_price = 100.0 + np.arange(days) * trend * 0.5

        df = pd.DataFrame({
            'date': dates,
            'code': code,
            'open': base_price,
            'high': base_price + 2,
            'low': base_price - 2,
            'close': base_price + trend * 0.2,  # æ”¶ç›˜ä»·ç•¥æœ‰å˜åŠ¨
            'volume': 100000 + np.random.randint(-1000, 1000, days),
            'amount': 10000000.0,
            # Mock è´¢åŠ¡æ•°æ®
            'pe_ttm': 10.0,
            'pb': 1.5,
            'roe': 0.15
        })
        frames.append(df)

    return pd.concat(frames).sort_values(['code', 'date']).reset_index(drop=True)


def test_full_system_lifecycle(mock_env, capsys):
    """
    ğŸš€ å…¨ç³»ç»Ÿé›†æˆæµ‹è¯• (End-to-End Integration Test)

    è¦†ç›–æµç¨‹:
    1. ETL: ç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®å¹¶å­˜å…¥ Parquet
    2. Alpha: è¯»å–æ•°æ®ï¼Œè®¡ç®—å› å­ï¼Œç”Ÿæˆ Dataset
    3. Train: åˆå§‹åŒ–æ¨¡å‹ï¼Œè¿è¡Œå¾®å‹è®­ç»ƒå¾ªç¯ï¼Œä¿å­˜ Checkpoint
    4. Inference: åŠ è½½ Checkpointï¼Œå¯¹æ–°æ•°æ®è¿›è¡Œæ¨ç†
    5. Consistency: éªŒè¯è®­ç»ƒå’Œæ¨ç†çš„ç‰¹å¾åˆ—æ˜¯å¦ä¸¥æ ¼å¯¹é½ (P0çº§é£é™©æ£€æŸ¥)
    6. Backtest: å°†æ¨ç†ä¿¡å·è¾“å…¥å›æµ‹å¼•æ“ï¼ŒéªŒè¯æ˜¯å¦äº§ç”Ÿäº¤æ˜“
    """

    print("\n>>> [Step 1] ETL & Data Processing...")
    raw_df = create_synthetic_data(mock_env)

    # æ¨¡æ‹Ÿ DataProvider._download_worker çš„ç»“æœ (å†™å…¥ parquet)
    for code in raw_df['code'].unique():
        sub_df = raw_df[raw_df['code'] == code].set_index('date')
        sub_df.to_parquet(os.path.join(mock_env.DATA_DIR, f"{code}.parquet"))

    # è¿è¡Œ DataProvider å¤„ç†é€»è¾‘ (ç”Ÿæˆ Cache)
    panel_df, feature_cols = DataProvider.load_and_process_panel(mode='train', force_refresh=True)

    assert len(panel_df) > 0
    assert len(feature_cols) > 0
    assert 'target' in panel_df.columns
    print(f"âœ… Data Processed. Features: {len(feature_cols)}, Samples: {len(panel_df)}")

    print("\n>>> [Step 2] Dataset Generation...")
    ds, num_features = DataProvider.make_dataset(panel_df, feature_cols)
    assert len(ds['train']) > 0
    # [Check] ç¡®ä¿ Test é›†ä¸ä¸ºç©ºï¼Œå¦åˆ™ Generator ä¼šæŠ›é”™
    assert len(ds['test']) > 0
    print(f"âœ… Dataset Created. Train: {len(ds['train'])}, Test: {len(ds['test'])}")

    print("\n>>> [Step 3] Model Training (Micro-Batch)...")
    model_config = SotaConfig(
        num_input_channels=num_features,
        context_length=Config.CONTEXT_LEN,
        patch_length=mock_env.PATCH_LEN,
        stride=mock_env.STRIDE,
        d_model=16,  # Tiny model
        num_hidden_layers=1,
        n_heads=2,
        dropout=0.1
    )
    model = PatchTSTForStock(model_config)

    training_args = TrainingArguments(
        output_dir=mock_env.OUTPUT_DIR,
        num_train_epochs=1,  # åªè·‘ 1 ä¸ª epoch
        max_steps=5,  # å¼ºåˆ¶åªè·‘ 5 æ­¥ï¼ŒéªŒè¯èƒ½å¦ backward å³å¯
        per_device_train_batch_size=4,
        learning_rate=1e-3,
        report_to="none",
        save_strategy="no",  # è®­ç»ƒä¸­ä¸å­˜ï¼Œæœ€åæ‰‹åŠ¨å­˜
        use_cpu=True
    )

    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=ds['train']
    )

    trainer.train()

    # ä¿å­˜æ¨¡å‹
    final_model_path = os.path.join(mock_env.OUTPUT_DIR, "final_model")
    trainer.save_model(final_model_path)

    # ä¿å­˜ç‰¹å¾åˆ—è¡¨ (æ¨¡æ‹Ÿç”Ÿäº§ç¯å¢ƒçš„ metadata)
    feature_meta_path = os.path.join(mock_env.OUTPUT_DIR, "feature_meta.pkl")
    with open(feature_meta_path, "wb") as f:
        pickle.dump(feature_cols, f)

    assert os.path.exists(os.path.join(final_model_path, "config.json"))
    assert os.path.exists(os.path.join(final_model_path, "model.safetensors"))
    print("âœ… Model Trained & Saved.")

    print("\n>>> [Step 4] Inference & Consistency Check...")
    # æ¨¡æ‹Ÿæ¨ç†æ¨¡å¼ï¼šé‡æ–°åŠ è½½æ¨¡å‹å’Œæ•°æ®
    loaded_model = PatchTSTForStock.from_pretrained(final_model_path)
    loaded_model.eval()

    # åŠ è½½æ¨ç†æ•°æ® (Mode='predict')
    pred_df, pred_features = DataProvider.load_and_process_panel(mode='predict', force_refresh=True)

    # ã€å…³é”®æ£€æŸ¥ã€‘éªŒè¯ç‰¹å¾å¯¹é½ (P0çº§é£é™©)
    # åœ¨ç”Ÿäº§ä¸­ï¼Œå¿…é¡»ç¡®ä¿æ¨ç†æ—¶çš„ç‰¹å¾é¡ºåºä¸è®­ç»ƒæ—¶å®Œå…¨ä¸€è‡´
    with open(feature_meta_path, "rb") as f:
        train_features = pickle.load(f)

    assert train_features == pred_features, "âŒ CRITICAL: Inference features mismatch Training features!"
    print("âœ… Feature Alignment Verified.")

    # æ„é€ æ¨ç† Batch
    # å–æœ€åä¸€å¤©çš„æ•°æ®è¿›è¡Œæ¨ç†
    last_date = pred_df['date'].max()
    target_group = pred_df[pred_df['date'] == last_date]
    codes = target_group['code'].unique()

    # ç®€å•æ¨¡æ‹Ÿæ¨ç†è¿‡ç¨‹ (ä¸èµ° full loopï¼Œç›´æ¥æµ‹ forward)
    # æ„é€ ä¸€ä¸ª (Batch, Seq, Feat)
    sample_input = torch.randn(len(codes), Config.CONTEXT_LEN, num_features)
    with torch.no_grad():
        output = loaded_model(past_values=sample_input)
        scores = output.logits.squeeze().numpy()

    assert len(scores) == len(codes)
    print(f"âœ… Inference Successful. Generated {len(scores)} scores.")

    print("\n>>> [Step 5] Backtest Execution...")
    # æ„é€  Backtest éœ€è¦çš„ Signal DataFrame
    # æˆ‘ä»¬æ‰‹åŠ¨åˆ¶é€ ä¸€ä¸ªå¼ºä¿¡å·ï¼š000001 æé«˜åˆ†ï¼Œ000002 æä½åˆ†
    # [Fix] ç¡®ä¿ç´¢å¼•æ˜¯ DatetimeIndexï¼Œä»¥ä¾¿ Backtest å¼•æ“èƒ½æ­£ç¡®ç´¢å¼•
    unique_dates = sorted(pred_df['date'].unique())
    signals = pd.DataFrame(index=unique_dates, columns=codes, dtype=float)
    signals[:] = -100.0  # é»˜è®¤æ— æ•ˆ

    # åœ¨æ‰€æœ‰æ—¥æœŸå¯¹ 000001 å‘å‡ºä¹°å…¥ä¿¡å·
    if "000001" in codes:
        signals["000001"] = 100.0

        # è¿è¡Œå›æµ‹
    # æ³¨æ„ï¼šæˆ‘ä»¬éœ€è¦ä¸ºå›æµ‹æä¾›è¡Œæƒ…æ•°æ®ï¼Œè¿™é‡Œå¤ç”¨ä¹‹å‰ç”Ÿæˆçš„ Parquet
    # è¿™é‡Œçš„ Config å·²ç»è¢« monkeypatch äº†ï¼Œéœ€è¦ç¡®ä¿å›æµ‹å¼•æ“èƒ½è¯»åˆ° Mock Data
    result = run_single_backtest(["000001"], with_fees=True, initial_cash=100000.0, top_k=1)

    assert result is not None
    # 000001 æ˜¯ä¸Šæ¶¨è¶‹åŠ¿ï¼Œä¸€ç›´æŒæœ‰åº”è¯¥ç›ˆåˆ©
    print(f"Final Value: {result['final_value']:.2f}")
    assert result['final_value'] > 100000.0, "Backtest should profit on uptrend stock with buy signal"
    assert result['total_trades'] > 0, "Should have executed at least one trade"

    print("âœ… Backtest Finished Successfully.")
    print("\nğŸ‰ğŸ‰ğŸ‰ ALL SYSTEMS GO! INTEGRATION TEST PASSED! ğŸ‰ğŸ‰ğŸ‰")